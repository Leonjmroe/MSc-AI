{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"predict_words_lstm.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pmUHS8DY8rdB"},"source":["# Predict the Next Word: Word-Level Language Modelling\n","\n","In this notebook, we will build a word-level language model for predicting the next word given a sequence of words.\n","\n","We use a text dataset called <a href=\"https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html\">Penn Tree Bank (PTB) dataset</a> to train, validate and test the language model. Penn Treebank (PTB) dataset, is widely used in machine learning for NLP (Natural Language Processing) research. Word-level PTB does not contain capital letters, numbers, and punctuations, and the vocabulary is capped at 10k unique words, which is relatively small in comparison to most modern datasets which can result in a larger number of out of vocabulary tokens. "]},{"cell_type":"markdown","metadata":{"id":"bxvtFrpy8w0A"},"source":["## Setting-Up 1: Mount Google Drive to the notebook\n"," \n","You can easily load data from Google Drive by mounting it to the notebook. To do this, type the following code in your notebook. Run it, you will see a link, and need enter some code. To get the code, you need first click the link. In the link, after logining to your Google Account, you will see a code. Copy it, and paste it here. \n","\n","Then you will see \"Mounted at /content/drive\" -- done!\n","\n","Now you will see your Google Drive files in the left pane (file explorer). Right click on the file and select 'Copy path'. Your file's full path will be like `/content/gdrive/MyDrive/Colab Notebooks/dogs_vs_cats.ipynb`. \n","\n","Your current directory will be `/content/`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSkh8LyZ8ycj","executionInfo":{"status":"ok","timestamp":1610927203390,"user_tz":0,"elapsed":26515,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"08be02b9-8ce3-4c03-e07e-ea07d6763dbf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OQJd0c4N88wC"},"source":["## Setting-Up 2: One click to enable FREE GPU \n","After you mounted your Google Drive and downloaded the dataset, let’s enable GPU in your Colab notebook and train your model. In Google Colab, it is very easy to do so. \n","\n","From task bar, click: Runtime ⇨ Change runtime type\n","\n","Choose 'GPU' in the Hardware accelerator."]},{"cell_type":"markdown","metadata":{"id":"ffLF-k818rdL"},"source":["## Setting data path\n","You need first upload the three txt files to your Google Drive. Then  change the `data_path` variable to match the location of this downloaded data."]},{"cell_type":"code","metadata":{"id":"pIk1nq3Q8rdL","executionInfo":{"status":"ok","timestamp":1610927204901,"user_tz":0,"elapsed":28013,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}}},"source":["import collections\n","import os\n","import tensorflow as tf\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n","from keras.layers import LSTM\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","data_path = '/content/drive/MyDrive/Colab Notebooks/data/PTB/' # change the data_path"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hY0lIQE38rdM"},"source":["## STEP 1: Preparing the data\n","In order to get the text data into the right shape for input into the Keras LSTM model, each unique word in the corpus must be assigned a unique integer index. Then the text corpus needs to be re-constituted in order, but rather than text words we have the integer identifiers in order. The three functions which do this in the code are `read_words()`, `build_vocab()` and `file_to_word_ids()`. See the comments for explanation. \n","\n","Then we call the `load_data()` function to run these functions. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-RnnVEkS8rdM","executionInfo":{"status":"ok","timestamp":1610927207008,"user_tz":0,"elapsed":30114,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"6d843909-a99c-4d37-cf29-3b816917247b"},"source":["\"\"\"\n","Split the given text file into separate words and sentence based characters \n","(i.e. end-of-sentence <eos>).\n","\"\"\"\n","def read_words(filename):\n","    with tf.io.gfile.GFile(filename, \"r\") as f:\n","        return f.read().replace(\"\\n\", \"<eos>\").split()\n","\n","\"\"\"\n","Build a vocabulary: each unique word is identified and assigned a unique integer.\n","In natual langage processing, not all words are \n","put into the vocabulary -  rather, usually \n","limited to a certain N number of most common words. \n","More frequent the word appears, the smaller number it is represented. \n","\"\"\" \n","def build_vocab(filename):\n","    data = read_words(filename)\n","\n","    counter = collections.Counter(data)\n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","\n","    words, _ = list(zip(*count_pairs))\n","    word_to_id = dict(zip(words, range(len(words))))\n","\n","    return word_to_id\n","\n","\"\"\"\n","Convert the original text file into a list of those unique integers, \n","where each word is substituted with its new integer identifier.\n","This allows the text data to be consumed in the neural network.\n","\"\"\"\n","def file_to_word_ids(filename, word_to_id):\n","    data = read_words(filename)\n","    return [word_to_id[word] for word in data if word in word_to_id]\n","\n","\n","def load_data():\n","    # get the data paths\n","    train_path = os.path.join(data_path, \"ptb.train.txt\")\n","    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n","    test_path = os.path.join(data_path, \"ptb.test.txt\")\n","\n","    # build the complete vocabulary, then convert text data to list of integers\n","    word_to_id = build_vocab(train_path)\n","    train_data = file_to_word_ids(train_path, word_to_id)\n","    valid_data = file_to_word_ids(valid_path, word_to_id)\n","    test_data = file_to_word_ids(test_path, word_to_id)\n","    size_vocabulary = len(word_to_id)\n","    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n","\n","    print(\"Vocabulary size: %d \" % size_vocabulary)\n","    # Disply the first 10 training data, they must be 10 integer numbers \n","    print(train_data[100:110]) \n","    # convert the integer numbers back into words\n","    print(\" \".join([reversed_dictionary[x] for x in train_data[100:110]]))\n","    return train_data, valid_data, test_data, size_vocabulary, reversed_dictionary\n","\n","# Load the data\n","train_data, valid_data, test_data, size_vocabulary, reversed_dictionary = load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Vocabulary size: 10000 \n","[431, 4115, 5, 14, 45, 55, 3, 72, 195, 1244]\n","workers exposed to it more than N years ago researchers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qJf_zYo98rdO"},"source":["## STEP 2: Data generator for progressive data loading\n","\n","Instead of loading all the training and validation data in memory, it is more feasible to loading the data *progressively*.\n","\n","When training neural networks, we generally feed data into them in small batches. Keras has some handy functions which can extract training data automatically from a pre-supplied Python iterator/generator object and input it to the model. One of these Keras functions is called `fit_generator`. \n","\n","In this case, we create a generator class `KerasBatchGenerator` that contains a method `generate()` to implement such a structure. \n","\n","The `num_steps` is the input sequence length. The `skip_steps` is the number of words we want to skip over between training samples within each batch. To make this a bit clearer, consider the following sentence: “*The cat sat on the mat, and ate his hat. Then he jumped up and spat*”. Suppose `num_steps=5` and `skip_steps=3`, then the first sequence would be “*The cat sat on the*”, and the second would be \"*on the mat and ate*\". Therefore, if `skip_steps=num_steps`, there would be no overlap between the succesive sequences. \n","\n","The variable `y` is the target word label (or category label) for each input variabel `x`. You may notice that the variable `y`'s third dimension is the size of the vocabulary, in this case, 10,000. This is because it is a one-hot representation, i.e., only one of the elementes in each vector is 1, the rest is 0, like this: (0, 0, 0, …, 1, 0, …, 0, 0). Keras offers a `to_categorical()` function to do such transformations.\n","\n","We then get both the generated training data and validation data by initiating two instances of the class `KerasBatchGenerator`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jc9Idyg88rdP","executionInfo":{"status":"ok","timestamp":1610927207008,"user_tz":0,"elapsed":30104,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"fdb9b0d2-fede-4be3-c381-8dd1ced8b034"},"source":["class KerasBatchGenerator(object):\n","\n","    def __init__(self, data, num_steps, batch_size, size_vocabulary, skip_step=5):\n","        self.data = data\n","        self.num_steps = num_steps # the input sequence length\n","        self.batch_size = batch_size\n","        self.size_vocabulary = size_vocabulary\n","        # this will track the progress of the batches sequentially through the\n","        # data set - once the data reaches the end of the data set it will reset\n","        # back to zero\n","        self.current_idx = 0\n","        # skip_step is the number of words which will be skipped before the next\n","        # batch is skimmed from the data set\n","        self.skip_step = skip_step\n","\n","    def generate(self):\n","        x = np.zeros((self.batch_size, self.num_steps))\n","        y = np.zeros((self.batch_size, self.num_steps, self.size_vocabulary))\n","        while True:\n","            for i in range(self.batch_size):\n","                if self.current_idx + self.num_steps >= len(self.data):\n","                    # reset the index back to the start of the data set\n","                    self.current_idx = 0\n","                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n","                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n","                # convert all of temp_y into a one hot representation\n","                y[i, :, :] = to_categorical(temp_y, num_classes=self.size_vocabulary)\n","                self.current_idx += self.skip_step\n","            yield x, y\n","\n","            \n","num_steps = 30\n","batch_size = 20\n","print(\"* Generate training and validation data ...\")\n","train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, size_vocabulary,\n","                                           skip_step=num_steps)\n","valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, size_vocabulary,\n","                                           skip_step=num_steps)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["* Generate training and validation data ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D23SJV8z8rdP"},"source":["## STEP 3: Build the LSTM model\n","\n","The full model architecutre looks like the following. \n","\n","<img src=\"https://drive.google.com/uc?id=15mKswIuW3CZOKghPF6eYDQSYZcvptVc0\">\n","\n","### Embedding layer\n","\n","Before we add the LSTM network, we need first convert our words (referenced by integers in the data) into meaningful embedding vectors - called **word embedding**. Here, we set `embedding_size` to be 500, means each word will be transformed into a 500 length vector. These embedding vectors will be learnt as part of the overall model learning. (As mentioned in the previous example, you could choose to use *pre-trained word embedding*.)\n","\n","The `Embedding()` layer takes the size of the vocabulary as its first argument (`input_dim`), then the size of the embedding vector as the second argument (`output_dim`). Also we need specify the `input_length` which is the length of input sequences, i.e. the number of steps/words in each sample. \n","\n","After word embedding, the input data for LSTM is in the shape of (`batch_size`, `num_steps`, `embedding_size`). \n","\n","### Stacked LSTM layers\n","\n","Then we add a stacked LSTM model with two LSTMs. The first argument for `LSTM` layer is the number of nodes for the hidden layer (or the length of hidden state vector). The next argument is the `return_sequences=True` means to return all of the outputs from the unrolled LSTM cell through time. If this argument is left out, the LSTM will simply provide the output vector from the last time step. \n","\n","The reason for returning all the output vectors of the first LSTM is because they will be used as inputs for the second LSTM. In this example we are trying to predict the very next word in the sequence, we could set the second `return_sequeces` to be `False`. However, if we compare the LSTM cell output at each time step with the very next word in the sequence – in this way we get `num_steps` sources to correct errors  rather than just one for each sample. Therefore, for both stacked LSTM layers, we want to return all the sequences. \n","\n","The output shape of each LSTM layer is (`batch_size`, `num_steps`, `hidden_size`).\n","\n","### TimeDistributed layer\n","\n","There is a special Keras layer for use in recurrent neural networks called `TimeDistributed`. This function adds an independent layer for each time step in the recurrent model. So, for instance, if we have 30 time steps in our model, a `TimeDistributed` layer operating on a `Dense` layer would produce 30 independent Dense layers, one for each time step. \n","\n","This output layer has a `softmax` activation applied to it, since predicting a word can be considered as a multi-class classification."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2T8KN26L8rdQ","executionInfo":{"status":"ok","timestamp":1610927212749,"user_tz":0,"elapsed":35833,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"930c4c3a-3330-47ef-ad00-cbe6fc944e67"},"source":["print(\"* Build the two-layer LSTM model ...\")\n","embedding_size = 500\n","hidden_size = 500\n","\n","model = Sequential()\n","model.add(Embedding(size_vocabulary, embedding_size, input_length=num_steps))\n","model.add(Dropout(0.5))\n","model.add(LSTM(hidden_size, return_sequences=True))\n","model.add(LSTM(hidden_size, return_sequences=True))\n","model.add(Dropout(0.5))\n","model.add(TimeDistributed(Dense(size_vocabulary)))\n","model.add(Activation('softmax'))\n","model.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["* Build the two-layer LSTM model ...\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 30, 500)           5000000   \n","_________________________________________________________________\n","dropout (Dropout)            (None, 30, 500)           0         \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 30, 500)           2002000   \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 30, 500)           2002000   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 30, 500)           0         \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, 30, 10000)         5010000   \n","_________________________________________________________________\n","activation (Activation)      (None, 30, 10000)         0         \n","=================================================================\n","Total params: 14,014,000\n","Trainable params: 14,014,000\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eTqz3tf38rdQ"},"source":["## STEP 4: Compile the model\n","Once you've completed you model, compile it. \n","\n","In this case, we are using `categorical_crossentropy` for loss and and `categorical_accuracy` for metrics, which are usually for multi-class classification. Actually, the word prediction is a multi-class classification problem, as each unique word can be considered as one category. It also explains why we use `softmax` activation function for the last layer.\n","\n","The optimizer that will be used is the `Adam` optimizer – an effective “all-round” optimizer with adaptive learning rate."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inqzzXLi8rdQ","executionInfo":{"status":"ok","timestamp":1610927212749,"user_tz":0,"elapsed":35823,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"13806036-d5be-4079-b363-0b32aec3f6dc"},"source":["print(\"* Compile the two-layer LSTM model ...\")\n","optimizer = Adam()\n","model.compile(loss='categorical_crossentropy', \n","              optimizer='adam', \n","              metrics=['categorical_accuracy'])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["* Compile the two-layer LSTM model ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xNuWb2zu8rdR"},"source":["## STEP 5: Train the model\n","Note that the `ModelCheckpoint()` function can include the epoch in its naming of the model, which is good for keeping track of things. The final step in training the Keras LSTM model is to call the aforementioned fit_generator function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mugWRmvA8rdR","executionInfo":{"status":"ok","timestamp":1610930978113,"user_tz":0,"elapsed":3801175,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"d31a2d91-f3d4-4b5f-c803-2d2fd5f66971"},"source":["num_epochs = 30\n","model_file_name = \"LSTM-2layer-{}-{}.hdf5\".format(num_steps,hidden_size)\n","\n","history = model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n","            validation_data=valid_data_generator.generate(),\n","            validation_steps=len(valid_data)//(batch_size*num_steps))\n","\n","model.save(data_path + \"/\" + model_file_name)\n","print(\"*Model is saved.\")\n","\n","import matplotlib.pyplot as plt\n","fig = plt.figure()\n","fig.add_subplot(1,2,1)\n","plt.plot(history.history['loss'], label='train loss')\n","plt.plot(history.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.grid(True)\n","plt.ylim([0,8])##\n","plt.xlabel('epoch')\n","\n","fig.add_subplot(1,2,2)\n","plt.plot(history.history['categorical_accuracy'], label='train accuracy')\n","plt.plot(history.history['val_categorical_accuracy'], label='val accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.ylim([0,1.0])\n","plt.xlabel('epoch')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["1549/1549 [==============================] - 137s 83ms/step - loss: 6.8727 - categorical_accuracy: 0.0660 - val_loss: 5.8626 - val_categorical_accuracy: 0.1493\n","Epoch 2/30\n","1549/1549 [==============================] - 127s 82ms/step - loss: 5.8346 - categorical_accuracy: 0.1543 - val_loss: 5.4578 - val_categorical_accuracy: 0.1829\n","Epoch 3/30\n","1549/1549 [==============================] - 127s 82ms/step - loss: 5.4636 - categorical_accuracy: 0.1790 - val_loss: 5.2642 - val_categorical_accuracy: 0.2021\n","Epoch 4/30\n","1549/1549 [==============================] - 127s 82ms/step - loss: 5.2217 - categorical_accuracy: 0.1952 - val_loss: 5.1454 - val_categorical_accuracy: 0.2126\n","Epoch 5/30\n","1549/1549 [==============================] - 126s 81ms/step - loss: 5.0448 - categorical_accuracy: 0.2062 - val_loss: 5.0711 - val_categorical_accuracy: 0.2199\n","Epoch 6/30\n","1549/1549 [==============================] - 126s 82ms/step - loss: 4.9016 - categorical_accuracy: 0.2160 - val_loss: 5.0243 - val_categorical_accuracy: 0.2249\n","Epoch 7/30\n","1549/1549 [==============================] - 126s 81ms/step - loss: 4.7834 - categorical_accuracy: 0.2221 - val_loss: 5.0024 - val_categorical_accuracy: 0.2294\n","Epoch 8/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.6823 - categorical_accuracy: 0.2274 - val_loss: 4.9911 - val_categorical_accuracy: 0.2310\n","Epoch 9/30\n","1549/1549 [==============================] - 126s 81ms/step - loss: 4.5920 - categorical_accuracy: 0.2336 - val_loss: 4.9955 - val_categorical_accuracy: 0.2340\n","Epoch 10/30\n","1549/1549 [==============================] - 126s 81ms/step - loss: 4.5115 - categorical_accuracy: 0.2378 - val_loss: 4.9982 - val_categorical_accuracy: 0.2373\n","Epoch 11/30\n","1549/1549 [==============================] - 126s 81ms/step - loss: 4.4402 - categorical_accuracy: 0.2420 - val_loss: 5.0117 - val_categorical_accuracy: 0.2387\n","Epoch 12/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.3746 - categorical_accuracy: 0.2456 - val_loss: 5.0209 - val_categorical_accuracy: 0.2409\n","Epoch 13/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.3167 - categorical_accuracy: 0.2494 - val_loss: 5.0386 - val_categorical_accuracy: 0.2417\n","Epoch 14/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.2658 - categorical_accuracy: 0.2528 - val_loss: 5.0562 - val_categorical_accuracy: 0.2430\n","Epoch 15/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.2190 - categorical_accuracy: 0.2552 - val_loss: 5.0804 - val_categorical_accuracy: 0.2433\n","Epoch 16/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 4.1752 - categorical_accuracy: 0.2578 - val_loss: 5.0968 - val_categorical_accuracy: 0.2441\n","Epoch 17/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 4.1385 - categorical_accuracy: 0.2603 - val_loss: 5.1150 - val_categorical_accuracy: 0.2446\n","Epoch 18/30\n","1549/1549 [==============================] - 125s 80ms/step - loss: 4.0997 - categorical_accuracy: 0.2630 - val_loss: 5.1393 - val_categorical_accuracy: 0.2453\n","Epoch 19/30\n","1549/1549 [==============================] - 123s 79ms/step - loss: 4.0661 - categorical_accuracy: 0.2646 - val_loss: 5.1447 - val_categorical_accuracy: 0.2458\n","Epoch 20/30\n","1549/1549 [==============================] - 123s 79ms/step - loss: 4.0328 - categorical_accuracy: 0.2670 - val_loss: 5.1675 - val_categorical_accuracy: 0.2455\n","Epoch 21/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 4.0009 - categorical_accuracy: 0.2691 - val_loss: 5.1860 - val_categorical_accuracy: 0.2457\n","Epoch 22/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 3.9716 - categorical_accuracy: 0.2712 - val_loss: 5.2147 - val_categorical_accuracy: 0.2464\n","Epoch 23/30\n","1549/1549 [==============================] - 125s 80ms/step - loss: 3.9439 - categorical_accuracy: 0.2735 - val_loss: 5.2240 - val_categorical_accuracy: 0.2467\n","Epoch 24/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.9210 - categorical_accuracy: 0.2746 - val_loss: 5.2412 - val_categorical_accuracy: 0.2476\n","Epoch 25/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.8945 - categorical_accuracy: 0.2769 - val_loss: 5.2618 - val_categorical_accuracy: 0.2468\n","Epoch 26/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.8753 - categorical_accuracy: 0.2789 - val_loss: 5.2721 - val_categorical_accuracy: 0.2477\n","Epoch 27/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.8566 - categorical_accuracy: 0.2797 - val_loss: 5.3064 - val_categorical_accuracy: 0.2469\n","Epoch 28/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.8382 - categorical_accuracy: 0.2813 - val_loss: 5.3309 - val_categorical_accuracy: 0.2464\n","Epoch 29/30\n","1549/1549 [==============================] - 125s 81ms/step - loss: 3.8170 - categorical_accuracy: 0.2833 - val_loss: 5.3460 - val_categorical_accuracy: 0.2469\n","Epoch 30/30\n","1549/1549 [==============================] - 124s 80ms/step - loss: 3.8003 - categorical_accuracy: 0.2842 - val_loss: 5.3546 - val_categorical_accuracy: 0.2468\n","*Model is saved.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'epoch')"]},"metadata":{"tags":[]},"execution_count":7},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"p8-Sq2D6TUFm"},"source":["After 30 epochs, you must see the training data set accuracy was around 28%, while validation set accuracy reached approximately 24.5%. This accuracy looks not as good as what we have seen in previous tasks, such as image classification. But think about it is a 10,000-class classification problem, and also many words are hard to predict, therefore this is not a bad result.  "]},{"cell_type":"markdown","metadata":{"id":"_P6EDqpuCb5r"},"source":["## STEP 6: Display some predicting examples\n","Let's display some next-word predicting examples by picking a 30-word sequence to compare the predicted word against what the actual word.  \n","\n","We first see some examples on the training set.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n08BCKMsWJqB","executionInfo":{"status":"ok","timestamp":1610930979415,"user_tz":0,"elapsed":3802464,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"bc610fa6-0824-4d82-abbc-9891f147901e"},"source":["model_file_name = \"LSTM-2layer-{}-{}.hdf5\".format(num_steps,hidden_size)\n","model = load_model(data_path + \"/\" + model_file_name)\n","\n","import numpy\n","print(\"Prediction example on training set:\")\n","id_start_word = 800 # change it to see other next-word predictions\n","current_seq_words = \"Current sequence: \"\n","for i in range(id_start_word,id_start_word+num_steps):\n","    current_seq_words += reversed_dictionary[train_data[i]] + \" \"\n","actual_word = reversed_dictionary[train_data[id_start_word+num_steps]]\n","print(current_seq_words)\n","print(\"The actual next word: \"+actual_word)\n","\n","current_seq = numpy.array(train_data[id_start_word:id_start_word+num_steps]).reshape(1,num_steps)\n","print(current_seq.shape)\n","prediction = model.predict(current_seq)\n","print(prediction.shape)\n","predict_word_idx = np.argmax(prediction[:, num_steps-1, :])\n","predict_word = reversed_dictionary[predict_word_idx]\n","\n","print(\"The predicted next word: \"+predict_word)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Prediction example on training set:\n","Current sequence: N N for the week ended tuesday <eos> compound yields assume reinvestment of dividends and that the current yield continues for a year <eos> average maturity of the funds ' \n","The actual next word: investments\n","(1, 30)\n","(1, 30, 10000)\n","The predicted next word: investments\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KZxs1nxyHXkE"},"source":["The same prediction can be done with the `test_data` as below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhA-kOR_Ddvn","executionInfo":{"status":"ok","timestamp":1610930980923,"user_tz":0,"elapsed":3803967,"user":{"displayName":"Hongping Cai","photoUrl":"","userId":"07707315300658361348"}},"outputId":"a888beee-e06a-4b8b-9757-0ff42a654332"},"source":["model_file_name = \"LSTM-2layer-{}-{}.hdf5\".format(num_steps,hidden_size)\n","model = load_model(data_path + \"/\" + model_file_name)\n","\n","import numpy\n","print(\"Prediction example on test set:\")\n","id_start_word = 800 # change it to see other next-word predictions\n","current_seq_words = \"Current sequence: \"\n","for i in range(id_start_word,id_start_word+num_steps):\n","    current_seq_words += reversed_dictionary[test_data[i]] + \" \"\n","actual_word = reversed_dictionary[test_data[id_start_word+num_steps]]\n","print(current_seq_words)\n","print(\"The actual next word: \"+actual_word)\n","\n","current_seq = numpy.array(test_data[id_start_word:id_start_word+num_steps]).reshape(1,num_steps)\n","print(current_seq.shape)\n","prediction = model.predict(current_seq)\n","print(prediction.shape)\n","predict_word_idx = np.argmax(prediction[:, num_steps-1, :])\n","predict_word = reversed_dictionary[predict_word_idx]\n","\n","print(\"The predicted next word: \"+predict_word)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Prediction example on test set:\n","Current sequence: signed by the big board and the chicago mercantile exchange trading was temporarily halted in chicago <eos> after the trading halt in the s&p N pit in chicago waves of \n","The actual next word: selling\n","(1, 30)\n","(1, 30, 10000)\n","The predicted next word: the\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WUlbXGRZG2SB"},"source":["If you set `id_strat_word` to 800 as what I did, you may see that the predicted word is \"the\", which does not match the actual word \"selling\". But if you look at the last word of the input sequence is \"of\", a \"the\" is actually a very reasonable prediction.  \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"00MM3UEOOQ2u"},"source":["\n","*{Opitonal task}* In the last step, I only wrote the code for predicting one word. You may change the code to predict more than one word (e.g., 10 words) using the same model, i.e., run the model prediction 10 times with different input sequences as follows.\n","\n","- input: [x0, ... x29]      ==> output: the first predicted word y0\n","- input: [x1, ..., x29, y0] ==> output: the second predicted word y1\n","- ...\n","- input: [x9, ..., x29, y0, ..., y8] ==> output the 10-th predicted word y9"]},{"cell_type":"markdown","metadata":{"id":"IqBPz--jxMS0"},"source":["## Reference\n","[1] Blog: <a href=https://adventuresinmachinelearning.com/keras-lstm-tutorial/> Keras LSTM tutorial - How to easily build a powerful deep learning language model</a>"]}]}