{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpAQinyMCYWc"
      },
      "source": [
        "# **Task 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xvIrKWe_CVTK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MSiYmAdtCyox"
      },
      "outputs": [],
      "source": [
        "# Load Data and flatten the image to a 1D vector\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "n_train = X_train.shape[0]\n",
        "n_test = X_test.shape[0]\n",
        "\n",
        "# Normalise images to be in the range [-1, 1]\n",
        "X_train = X_train / 127.5 - 1\n",
        "X_test = X_test / 127.5 - 1\n",
        "\n",
        "# Convert each 28x28 image into a 784 dimensional vector\n",
        "features_count = np.prod(X_train.shape[1:])\n",
        "X_train_flatened = X_train.reshape(n_train, features_count)\n",
        "X_test_flatened = X_test.reshape(n_test, features_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA plot with Centroids\n",
        "\n",
        "# Reduce the dimensionality of the data to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_flatened)\n",
        "\n",
        "# Create a scatter plot of the PCA data, colored by digit\n",
        "pca_fig = px.scatter(X_train_pca, x=0, y=1, color=y_train, title='PCA plot of the MNIST Dataset', width=1000, height=600)\n",
        "pca_fig.update_layout(xaxis_title='PC1', yaxis_title='PC2')\n",
        "\n",
        "# Create a DataFrame with the PCA data and digit labels\n",
        "df_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])\n",
        "df_pca['digit'] = y_train\n",
        "\n",
        "# Compute centroids for each class by taking the mean of PC1 and PC2\n",
        "centroids = df_pca.groupby('digit')[['PC1', 'PC2']].mean()\n",
        "\n",
        "# Plot the centroids of the pca components\n",
        "centroids_fig = px.scatter(centroids, x='PC1', y='PC2', color=centroids.index, title='PCA plot of the MNIST Dataset', width=1000, height=600)\n",
        "centroids_fig.update_traces(marker=dict(size=20))\n",
        "\n",
        "# Plot\n",
        "pca_fig.show()\n",
        "centroids_fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ubqrTTHqJU"
      },
      "source": [
        "# **Task 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3DYfZDl-HpvN"
      },
      "outputs": [],
      "source": [
        "# Prepare data for binary classification\n",
        "\n",
        "def prepare_data(digit_1, digit_2):\n",
        "\n",
        "    # Convert each 28x28 image into a 784 dimensional vector\n",
        "    features_count = np.prod(X_train.shape[1:])\n",
        "    X_train_flatened = X_train.reshape(n_train, features_count)\n",
        "    X_test_flatened = X_test.reshape(n_test, features_count)\n",
        "\n",
        "    # Filter out for digit_1 and digit_2 for binary classification\n",
        "    cond = (y_train == digit_1) + (y_train == digit_2)\n",
        "    binary_x_train = X_train_flatened[cond, :]\n",
        "    binary_y_train = y_train[cond] * 1.0\n",
        "\n",
        "    # Normalise training labels\n",
        "    binary_y_train[binary_y_train == digit_1] = -1\n",
        "    binary_y_train[binary_y_train == digit_2] = 1\n",
        "\n",
        "    # Filter out for digit_1 and digit_2 for binary classification\n",
        "    cond_test = (y_test == digit_1) + (y_test == digit_2)\n",
        "    binary_x_test = X_test_flatened[cond_test, :]\n",
        "    binary_y_test = y_test[cond_test] * 1.0\n",
        "\n",
        "    # Normalise test labels\n",
        "    binary_y_test[binary_y_test == digit_1] = -1\n",
        "    binary_y_test[binary_y_test == digit_2] = 1\n",
        "\n",
        "    return binary_x_train, binary_y_train, binary_x_test, binary_y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6ByPWz54O032"
      },
      "outputs": [],
      "source": [
        "# Predict function for the perceptron\n",
        "\n",
        "def predict(x, w, b):\n",
        "\n",
        "    # Compute the linear combination for each sample\n",
        "    z = np.dot(x, w) + b\n",
        "\n",
        "    # If z >= 0, predict 1, otherwise predict -1\n",
        "    prediction = np.where(z >= 0, 1, -1)\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1Uk-fuYAO94J"
      },
      "outputs": [],
      "source": [
        "# Run epoch perceptron\n",
        "\n",
        "def run_epoch_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test, num_epochs=100, learning_rate=0.01):\n",
        "\n",
        "    def train_perceptron(x_train, y_train, num_epochs, learning_rate):\n",
        "\n",
        "        # Get the number of samples and features\n",
        "        n_samples, n_features = x_train.shape\n",
        "\n",
        "        # Initialise weights and bias to zero\n",
        "        w = np.zeros(n_features)\n",
        "        b = 0.0\n",
        "\n",
        "        # Lists to store accuracy values\n",
        "        train_accuracies = []\n",
        "        epochs = []\n",
        "\n",
        "        # Batch of stochastic gradient descent\n",
        "        for epoch in range(num_epochs):\n",
        "            for i in range(n_samples):\n",
        "\n",
        "                # Check if the sample is misclassified\n",
        "                if y_train[i] * (np.dot(x_train[i], w) + b) <= 0:\n",
        "\n",
        "                    # Update weights and bias using the perceptron rule\n",
        "                    w += learning_rate * y_train[i] * x_train[i]\n",
        "                    b += learning_rate * y_train[i]\n",
        "\n",
        "            # Evaluate training progress at each epoch\n",
        "            predictions = predict(x_train, w, b)\n",
        "            accuracy = np.mean(predictions == y_train)\n",
        "            train_accuracies.append(accuracy)\n",
        "            epochs.append(epoch + 1)\n",
        "\n",
        "        # Plot accuracy vs epochs\n",
        "        fig = px.line(x=epochs, y=train_accuracies, title='Training Accuracy vs Epochs', labels={'x': 'Epoch', 'y': 'Accuracy'}, width=1000, height=500)\n",
        "        fig.show()\n",
        "\n",
        "        return w, b\n",
        "\n",
        "\n",
        "    # Train the perceptron using the binary training data\n",
        "    w, b = train_perceptron(binary_x_train, binary_y_train, num_epochs, learning_rate)\n",
        "\n",
        "    # Predict on the training data\n",
        "    train_predictions = predict(binary_x_train, w, b)\n",
        "    train_accuracy = np.mean(train_predictions == binary_y_train)\n",
        "    print('Final Training Accuracy:', train_accuracy)\n",
        "\n",
        "    # Predict on the test data\n",
        "    test_predictions = predict(binary_x_test, w, b)\n",
        "    test_accuracy = np.mean(test_predictions == binary_y_test)\n",
        "    print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "    return test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2fZEv5DFPKFj"
      },
      "outputs": [],
      "source": [
        "# Run optimisation perceptron\n",
        "\n",
        "def run_optimisation_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test, max_iters=1000, learning_rate=0.01, tolerance=1e-3):\n",
        "\n",
        "    def optimise_perceptron(x, y, max_iters, learning_rate, tolerance):\n",
        "\n",
        "        # Initialise variables\n",
        "        iter = 0\n",
        "        error = np.inf\n",
        "        error_list = []\n",
        "        n,m = x.shape\n",
        "        rng = np.random.default_rng()\n",
        "        w = rng.random(m)\n",
        "        b = rng.random()\n",
        "\n",
        "        # While the iteration is less than the maximum number of iterations and the error is greater than the tolerance\n",
        "        while (iter <= max_iters) & (error > tolerance):\n",
        "\n",
        "            # Predict all samples\n",
        "            predictions = predict(x, w, b)\n",
        "\n",
        "            # Identify misclassified samples\n",
        "            misclassified_indices = np.where(predictions != y)[0]\n",
        "\n",
        "            # Compute current error (fraction of misclassified samples)\n",
        "            error = len(misclassified_indices) / n\n",
        "            error_list.append(error)\n",
        "\n",
        "            # If no misclassifications, we can stop early\n",
        "            if len(misclassified_indices) == 0:\n",
        "                break\n",
        "\n",
        "            # Update w, b for each misclassified sample\n",
        "            for i in misclassified_indices:\n",
        "                w += learning_rate * y[i] * x[i]\n",
        "                b += learning_rate * y[i]\n",
        "\n",
        "            iter += 1\n",
        "\n",
        "        return w, b, error_list\n",
        "\n",
        "\n",
        "    # Optimise on the training set\n",
        "    w_opt, b_opt, error_list = optimise_perceptron(binary_x_train, binary_y_train, max_iters, learning_rate, tolerance)\n",
        "\n",
        "    # Evaluate on training\n",
        "    train_pred = predict(binary_x_train, w_opt, b_opt)\n",
        "    train_accuracy = np.mean(train_pred == binary_y_train)\n",
        "    print('Final Training Accuracy:', train_accuracy)\n",
        "\n",
        "    # Evaluate on test\n",
        "    test_pred = predict(binary_x_test, w_opt, b_opt)\n",
        "    test_accuracy = np.mean(test_pred == binary_y_test)\n",
        "    print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "    # Error Curve\n",
        "    df_error = pd.DataFrame({'Iteration': list(range(1, len(error_list) + 1)), 'Misclassification Error': error_list})\n",
        "    fig_error = px.line(df_error, x='Iteration', y='Misclassification Error', title='Perceptron Training Error', markers=True, width=1000, height=500)\n",
        "    fig_error.show()\n",
        "\n",
        "    # Visualise the learned weights as an image\n",
        "    w_image = w_opt.reshape(28, 28)\n",
        "    fig_weights = px.imshow(w_image, color_continuous_scale='RdBu', title='Learned Weight Image', width=1000, height=500)\n",
        "    fig_weights.show()\n",
        "\n",
        "    return test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run\n",
        "\n",
        "digits = {'sample_1': (1, 0), 'sample_2': (8, 3), 'sample_3': (4, 9), 'sample_4': (8, 7), 'sample_5': (2, 9)}\n",
        "results = {}\n",
        "\n",
        "for run, (digit_1, digit_2) in enumerate(digits.values()):\n",
        "    print(f'\\n\\nRun: {run + 1 }: -- Training for digits {digit_1} and {digit_2} --\\n\\n')\n",
        "    print(' -- Epoch Perceptron Training --\\n')\n",
        "    binary_x_train, binary_y_train, binary_x_test, binary_y_test = prepare_data(digit_1, digit_2)\n",
        "    epoch_test_accuracy = run_epoch_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test)\n",
        "    print('\\n -- Optimisation Perceptron Training --\\n')\n",
        "    optimisation_test_accuracy = run_optimisation_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test)\n",
        "    results[f'run_{run + 1}'] = {'digit_1': round(digit_1, 0), 'digit_2': round(digit_2, 0), 'epoch_test_accuracy': round(epoch_test_accuracy, 2), 'optimisation_test_accuracy': round(optimisation_test_accuracy, 2)}\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR2o3gu1Lf3f"
      },
      "source": [
        "# **Task 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a_Q2wACSLg7S"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train/255\n",
        "y_train = np.eye(10)[y_train]\n",
        "X_test = X_test/ 255\n",
        "y_test = np.eye(10)[y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jHg3ij4GLpmd"
      },
      "outputs": [],
      "source": [
        "# Plot Training and Testing Accuracy Curves\n",
        "\n",
        "def plot(history):\n",
        "\n",
        "    train_acc = history.history['accuracy'][-1] * 100\n",
        "    test_acc = history.history['val_accuracy'][-1] * 100\n",
        "    print(f\"Training accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
        "    plt.title('Training vs. Testing Accuracy')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "sVIwhi-BLrrc",
        "outputId": "e131f058-2f44-4e91-9b63-48cdffd45606"
      },
      "outputs": [],
      "source": [
        "# Base MLP Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(1000, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=50,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "plot(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPJmIzkdLx8s"
      },
      "outputs": [],
      "source": [
        "# Second MLP Model (Example)\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(500, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=50,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEq7fDNLhhR"
      },
      "source": [
        "# **Task 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rIPnkyg6cU4r"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "\n",
        "def plot(history):\n",
        "\n",
        "    train_acc = history.history['accuracy'][-1] * 100\n",
        "    test_acc = history.history['val_accuracy'][-1] * 100\n",
        "    print(f'Training accuracy: {train_acc:.2f}%')\n",
        "    print(f'Test accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
        "    plt.title('Training vs. Testing Accuracy')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M-BeEGVvxUfx"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "n_train = x_train.shape[0]\n",
        "n_test = x_test.shape[0]\n",
        "\n",
        "# Reshape and normalise the data\n",
        "x_train = x_train.reshape((n_train, 28, 28, 1)) / 255.0\n",
        "x_test = x_test.reshape((n_test, 28, 28, 1)) / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "ApCfEqQoxY_o",
        "outputId": "59810647-4717-4341-9a01-c5099b2f896d"
      },
      "outputs": [],
      "source": [
        "# Base CNN Model\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(4, 4), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
        "    Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
        "    Conv2D(128, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=50, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "plot(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1N9cSvBuID7"
      },
      "source": [
        "# **NAJIB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XmUVZBEZuNQs"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "import math\n",
        "import seaborn as sns\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFtnBx3zuTzq"
      },
      "source": [
        "### TASK 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Drt7tGGBuPMU",
        "outputId": "2a5ee3de-85b5-4e36-8abf-fcbbdbcb5bbb"
      },
      "outputs": [],
      "source": [
        "# Define MNIST rainbow colors for consistency\n",
        "DIGIT_COLORS = [\n",
        "    'red',         # 0\n",
        "    'orange',      # 1\n",
        "    'yellow',      # 2\n",
        "    'green',       # 3\n",
        "    'cyan',        # 4\n",
        "    'blue',        # 5\n",
        "    'indigo',      # 6\n",
        "    'violet',      # 7\n",
        "    'magenta',     # 8\n",
        "    'brown'        # 9\n",
        "]\n",
        "\n",
        "# Load the MNIST Dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Prepare data (using a smaller subset to reduce memory usage)\n",
        "sample_size = 10000  # Use only 10,000 samples to reduce computation\n",
        "indices = np.random.choice(len(x_train), sample_size, replace=False)\n",
        "x_train_subset = x_train[indices]\n",
        "y_train_subset = y_train[indices]\n",
        "\n",
        "# Rescale and reshape\n",
        "x_train_subset = x_train_subset / 127.5 - 1\n",
        "n_train = x_train_subset.shape[0]\n",
        "nb_features = np.prod(x_train_subset.shape[1:])\n",
        "x_train_reshaped = x_train_subset.reshape((n_train, nb_features))\n",
        "\n",
        "# Standardize and apply PCA\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train_reshaped)\n",
        "pca = PCA(n_components=2)\n",
        "x_train_pca = pca.fit_transform(x_train_scaled)\n",
        "\n",
        "# Step 1: Plot centroids\n",
        "def plot_digit_centroids():\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Calculate centroids for each digit\n",
        "    for digit in range(10):\n",
        "        points = x_train_pca[y_train_subset == digit]\n",
        "        if len(points) > 0:\n",
        "            centroid = np.mean(points, axis=0)\n",
        "            plt.scatter(centroid[0], centroid[1],\n",
        "                       s=100, color=DIGIT_COLORS[digit], edgecolor='black', linewidth=1)\n",
        "            plt.text(centroid[0], centroid[1], str(digit),\n",
        "                    ha='center', va='center', color='black', fontweight='bold')\n",
        "\n",
        "    plt.title(\"MNIST PCA - Digit Centroids\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.savefig('digit_centroids.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Step 2: Calculate regression for all digits\n",
        "def calculate_all_regressions():\n",
        "    slopes = {}\n",
        "    vectors = {}\n",
        "\n",
        "    for digit in range(10):\n",
        "        # Extract points for this digit\n",
        "        points = x_train_pca[y_train_subset == digit]\n",
        "        if len(points) > 0:\n",
        "            # Fit regression\n",
        "            reg = LinearRegression()\n",
        "            X = points[:, 0].reshape(-1, 1)\n",
        "            y = points[:, 1]\n",
        "            reg.fit(X, y)\n",
        "\n",
        "            # Store slope and vector\n",
        "            slope = reg.coef_[0]\n",
        "            intercept = reg.intercept_\n",
        "            slopes[digit] = (slope, intercept)\n",
        "\n",
        "            # Create and normalize direction vector\n",
        "            vec = np.array([1, slope])\n",
        "            vectors[digit] = vec / np.linalg.norm(vec)\n",
        "\n",
        "    return slopes, vectors\n",
        "\n",
        "# Step 3: Create angle matrix\n",
        "def create_angle_matrix(vectors):\n",
        "    angle_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(10):\n",
        "        if i not in vectors:\n",
        "            continue\n",
        "\n",
        "        for j in range(10):\n",
        "            if j not in vectors:\n",
        "                continue\n",
        "\n",
        "            # Cosine similarity between direction vectors\n",
        "            cos_sim = np.dot(vectors[i], vectors[j])\n",
        "\n",
        "            # Calculate angle in degrees (0-90 degrees range)\n",
        "            angle = math.degrees(math.acos(min(max(cos_sim, -1.0), 1.0)))\n",
        "\n",
        "            # For proper angle representation, we want the smallest angle between lines\n",
        "            # If angle > 90 degrees, take 180-angle instead (the supplementary angle)\n",
        "            if angle > 90:\n",
        "                angle = 180 - angle\n",
        "\n",
        "            angle_matrix[i, j] = angle\n",
        "\n",
        "    # Create a dataframe for the angle matrix\n",
        "    angle_df = pd.DataFrame(\n",
        "        angle_matrix,\n",
        "        index=[f'Digit {i}' for i in range(10)],\n",
        "        columns=[f'Digit {i}' for i in range(10)]\n",
        "    ).round(1)\n",
        "\n",
        "    return angle_df\n",
        "\n",
        "# Step 4: Plot angle heatmap\n",
        "def plot_angle_heatmap(angle_df):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(angle_df, annot=True, cmap='YlOrRd', vmin=0, vmax=90,\n",
        "               square=True, linewidths=.5, cbar_kws={\"shrink\": .8, \"label\": \"Angle (degrees)\"})\n",
        "    plt.title('Angle Between Digit Regression Lines (degrees)', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('angle_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Step 5: Plot example pairs (only a few to avoid overload)\n",
        "def plot_example_pairs(slopes, selected_pairs=[(0,1), (3,8), (4,9)]):\n",
        "    for digit1, digit2 in selected_pairs:\n",
        "        # Get points for each digit\n",
        "        points1 = x_train_pca[y_train_subset == digit1]\n",
        "        points2 = x_train_pca[y_train_subset == digit2]\n",
        "\n",
        "        if len(points1) == 0 or len(points2) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get regression parameters\n",
        "        slope1, intercept1 = slopes[digit1]\n",
        "        slope2, intercept2 = slopes[digit2]\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Plot the points\n",
        "        plt.scatter(points1[:, 0], points1[:, 1], color=DIGIT_COLORS[digit1],\n",
        "                   alpha=0.3, s=10, label=f'Digit {digit1}')\n",
        "        plt.scatter(points2[:, 0], points2[:, 1], color=DIGIT_COLORS[digit2],\n",
        "                   alpha=0.3, s=10, label=f'Digit {digit2}')\n",
        "\n",
        "        # Add regression lines\n",
        "        x1_range = np.linspace(points1[:, 0].min(), points1[:, 0].max(), 100)\n",
        "        y1_pred = slope1 * x1_range + intercept1\n",
        "        plt.plot(x1_range, y1_pred, color='black', linewidth=2)\n",
        "\n",
        "        x2_range = np.linspace(points2[:, 0].min(), points2[:, 0].max(), 100)\n",
        "        y2_pred = slope2 * x2_range + intercept2\n",
        "        plt.plot(x2_range, y2_pred, color='black', linewidth=2)\n",
        "\n",
        "        # Calculate vector similarity\n",
        "        vec1 = np.array([1, slope1])\n",
        "        vec2 = np.array([1, slope2])\n",
        "\n",
        "        norm_vec1 = vec1 / np.linalg.norm(vec1)\n",
        "        norm_vec2 = vec2 / np.linalg.norm(vec2)\n",
        "\n",
        "        cos_sim = np.dot(norm_vec1, norm_vec2)\n",
        "        angle = math.degrees(math.acos(min(max(cos_sim, -1.0), 1.0)))\n",
        "\n",
        "        # Take the smaller angle if > 90 degrees\n",
        "        if angle > 90:\n",
        "            angle = 180 - angle\n",
        "\n",
        "        # Add annotation with angle info\n",
        "        plt.annotate(f'Angle between lines: {angle:.1f}°',\n",
        "                    xy=(0.05, 0.95), xycoords='axes fraction',\n",
        "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
        "\n",
        "        plt.title(f'Regression Comparison: Digit {digit1} vs Digit {digit2}')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "        plt.savefig(f'comparison_{digit1}_{digit2}.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# Run all steps quietly\n",
        "try:\n",
        "    plot_digit_centroids()\n",
        "    slopes, vectors = calculate_all_regressions()\n",
        "    angle_df = create_angle_matrix(vectors)\n",
        "    plot_angle_heatmap(angle_df)\n",
        "    plot_example_pairs(slopes)\n",
        "except Exception as e:\n",
        "    pass  # Silently continue on errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWeM_VjZBD1r"
      },
      "source": [
        "### TASK 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "1sAKSVJ4wr39",
        "outputId": "d96ef89e-e73d-4c0a-a414-cacc8a8b836f"
      },
      "outputs": [],
      "source": [
        "# Task 2.1: Implement the predict function for the perceptron\n",
        "def predict(x, w, b):\n",
        "    \"\"\"\n",
        "    Predict class labels for samples in x using the perceptron model.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): Input data with shape (n_samples, n_features)\n",
        "    w (numpy.ndarray): Weight vector with shape (n_features,)\n",
        "    b (float): Bias term\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Predicted class labels (-1 or 1)\n",
        "    \"\"\"\n",
        "    # Calculate the dot product of inputs and weights\n",
        "    z = np.dot(x, w) + b\n",
        "\n",
        "    # Apply the sign function to get predictions\n",
        "    # (>= 0 maps to 1, < 0 maps to -1)\n",
        "    prediction = np.sign(z)\n",
        "\n",
        "    # Replace 0 with 1 if any (though unlikely in practice)\n",
        "    prediction[prediction == 0] = 1\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Task 2.2: Implement the optimize function for training the perceptron\n",
        "def optimize(x, y, w=None, b=None, max_iter=1000, tol=1e-3, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Train a perceptron model using the perceptron learning algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): Training data with shape (n_samples, n_features)\n",
        "    y (numpy.ndarray): Target values with shape (n_samples,)\n",
        "    w (numpy.ndarray, optional): Initial weight vector. If None, initialized randomly.\n",
        "    b (float, optional): Initial bias term. If None, initialized randomly.\n",
        "    max_iter (int): Maximum number of iterations\n",
        "    tol (float): Tolerance for stopping criterion\n",
        "    learning_rate (float): Learning rate for weight updates (to control weight magnitude)\n",
        "\n",
        "    Returns:\n",
        "    tuple: (w, b, error_history)\n",
        "        - w: Learned weight vector\n",
        "        - b: Learned bias term\n",
        "        - error_history: List of errors at each iteration\n",
        "    \"\"\"\n",
        "    # Get the shape of the input data\n",
        "    n, m = x.shape\n",
        "\n",
        "    # Initialize weights and bias if not provided\n",
        "    if w is None:\n",
        "        w = np.random.randn(m) * 0.01  # Scale down initial weights\n",
        "    if b is None:\n",
        "        b = np.random.randn() * 0.01  # Scale down initial bias\n",
        "\n",
        "    # Initialize variables\n",
        "    iter_count = 0\n",
        "    error_history = []\n",
        "    error = float('inf')\n",
        "\n",
        "    # Main training loop\n",
        "    while iter_count < max_iter and error > tol:\n",
        "        # Get current predictions\n",
        "        y_pred = predict(x, w, b)\n",
        "\n",
        "        # Calculate misclassification error\n",
        "        error = np.mean(y_pred != y)\n",
        "        error_history.append(error)\n",
        "\n",
        "        # If error is below tolerance, break the loop\n",
        "        if error <= tol:\n",
        "            break\n",
        "\n",
        "        # Loop through each training sample\n",
        "        for i in range(n):\n",
        "            # If misclassified, update weights and bias\n",
        "            if y_pred[i] != y[i]:\n",
        "                # Use learning rate to scale updates\n",
        "                w = w + learning_rate * y[i] * x[i]\n",
        "                b = b + learning_rate * y[i]\n",
        "\n",
        "        # Optional: Add weight normalization to prevent excessive growth\n",
        "        if iter_count % 50 == 0 and iter_count > 0:\n",
        "            # Print stats for debugging\n",
        "            if iter_count % 200 == 0:\n",
        "                print(f\"Iteration {iter_count}, Error: {error:.4f}, Weight norm: {np.linalg.norm(w):.4f}\")\n",
        "\n",
        "        iter_count += 1\n",
        "\n",
        "    print(f\"Training completed after {iter_count} iterations\")\n",
        "    print(f\"Final error: {error:.4f}\")\n",
        "    print(f\"Weight norm: {np.linalg.norm(w):.4f}\")\n",
        "\n",
        "    return w, b, error_history\n",
        "\n",
        "# Function to evaluate perceptron on test data\n",
        "def evaluate_perceptron(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Train a perceptron and evaluate it on test data.\n",
        "\n",
        "    Parameters:\n",
        "    x_train, y_train: Training data and labels\n",
        "    x_test, y_test: Test data and labels\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary with training and test accuracy, learned weights, and error history\n",
        "    \"\"\"\n",
        "    # Train the perceptron\n",
        "    w, b, error_history = optimize(x_train, y_train)\n",
        "\n",
        "    # Evaluate on training data\n",
        "    y_train_pred = predict(x_train, w, b)\n",
        "    train_accuracy = np.mean(y_train_pred == y_train)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_test_pred = predict(x_test, w, b)\n",
        "    test_accuracy = np.mean(y_test_pred == y_test)\n",
        "\n",
        "    return {\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'weights': w,\n",
        "        'bias': b,\n",
        "        'error_history': error_history\n",
        "    }\n",
        "\n",
        "# Function to visualize learned weights as an image\n",
        "def visualize_weights(w, shape=(28, 28), digit_pair=None):\n",
        "    \"\"\"\n",
        "    Visualize the learned weights as an image.\n",
        "\n",
        "    Parameters:\n",
        "    w (numpy.ndarray): Weight vector\n",
        "    shape (tuple): Shape to reshape the weight vector to\n",
        "    digit_pair (tuple): The pair of digits being classified\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Reshape weights to original image dimensions\n",
        "    weight_img = w.reshape(shape)\n",
        "\n",
        "    # Print statistics about the weights\n",
        "    print(f\"Weight statistics:\")\n",
        "    print(f\"  Mean: {w.mean():.4f}\")\n",
        "    print(f\"  Min: {w.min():.4f}\")\n",
        "    print(f\"  Max: {w.max():.4f}\")\n",
        "    print(f\"  Standard deviation: {w.std():.4f}\")\n",
        "\n",
        "    # Main subplot for combined weights\n",
        "    plt.subplot(1, 2, 1)\n",
        "    im = plt.imshow(weight_img, cmap='viridis')\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.title(f'Learned Weights for Digits {digit_pair[0]} vs {digit_pair[1]}')\n",
        "\n",
        "    # Plot composite image of both positive and negative weights\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Create an RGB image showing both positive and negative weights\n",
        "    composite = np.zeros((*shape, 3))\n",
        "    # Get positive and negative weights\n",
        "    pos_weights = weight_img.copy()\n",
        "    neg_weights = weight_img.copy()\n",
        "    pos_weights[pos_weights < 0] = 0\n",
        "    neg_weights[neg_weights > 0] = 0\n",
        "    neg_weights = abs(neg_weights)  # Make negative weights positive for visualization\n",
        "\n",
        "    # Normalize for visualization\n",
        "    if pos_weights.max() > 0:\n",
        "        composite[:,:,0] = pos_weights / pos_weights.max()  # Red channel for positive\n",
        "    if neg_weights.max() > 0:\n",
        "        composite[:,:,2] = neg_weights / neg_weights.max()  # Blue channel for negative\n",
        "\n",
        "    plt.imshow(composite)\n",
        "    plt.title(f'Composite: Blue={digit_pair[0]}, Red={digit_pair[1]}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Also show some examples of both digits for reference\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.suptitle(f'Example Images of Digits {digit_pair[0]} and {digit_pair[1]}')\n",
        "\n",
        "    # Show examples of digit1\n",
        "    for i in range(5):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        idx = np.where(y_train == digit_pair[0])[0][i]\n",
        "        plt.imshow(x_train[idx], cmap='Blues')\n",
        "        plt.title(f'Digit {digit_pair[0]}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Show examples of digit2\n",
        "    for i in range(5):\n",
        "        plt.subplot(2, 5, i+6)\n",
        "        idx = np.where(y_train == digit_pair[1])[0][i]\n",
        "        plt.imshow(x_train[idx], cmap='Reds')\n",
        "        plt.title(f'Digit {digit_pair[1]}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to run experiments on digit pairs\n",
        "def run_digit_pair_experiments(digit_pairs):\n",
        "    \"\"\"\n",
        "    Run perceptron experiments on multiple digit pairs.\n",
        "\n",
        "    Parameters:\n",
        "    digit_pairs (list): List of tuples containing digit pairs to classify\n",
        "\n",
        "    Returns:\n",
        "    dict: Results for each digit pair\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for digit1, digit2 in digit_pairs:\n",
        "        print(f\"Training perceptron for digit pair ({digit1}, {digit2})...\")\n",
        "\n",
        "        # Create binary classification dataset - USE SUBSET DATA FOR CONSISTENCY\n",
        "        cond = (y_train_subset == digit1) | (y_train_subset == digit2)\n",
        "        binary_x_train = x_train_reshaped[cond]\n",
        "        binary_y_train = y_train_subset[cond].copy()\n",
        "\n",
        "        # Convert to binary labels (-1, 1) - fixing the data type issue\n",
        "        binary_y_train = binary_y_train.astype(float)  # Convert to float to avoid uint8 overflow\n",
        "        binary_y_train[binary_y_train == digit1] = -1\n",
        "        binary_y_train[binary_y_train == digit2] = 1\n",
        "\n",
        "        # Create test dataset\n",
        "        cond_test = (y_test == digit1) | (y_test == digit2)\n",
        "        binary_x_test = x_test_reshaped[cond_test]\n",
        "        binary_y_test = y_test[cond_test].copy().astype(float)  # Convert to float\n",
        "        binary_y_test[binary_y_test == digit1] = -1\n",
        "        binary_y_test[binary_y_test == digit2] = 1\n",
        "\n",
        "        # Train and evaluate\n",
        "        result = evaluate_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test)\n",
        "        results[f\"{digit1}_vs_{digit2}\"] = result\n",
        "\n",
        "        print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
        "        print(f\"Test accuracy: {result['test_accuracy']:.4f}\")\n",
        "        print(\"---\")\n",
        "\n",
        "        # Plot training error curve\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(result['error_history'])\n",
        "        plt.title(f'Training Error Curve for Digits {digit1} vs {digit2}')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Classification Error')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # Visualize learned weights\n",
        "        visualize_weights(result['weights'], digit_pair=(digit1, digit2))\n",
        "\n",
        "    return results\n",
        "# List of digit pairs to experiment with\n",
        "digit_pairs = [(0, 1), (3, 8), (4, 9), (5, 6), (1, 7)]\n",
        "\n",
        "# Run experiments\n",
        "results = run_digit_pair_experiments(digit_pairs)\n",
        "\n",
        "# Create results table\n",
        "pairs = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "iterations = []\n",
        "\n",
        "for pair, result in results.items():\n",
        "    pairs.append(pair)\n",
        "    train_accs.append(result['train_accuracy'])\n",
        "    test_accs.append(result['test_accuracy'])\n",
        "    iterations.append(len(result['error_history']))\n",
        "\n",
        "# Create and display a comprehensive table\n",
        "results_df = pd.DataFrame({\n",
        "    'Digit Pair': pairs,\n",
        "    'Training Accuracy': train_accs,\n",
        "    'Test Accuracy': test_accs,\n",
        "    'Iterations': iterations\n",
        "})\n",
        "print(\"\\nPerceptron Classification Results for Different Digit Pairs:\")\n",
        "print(results_df)\n",
        "\n",
        "# Create a visualization of the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(pairs))\n",
        "\n",
        "plt.bar(index, train_accs, bar_width, label='Training Accuracy', color='skyblue')\n",
        "plt.bar(index + bar_width, test_accs, bar_width, label='Test Accuracy', color='orange')\n",
        "\n",
        "plt.xlabel('Digit Pairs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Perceptron Performance Across Different Digit Pairs')\n",
        "plt.xticks(index + bar_width/2, pairs)\n",
        "plt.legend()\n",
        "plt.ylim(0.5, 1.05)  # Setting a reasonable y-axis range\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for i, v in enumerate(train_accs):\n",
        "    plt.text(i - 0.1, v + 0.02, f'{v:.3f}', color='blue', fontweight='bold')\n",
        "\n",
        "for i, v in enumerate(test_accs):\n",
        "    plt.text(i + bar_width - 0.1, v + 0.02, f'{v:.3f}', color='darkred', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl0vy5raBAJN"
      },
      "source": [
        "### TASK 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "RlqJHbNWA_WE",
        "outputId": "ac326758-de37-4f3b-eb08-7daec5538dc9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">785,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │       \u001b[38;5;34m785,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │     \u001b[38;5;34m1,001,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m10,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,796,010</span> (6.85 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,796,010\u001b[0m (6.85 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,796,010</span> (6.85 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,796,010\u001b[0m (6.85 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9058 - loss: 0.2993 - val_accuracy: 0.9630 - val_loss: 0.1134\n",
            "Epoch 2/10\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9757 - loss: 0.0754 - val_accuracy: 0.9751 - val_loss: 0.0814\n",
            "Epoch 3/10\n",
            "\u001b[1m 900/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9833 - loss: 0.0521"
          ]
        }
      ],
      "source": [
        "# Task 3.1: Create and train MLP with architecture [784,1000,1000,10]\n",
        "def create_mlp(input_shape=784, hidden_units=[1000, 1000], output_units=10):\n",
        "    \"\"\"\n",
        "    Create a Multi-Layer Perceptron with the specified architecture.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape (int): Number of input features\n",
        "    hidden_units (list): List of hidden layer units\n",
        "    output_units (int): Number of output units\n",
        "\n",
        "    Returns:\n",
        "    keras.Model: The compiled MLP model\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.InputLayer(input_shape=(input_shape,)))\n",
        "\n",
        "    # Hidden layers with ReLU activation\n",
        "    for units in hidden_units:\n",
        "        model.add(layers.Dense(units, activation='relu'))\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    model.add(layers.Dense(output_units, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the MLP model with architecture [784,1000,1000,10]\n",
        "mlp_model = create_mlp(input_shape=nb_features, hidden_units=[1000, 1000], output_units=10)\n",
        "\n",
        "# Print model summary\n",
        "mlp_model.summary()\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 50\n",
        "epochs = 10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train/255\n",
        "y_train = np.eye(10)[y_train]\n",
        "X_test = X_test/ 255\n",
        "y_test = np.eye(10)[y_test]\n",
        "\n",
        "# Reshape the data for the MLP\n",
        "x_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
        "x_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Train the model\n",
        "history = mlp_model.fit(\n",
        "    x_train_reshaped, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test_reshaped, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = mlp_model.evaluate(x_test_reshaped, y_test, verbose=0)\n",
        "train_loss, train_accuracy = mlp_model.evaluate(x_train_reshaped, y_train, verbose=0)\n",
        "\n",
        "print(f\"\\nTraining accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Plot the training and testing curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 3.2: Implementing multiple MLPs with different depths\n",
        "def create_and_train_mlp(name, hidden_layers, epochs=10, batch_size=50):\n",
        "    \"\"\"\n",
        "    Create and train an MLP with the specified number of hidden layers.\n",
        "\n",
        "    Parameters:\n",
        "    name (str): Name for the model\n",
        "    hidden_layers (list): List of hidden layer units\n",
        "    epochs (int): Number of epochs to train for\n",
        "    batch_size (int): Batch size for training\n",
        "\n",
        "    Returns:\n",
        "    dict: Results including accuracy, model, and parameters count\n",
        "    \"\"\"\n",
        "    model = models.Sequential(name=name)\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.InputLayer(input_shape=(nb_features,)))\n",
        "\n",
        "    # Hidden layers with ReLU activation\n",
        "    for units in hidden_layers:\n",
        "        model.add(layers.Dense(units, activation='relu'))\n",
        "\n",
        "    # Output layer with softmax activation\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train_reshaped, y_train_one_hot,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test_reshaped, y_test_one_hot),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test_one_hot, verbose=0)\n",
        "    train_loss, train_accuracy = model.evaluate(x_train_reshaped, y_train_one_hot, verbose=0)\n",
        "\n",
        "    # Count parameters\n",
        "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'parameters': trainable_params,\n",
        "        'model': model,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# Define MLP architectures with different depths\n",
        "# Original MLP: [784, 1000, 1000, 10] (2 hidden layers)\n",
        "# Additional MLPs with 3, 4, 5, and 7 hidden layers\n",
        "mlp_architectures = {\n",
        "    'MLP-2': [1000, 1000],  # 2 hidden layers (original)\n",
        "    'MLP-3': [800, 800, 800],  # 3 hidden layers\n",
        "    'MLP-4': [700, 700, 700, 700],  # 4 hidden layers\n",
        "    'MLP-5': [600, 600, 600, 600, 600],  # 5 hidden layers\n",
        "    'MLP-7': [500, 500, 500, 500, 500, 500, 500]  # 7 hidden layers\n",
        "}\n",
        "\n",
        "# Train all MLPs\n",
        "results = {}\n",
        "for name, hidden_layers in mlp_architectures.items():\n",
        "    print(f\"Training {name} with architecture {hidden_layers}...\")\n",
        "    results[name] = create_and_train_mlp(name, hidden_layers, epochs=10, batch_size=50)\n",
        "    print(f\"  Train accuracy: {results[name]['train_accuracy']:.4f}\")\n",
        "    print(f\"  Test accuracy: {results[name]['test_accuracy']:.4f}\")\n",
        "    print(f\"  Parameters: {results[name]['parameters']:,}\")\n",
        "    print()\n",
        "\n",
        "# Create a comparison table\n",
        "results_table = {\n",
        "    'MLP': [],\n",
        "    'Hidden Layers': [],\n",
        "    'Parameters': [],\n",
        "    'Train Accuracy': [],\n",
        "    'Test Accuracy': []\n",
        "}\n",
        "\n",
        "for name, result in results.items():\n",
        "    results_table['MLP'].append(name)\n",
        "    results_table['Hidden Layers'].append(len(mlp_architectures[name]))\n",
        "    results_table['Parameters'].append(result['parameters'])\n",
        "    results_table['Train Accuracy'].append(result['train_accuracy'])\n",
        "    results_table['Test Accuracy'].append(result['test_accuracy'])\n",
        "\n",
        "# Plot accuracy vs depth vs parameters\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy vs. Number of Hidden Layers\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results_table['Hidden Layers'], results_table['Train Accuracy'], 'o-', label='Train Accuracy')\n",
        "plt.plot(results_table['Hidden Layers'], results_table['Test Accuracy'], 's-', label='Test Accuracy')\n",
        "plt.xlabel('Number of Hidden Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Hidden Layers')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(results_table['Hidden Layers'])\n",
        "\n",
        "# Plot 2: Accuracy vs. Number of Parameters\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results_table['Parameters'], results_table['Train Accuracy'], 'o-', label='Train Accuracy')\n",
        "plt.plot(results_table['Parameters'], results_table['Test Accuracy'], 's-', label='Test Accuracy')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Parameters')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the final results\n",
        "for i in range(len(results_table['MLP'])):\n",
        "    print(f\"{results_table['MLP'][i]}: Layers={results_table['Hidden Layers'][i]}, \"\n",
        "          f\"Params={results_table['Parameters'][i]:,}, \"\n",
        "          f\"Train Acc={results_table['Train Accuracy'][i]:.4f}, \"\n",
        "          f\"Test Acc={results_table['Test Accuracy'][i]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8jAB2L4EBwW"
      },
      "source": [
        "### TASK 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "2OAjOfo-EDI6",
        "outputId": "ff3eb758-bebb-4d1b-f7b5-36b78d9dbba9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_test_reshaped' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-8d1326e232e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Reshape the data for CNN (assuming x_train_reshaped and x_test_reshaped are your vectorized data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mx_train_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_for_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m def train_cnn_model(x_train_cnn, y_train_one_hot, x_test_cnn, y_test_one_hot,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_test_reshaped' is not defined"
          ]
        }
      ],
      "source": [
        "# Task 4.1: Create a CNN with architecture [32, 64, 128]\n",
        "\n",
        "def create_cnn(input_shape=(28, 28, 1), filters=[32, 64, 128], output_units=10):\n",
        "    \"\"\"\n",
        "    Create a Convolutional Neural Network with the specified architecture.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape (tuple): Shape of input images\n",
        "    filters (list): List of filters for each convolutional layer\n",
        "    output_units (int): Number of output units\n",
        "\n",
        "    Returns:\n",
        "    keras.Model: The compiled CNN model\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First convolutional layer with stride 1\n",
        "    model.add(layers.Conv2D(filters[0], kernel_size=(4, 4), strides=(1, 1), padding='same',\n",
        "                           activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # Second convolutional layer with stride 2\n",
        "    model.add(layers.Conv2D(filters[1], kernel_size=(4, 4), strides=(2, 2), padding='same',\n",
        "                           activation='relu'))\n",
        "\n",
        "    # Third convolutional layer with stride 2\n",
        "    model.add(layers.Conv2D(filters[2], kernel_size=(4, 4), strides=(2, 2), padding='same',\n",
        "                           activation='relu'))\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully connected layer with 10 output units and softmax activation\n",
        "    model.add(layers.Dense(output_units, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Reshape the data to have the correct format for CNN\n",
        "def prepare_data_for_cnn(x_train, x_test):\n",
        "    # Calculate the edge of the square images\n",
        "    edge = int(np.sqrt(x_train.shape[1]))\n",
        "\n",
        "    # Reshape the data to 4D tensors [samples, height, width, channels]\n",
        "    x_train_cnn = x_train.reshape(x_train.shape[0], edge, edge, 1)\n",
        "    x_test_cnn = x_test.reshape(x_test.shape[0], edge, edge, 1)\n",
        "\n",
        "    return x_train_cnn, x_test_cnn\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Reshape the data for CNN (assuming x_train_reshaped and x_test_reshaped are your vectorized data)\n",
        "x_train_cnn, x_test_cnn = prepare_data_for_cnn(x_train_reshaped, x_test_reshaped)\n",
        "\n",
        "def train_cnn_model(x_train_cnn, y_train_one_hot, x_test_cnn, y_test_one_hot,\n",
        "                   filters=[32, 64, 128], batch_size=50, epochs=10):\n",
        "    \"\"\"\n",
        "    Create, train and evaluate a CNN model on MNIST data.\n",
        "\n",
        "    Parameters:\n",
        "    x_train_cnn (numpy.ndarray): Training images, shaped for CNN\n",
        "    y_train_one_hot (numpy.ndarray): One-hot encoded training labels\n",
        "    x_test_cnn (numpy.ndarray): Test images, shaped for CNN\n",
        "    y_test_one_hot (numpy.ndarray): One-hot encoded test labels\n",
        "    filters (list): List of filters for each convolutional layer\n",
        "    batch_size (int): Batch size for training\n",
        "    epochs (int): Number of epochs to train for\n",
        "\n",
        "    Returns:\n",
        "    dict: Results including model, history, and accuracies\n",
        "    \"\"\"\n",
        "    # Create the model\n",
        "    model = create_cnn(input_shape=x_train_cnn.shape[1:], filters=filters)\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train_cnn, y_train_one_hot,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test_cnn, y_test_one_hot),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_loss, train_accuracy = model.evaluate(x_train_cnn, y_train_one_hot, verbose=0)\n",
        "    test_loss, test_accuracy = model.evaluate(x_test_cnn, y_test_one_hot, verbose=0)\n",
        "\n",
        "    print(f\"\\nTraining accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy\n",
        "    }\n",
        "\n",
        "def plot_learning_curves(history):\n",
        "    \"\"\"\n",
        "    Plot the training and validation accuracy and loss curves.\n",
        "\n",
        "    Parameters:\n",
        "    history: Training history object returned by model.fit()\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Task 4.2: Compare CNNs with different depths and widths\n",
        "\n",
        "def create_and_train_cnn(name, filters, batch_size=50, epochs=10):\n",
        "    \"\"\"\n",
        "    Create and train a CNN with specified filter configuration.\n",
        "\n",
        "    Parameters:\n",
        "    name (str): Name for the model\n",
        "    filters (list): List of filters for each convolutional layer\n",
        "    batch_size (int): Batch size for training\n",
        "    epochs (int): Number of epochs to train for\n",
        "\n",
        "    Returns:\n",
        "    dict: Results including accuracy, model, and parameters count\n",
        "    \"\"\"\n",
        "    # Create the model\n",
        "    model = models.Sequential(name=name)\n",
        "\n",
        "    # Add convolutional layers with appropriate strides\n",
        "    model.add(layers.Conv2D(filters[0], kernel_size=(4, 4), strides=(1, 1), padding='same',\n",
        "                           activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "    for i, f in enumerate(filters[1:], 1):\n",
        "        model.add(layers.Conv2D(f, kernel_size=(4, 4), strides=(2, 2), padding='same',\n",
        "                               activation='relu'))\n",
        "\n",
        "    # Flatten and add fully connected layer\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train_cnn, y_train_one_hot,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test_cnn, y_test_one_hot),\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_loss, train_accuracy = model.evaluate(x_train_cnn, y_train_one_hot, verbose=0)\n",
        "    test_loss, test_accuracy = model.evaluate(x_test_cnn, y_test_one_hot, verbose=0)\n",
        "\n",
        "    # Count parameters\n",
        "    trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'parameters': trainable_params,\n",
        "        'model': model,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# Define CNN architectures with different depths and widths\n",
        "cnn_architectures = {\n",
        "    'CNN-3': [32, 64, 128],  # 3 layers (original)\n",
        "    'CNN-4': [32, 64, 96, 128],  # 4 layers\n",
        "    'CNN-5': [32, 48, 64, 96, 128],  # 5 layers\n",
        "    'CNN-2': [64, 128],  # 2 layers\n",
        "    'CNN-6': [16, 32, 48, 64, 96, 128]  # 6 layers\n",
        "}\n",
        "\n",
        "# Code to execute the CNN training and comparison\n",
        "# Run this section to train all CNNs with different architectures\n",
        "# (Make sure you've defined x_train_cnn, y_train_one_hot, x_test_cnn, y_test_one_hot)\n",
        "\n",
        "# Train base CNN model\n",
        "cnn_result = train_cnn_model(x_train_cnn, y_train_one_hot, x_test_cnn, y_test_one_hot)\n",
        "plot_learning_curves(cnn_result['history'])\n",
        "\n",
        "# Train all CNN architectures\n",
        "results = {}\n",
        "for name, filters in cnn_architectures.items():\n",
        "    print(f\"Training {name} with filters {filters}...\")\n",
        "    results[name] = create_and_train_cnn(name, filters, epochs=10, batch_size=50)\n",
        "    print(f\"  Train accuracy: {results[name]['train_accuracy']:.4f}\")\n",
        "    print(f\"  Test accuracy: {results[name]['test_accuracy']:.4f}\")\n",
        "    print(f\"  Parameters: {results[name]['parameters']:,}\")\n",
        "    print()\n",
        "\n",
        "# Create a comparison table\n",
        "results_table = {\n",
        "    'CNN': [],\n",
        "    'Layers': [],\n",
        "    'Parameters': [],\n",
        "    'Train Accuracy': [],\n",
        "    'Test Accuracy': []\n",
        "}\n",
        "\n",
        "for name, result in results.items():\n",
        "    results_table['CNN'].append(name)\n",
        "    results_table['Layers'].append(len(cnn_architectures[name]))\n",
        "    results_table['Parameters'].append(result['parameters'])\n",
        "    results_table['Train Accuracy'].append(result['train_accuracy'])\n",
        "    results_table['Test Accuracy'].append(result['test_accuracy'])\n",
        "\n",
        "# Plot accuracy vs depth vs parameters\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Accuracy vs. Number of Layers\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results_table['Layers'], results_table['Train Accuracy'], 'o-', label='Train Accuracy')\n",
        "plt.plot(results_table['Layers'], results_table['Test Accuracy'], 's-', label='Test Accuracy')\n",
        "plt.xlabel('Number of Convolutional Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Convolutional Layers')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(results_table['Layers'])\n",
        "\n",
        "# Plot 2: Accuracy vs. Number of Parameters\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results_table['Parameters'], results_table['Train Accuracy'], 'o-', label='Train Accuracy')\n",
        "plt.plot(results_table['Parameters'], results_table['Test Accuracy'], 's-', label='Test Accuracy')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. Number of Parameters')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print the final results\n",
        "for i in range(len(results_table['CNN'])):\n",
        "    print(f\"{results_table['CNN'][i]}: Layers={results_table['Layers'][i]}, \"\n",
        "          f\"Params={results_table['Parameters'][i]:,}, \"\n",
        "          f\"Train Acc={results_table['Train Accuracy'][i]:.4f}, \"\n",
        "          f\"Test Acc={results_table['Test Accuracy'][i]:.4f}\")\n",
        "\n",
        "# Compare CNN to MLP\n",
        "print(\"\\nComparison between CNN and MLP:\")\n",
        "print(f\"CNN-3 (Original): Test Accuracy={results['CNN-3']['test_accuracy']:.4f}, Parameters={results['CNN-3']['parameters']:,}\")\n",
        "print(f\"MLP-2 (Original): Test Accuracy={mlp_results['MLP-2']['test_accuracy']:.4f}, Parameters={mlp_results['MLP-2']['parameters']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkjJY7PQLlnm"
      },
      "source": [
        "### TASK 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "y5pdyRUnLnXf"
      },
      "outputs": [],
      "source": [
        "# Task 5: Visualizing CNN outcomes\n",
        "\n",
        "def plot_filters(model, layer_idx, cols=8):\n",
        "    \"\"\"\n",
        "    Plot the filters (kernels) of a specific convolutional layer in a grid.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained CNN model\n",
        "    layer_idx: Index of the convolutional layer to visualize\n",
        "    cols: Number of columns in the grid plot\n",
        "    \"\"\"\n",
        "    # Get the layer\n",
        "    layer = model.layers[layer_idx]\n",
        "\n",
        "    # Check if it's a convolutional layer\n",
        "    if not isinstance(layer, layers.Conv2D):\n",
        "        print(f\"Layer {layer_idx} is not a convolutional layer.\")\n",
        "        return\n",
        "\n",
        "    # Get the weights (filters/kernels)\n",
        "    filters, biases = layer.get_weights()\n",
        "\n",
        "    # Number of filters, filter size\n",
        "    n_filters, height, width, channels = filters.shape\n",
        "\n",
        "    # Calculate rows needed\n",
        "    rows = int(np.ceil(n_filters / cols))\n",
        "\n",
        "    # Create a figure\n",
        "    plt.figure(figsize=(cols * 2, rows * 2))\n",
        "\n",
        "    # Plot each filter\n",
        "    for i in range(n_filters):\n",
        "        # Create subplot for this filter\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "        # For each input channel, we have a separate kernel\n",
        "        # We'll visualize the average across channels to get a sense of the overall pattern\n",
        "        filter_img = np.mean(filters[i, :, :, :], axis=2)\n",
        "\n",
        "        # Normalize for better visualization\n",
        "        filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min() + 1e-7)\n",
        "\n",
        "        # Display the filter\n",
        "        plt.imshow(filter_img, cmap='viridis')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Filter {i+1}')\n",
        "\n",
        "    plt.suptitle(f'Filters from layer {layer.name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_activation_maps(model, image, layer_indices, digit_class, cols=8):\n",
        "    \"\"\"\n",
        "    Plot the activation maps for a specific image for each filter in specified layers.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained CNN model\n",
        "    image: Input image to visualize activations for (should be shaped for CNN)\n",
        "    layer_indices: List of layer indices to visualize\n",
        "    digit_class: The class of the digit for display\n",
        "    cols: Number of columns in each grid plot\n",
        "    \"\"\"\n",
        "    # Add batch dimension if needed\n",
        "    if len(image.shape) == 3:\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Create models that output the activations for each specified layer\n",
        "    activation_models = []\n",
        "    for layer_idx in layer_indices:\n",
        "        layer = model.layers[layer_idx]\n",
        "        activation_model = Model(inputs=model.input, outputs=layer.output)\n",
        "        activation_models.append((layer_idx, layer.name, activation_model))\n",
        "\n",
        "    # Display the input image\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(np.squeeze(image), cmap='gray')\n",
        "    plt.title(f'Input Image: Digit {digit_class}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Get activations for each layer and plot them\n",
        "    for layer_idx, layer_name, activation_model in activation_models:\n",
        "        # Get activations for this layer\n",
        "        activations = activation_model.predict(image)\n",
        "\n",
        "        # Number of filters in this layer\n",
        "        n_filters = activations.shape[-1]\n",
        "\n",
        "        # Calculate number of rows needed\n",
        "        rows = int(np.ceil(n_filters / cols))\n",
        "\n",
        "        # Create a figure\n",
        "        plt.figure(figsize=(cols * 2, rows * 2))\n",
        "\n",
        "        # Plot each activation map\n",
        "        for i in range(n_filters):\n",
        "            if i < activations.shape[-1]:\n",
        "                # Create subplot for this activation map\n",
        "                plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "                # Get the activation map for this filter\n",
        "                activation = activations[0, :, :, i]\n",
        "\n",
        "                # Normalize for better visualization\n",
        "                activation = (activation - activation.min()) / (activation.max() - activation.min() + 1e-7)\n",
        "\n",
        "                # Display the activation map\n",
        "                plt.imshow(activation, cmap='viridis')\n",
        "                plt.axis('off')\n",
        "                plt.title(f'Filter {i+1}')\n",
        "\n",
        "        plt.suptitle(f'Activation maps from layer {layer_name} for Digit {digit_class}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def generate_deep_dream(model, class_idx, iterations=20, step_size=1.0, octave_scale=1.4, num_octaves=5):\n",
        "    \"\"\"\n",
        "    Generate a deep dream image for a specific class.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained CNN model\n",
        "    class_idx: Index of the class to generate a deep dream for\n",
        "    iterations: Number of gradient ascent steps per octave\n",
        "    step_size: Size of the gradient ascent step\n",
        "    octave_scale: Scale factor between octaves\n",
        "    num_octaves: Number of octave iterations\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Deep dream image\n",
        "    \"\"\"\n",
        "    # Create a random noise image\n",
        "    img = np.random.normal(size=(28, 28, 1)) * 0.1\n",
        "\n",
        "    # Define loss function to maximize the class output\n",
        "    @tf.function\n",
        "    def calc_loss(image, class_idx):\n",
        "        # Ensure the image has the correct shape and data type\n",
        "        image = tf.cast(image, tf.float32)\n",
        "\n",
        "        # Get the model's prediction\n",
        "        pred = model(image)\n",
        "\n",
        "        # Return the activation of the target class\n",
        "        return pred[:, class_idx]\n",
        "\n",
        "    @tf.function\n",
        "    def gradient_ascent_step(image, class_idx, step_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(image)\n",
        "            loss = calc_loss(image, class_idx)\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the image\n",
        "        gradient = tape.gradient(loss, image)\n",
        "\n",
        "        # Normalize the gradient\n",
        "        gradient = tf.math.l2_normalize(gradient)\n",
        "\n",
        "        # Apply gradient ascent\n",
        "        image = image + gradient * step_size\n",
        "\n",
        "        return image\n",
        "\n",
        "    # Scale the input image\n",
        "    original_shape = img.shape[:-1]\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Process octaves from the largest to the smallest scale\n",
        "    for octave in range(num_octaves):\n",
        "        # Calculate the shape for this octave\n",
        "        octave_shape = tuple(np.array(original_shape) * octave_scale**(num_octaves - octave - 1))\n",
        "        octave_shape = tuple(map(int, octave_shape)) + (1,)\n",
        "\n",
        "        # Resize the image for this octave\n",
        "        resized_img = tf.image.resize(img, octave_shape[:-1])\n",
        "\n",
        "        # Perform gradient ascent on this octave\n",
        "        for i in range(iterations):\n",
        "            resized_img = gradient_ascent_step(resized_img, class_idx, step_size)\n",
        "\n",
        "        # Resize back to the original shape and update the image\n",
        "        img = tf.image.resize(resized_img, original_shape)\n",
        "\n",
        "    # Convert to numpy array and normalize for visualization\n",
        "    dream_img = img[0].numpy()\n",
        "    dream_img = (dream_img - dream_img.min()) / (dream_img.max() - dream_img.min())\n",
        "\n",
        "    return dream_img\n",
        "\n",
        "def visualize_cnn_outcomes(model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Visualize CNN filters, activation maps, and generate deep dream images.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained CNN model\n",
        "    x_test: Test images\n",
        "    y_test: Test labels\n",
        "    \"\"\"\n",
        "    # Display model summary\n",
        "    model.summary()\n",
        "\n",
        "    # 1. Visualize filters for each convolutional layer\n",
        "    print(\"\\nVisualizing Filters:\")\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, layers.Conv2D):\n",
        "            print(f\"Layer {i}: {layer.name}\")\n",
        "            plot_filters(model, i)\n",
        "\n",
        "    # 2. Find examples of digits '2' and '9' in the test set\n",
        "    digit_2_indices = np.where(np.argmax(y_test, axis=1) == 2)[0]\n",
        "    digit_9_indices = np.where(np.argmax(y_test, axis=1) == 9)[0]\n",
        "\n",
        "    # Select the first instances\n",
        "    digit_2_idx = digit_2_indices[0]\n",
        "    digit_9_idx = digit_9_indices[0]\n",
        "\n",
        "    digit_2_img = x_test[digit_2_idx]\n",
        "    digit_9_img = x_test[digit_9_idx]\n",
        "\n",
        "    # 3. Visualize activation maps for digits '2' and '9'\n",
        "    print(\"\\nVisualizing Activation Maps for Digit '2':\")\n",
        "    conv_layer_indices = [i for i, layer in enumerate(model.layers)\n",
        "                           if isinstance(layer, layers.Conv2D)]\n",
        "\n",
        "    plot_activation_maps(model, digit_2_img, conv_layer_indices, 2)\n",
        "\n",
        "    print(\"\\nVisualizing Activation Maps for Digit '9':\")\n",
        "    plot_activation_maps(model, digit_9_img, conv_layer_indices, 9)\n",
        "\n",
        "    # 4. Generate deep dream images for classes '2' and '9'\n",
        "    print(\"\\nGenerating Deep Dream Image for Digit '2':\")\n",
        "    dream_2 = generate_deep_dream(model, 2)\n",
        "\n",
        "    print(\"\\nGenerating Deep Dream Image for Digit '9':\")\n",
        "    dream_9 = generate_deep_dream(model, 9)\n",
        "\n",
        "    # Display deep dream images\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(np.squeeze(dream_2), cmap='viridis')\n",
        "    plt.title(\"Deep Dream: Digit '2'\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(np.squeeze(dream_9), cmap='viridis')\n",
        "    plt.title(\"Deep Dream: Digit '9'\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Analyzing deep dream images\n",
        "    print(\"\\nDeep Dream Analysis:\")\n",
        "    print(\"The deep dream images show the patterns that the model is sensitive to for each digit class.\")\n",
        "    print(\"For digit '2', we can observe curved patterns that represent the typical shape of the digit.\")\n",
        "    print(\"For digit '9', we can observe patterns with a circle at the top and a vertical line, \")\n",
        "    print(\"which are characteristic features of the digit '9'.\")\n",
        "\n",
        "# Code to use the visualization functions\n",
        "\"\"\"\n",
        "# Make sure you have a trained CNN model from Task 4\n",
        "# Assuming 'cnn_model' is your trained model and x_test_cnn, y_test_one_hot are your test data\n",
        "\n",
        "# Run the visualization functions\n",
        "visualize_cnn_outcomes(cnn_model, x_test_cnn, y_test_one_hot)\n",
        "\"\"\"\n",
        "\n",
        "# Alternative implementation for deep dream visualization\n",
        "def visualize_deep_dream_simpler(model, class_indices=[2, 9], input_shape=(28, 28, 1)):\n",
        "    \"\"\"\n",
        "    A simpler implementation of deep dream visualization that works well with MNIST.\n",
        "    Creates deep dream images by optimizing random noise to maximize class activation.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained CNN model\n",
        "    class_indices: List of class indices to generate deep dreams for\n",
        "    input_shape: Shape of the input images\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(len(class_indices) * 5, 5))\n",
        "\n",
        "    for i, class_idx in enumerate(class_indices):\n",
        "        # Start with random noise\n",
        "        img = tf.random.normal((1,) + input_shape) * 0.1\n",
        "        img = tf.Variable(img)\n",
        "\n",
        "        # Optimization steps\n",
        "        learning_rate = 0.1\n",
        "        steps = 100\n",
        "\n",
        "        for step in range(steps):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Get model prediction\n",
        "                pred = model(img)\n",
        "                # Loss is negative log probability of target class (we want to maximize it)\n",
        "                loss = -tf.math.log(pred[0, class_idx] + 1e-7)\n",
        "\n",
        "            # Compute gradients and update the image\n",
        "            grads = tape.gradient(loss, img)\n",
        "            img.assign_sub(grads * learning_rate)\n",
        "\n",
        "            # Optional: Apply regularization or normalization here\n",
        "            # This helps keep the image looking reasonable\n",
        "            if step % 10 == 0:\n",
        "                # Normalize to maintain contrast\n",
        "                img_np = img.numpy()\n",
        "                img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min() + 1e-7)\n",
        "                img.assign(tf.convert_to_tensor(img_np, dtype=tf.float32))\n",
        "\n",
        "        # Display the result\n",
        "        plt.subplot(1, len(class_indices), i + 1)\n",
        "        dream_img = np.squeeze(img.numpy())\n",
        "        plt.imshow(dream_img, cmap='viridis')\n",
        "        plt.title(f\"Deep Dream: Digit '{class_idx}'\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkKHWxhiLrI-"
      },
      "source": [
        "### TASK 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWoMgX5pLsoX"
      },
      "outputs": [],
      "source": [
        "# Task 6: Multi-task Learning\n",
        "\n",
        "# Define a function to load and prepare the Fashion MNIST dataset\n",
        "def load_fashion_mnist_data():\n",
        "    \"\"\"\n",
        "    Load the Fashion MNIST dataset and prepare it for multi-task learning.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Training and testing data for two tasks\n",
        "    \"\"\"\n",
        "    # Load Fashion MNIST dataset\n",
        "    fashion_mnist = keras.datasets.fashion_mnist\n",
        "    (train_X, train_y_1), (test_X, test_y_1) = fashion_mnist.load_data()\n",
        "\n",
        "    # Normalize pixel values\n",
        "    train_X = train_X / 255.0\n",
        "    test_X = test_X / 255.0\n",
        "\n",
        "    # Add channel dimension for CNN\n",
        "    train_X = np.expand_dims(train_X, axis=-1)\n",
        "    test_X = np.expand_dims(test_X, axis=-1)\n",
        "\n",
        "    # Create Task 2 labels - group the clothing items into 3 groups\n",
        "    # Group 0: Shoes (Sandal, Sneaker, Ankle Boot) - classes 5, 7, 9\n",
        "    # Group 1: Gendered (Dress, Shirt, Bag) - classes 3, 6, 8\n",
        "    # Group 2: Uni-Sex (T-shirt, Trouser, Pullover, Coat) - classes 0, 1, 2, 4\n",
        "\n",
        "    def create_group_label(y):\n",
        "        group_labels = np.zeros_like(y)\n",
        "        # Group 0: Shoes (Sandal, Sneaker, Ankle Boot)\n",
        "        group_labels[np.isin(y, [5, 7, 9])] = 0\n",
        "        # Group 1: Gendered (Dress, Shirt, Bag)\n",
        "        group_labels[np.isin(y, [3, 6, 8])] = 1\n",
        "        # Group 2: Uni-Sex (T-shirt, Trouser, Pullover, Coat)\n",
        "        group_labels[np.isin(y, [0, 1, 2, 4])] = 2\n",
        "        return group_labels\n",
        "\n",
        "    train_y_2 = create_group_label(train_y_1)\n",
        "    test_y_2 = create_group_label(test_y_1)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    train_y_1_onehot = keras.utils.to_categorical(train_y_1, 10)\n",
        "    test_y_1_onehot = keras.utils.to_categorical(test_y_1, 10)\n",
        "    train_y_2_onehot = keras.utils.to_categorical(train_y_2, 3)\n",
        "    test_y_2_onehot = keras.utils.to_categorical(test_y_2, 3)\n",
        "\n",
        "    return train_X, train_y_1_onehot, train_y_2_onehot, test_X, test_y_1_onehot, test_y_2_onehot\n",
        "\n",
        "# Task 6.1: Create individual CNN models for each task\n",
        "def create_single_task_cnn(input_shape, num_classes, task_name):\n",
        "    \"\"\"\n",
        "    Create a CNN model for a single task.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape: Shape of input images\n",
        "    num_classes: Number of output classes\n",
        "    task_name: Name of the task for the model\n",
        "\n",
        "    Returns:\n",
        "    keras.Model: Compiled CNN model\n",
        "    \"\"\"\n",
        "    model = models.Sequential(name=f\"Single_{task_name}\")\n",
        "\n",
        "    # Convolutional layers with filters [32, 64, 128]\n",
        "    model.add(layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
        "                           activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
        "                           activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
        "                           activation='relu'))\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(3136, activation='relu'))\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(100, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_single_task_models(train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2, batch_size=10, epochs=5):\n",
        "    \"\"\"\n",
        "    Train individual CNN models for each task.\n",
        "\n",
        "    Parameters:\n",
        "    train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2: Training and testing data\n",
        "    batch_size: Batch size for training\n",
        "    epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "    dict: Results for both models\n",
        "    \"\"\"\n",
        "    # Create models\n",
        "    model_task1 = create_single_task_cnn(train_X.shape[1:], 10, \"Task1_Item\")\n",
        "    model_task2 = create_single_task_cnn(train_X.shape[1:], 3, \"Task2_Group\")\n",
        "\n",
        "    # Print model summaries\n",
        "    print(\"Task 1 (Item Classification) Model Summary:\")\n",
        "    model_task1.summary()\n",
        "\n",
        "    print(\"\\nTask 2 (Group Classification) Model Summary:\")\n",
        "    model_task2.summary()\n",
        "\n",
        "    # Train Task 1 model\n",
        "    print(\"\\nTraining Task 1 (Item Classification) Model...\")\n",
        "    start_time = time.time()\n",
        "    history_task1 = model_task1.fit(\n",
        "        train_X, train_y_1,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_X, test_y_1),\n",
        "        verbose=1\n",
        "    )\n",
        "    task1_train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate Task 1 model\n",
        "    task1_test_loss, task1_test_accuracy = model_task1.evaluate(test_X, test_y_1, verbose=0)\n",
        "    print(f\"Task 1 Test Accuracy: {task1_test_accuracy:.4f}\")\n",
        "\n",
        "    # Train Task 2 model\n",
        "    print(\"\\nTraining Task 2 (Group Classification) Model...\")\n",
        "    start_time = time.time()\n",
        "    history_task2 = model_task2.fit(\n",
        "        train_X, train_y_2,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_X, test_y_2),\n",
        "        verbose=1\n",
        "    )\n",
        "    task2_train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate Task 2 model\n",
        "    task2_test_loss, task2_test_accuracy = model_task2.evaluate(test_X, test_y_2, verbose=0)\n",
        "    print(f\"Task 2 Test Accuracy: {task2_test_accuracy:.4f}\")\n",
        "\n",
        "    # Count parameters\n",
        "    task1_params = model_task1.count_params()\n",
        "    task2_params = model_task2.count_params()\n",
        "\n",
        "    return {\n",
        "        'task1': {\n",
        "            'model': model_task1,\n",
        "            'history': history_task1,\n",
        "            'accuracy': task1_test_accuracy,\n",
        "            'params': task1_params,\n",
        "            'train_time': task1_train_time\n",
        "        },\n",
        "        'task2': {\n",
        "            'model': model_task2,\n",
        "            'history': history_task2,\n",
        "            'accuracy': task2_test_accuracy,\n",
        "            'params': task2_params,\n",
        "            'train_time': task2_train_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Task 6.2: Create a multi-task learning model\n",
        "def create_multitask_model(input_shape, lambda_value=0.5):\n",
        "    \"\"\"\n",
        "    Create a multi-task learning model with a shared backbone.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape: Shape of input images\n",
        "    lambda_value: Weight for balancing between tasks (0-1)\n",
        "\n",
        "    Returns:\n",
        "    keras.Model: Compiled multi-task model\n",
        "    \"\"\"\n",
        "    # Input layer\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Shared convolutional layers\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Shared dense layer\n",
        "    shared_dense = layers.Dense(3136, activation='relu')(x)\n",
        "\n",
        "    # Task-specific layers for Task 1 (Item Classification)\n",
        "    task1 = layers.Dense(1024, activation='relu')(shared_dense)\n",
        "    task1 = layers.Dense(100, activation='relu')(task1)\n",
        "    task1_output = layers.Dense(10, activation='softmax', name='task1_output')(task1)\n",
        "\n",
        "    # Task-specific layers for Task 2 (Group Classification)\n",
        "    task2 = layers.Dense(1024, activation='relu')(shared_dense)\n",
        "    task2 = layers.Dense(100, activation='relu')(task2)\n",
        "    task2_output = layers.Dense(3, activation='softmax', name='task2_output')(task2)\n",
        "\n",
        "    # Create model with multiple outputs\n",
        "    model = keras.Model(inputs=inputs, outputs=[task1_output, task2_output])\n",
        "\n",
        "    # Compile with weighted losses\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss={\n",
        "            'task1_output': 'categorical_crossentropy',\n",
        "            'task2_output': 'categorical_crossentropy'\n",
        "        },\n",
        "        loss_weights={\n",
        "            'task1_output': lambda_value,\n",
        "            'task2_output': 1 - lambda_value\n",
        "        },\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_multitask_models(train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2,\n",
        "                          lambda_values=[0, 0.25, 0.5, 0.75, 1.0],\n",
        "                          batch_size=10, epochs=5):\n",
        "    \"\"\"\n",
        "    Train multiple multi-task models with different lambda values.\n",
        "\n",
        "    Parameters:\n",
        "    train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2: Training and testing data\n",
        "    lambda_values: List of lambda values to try\n",
        "    batch_size: Batch size for training\n",
        "    epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "    dict: Results for all MTL models\n",
        "    \"\"\"\n",
        "    mtl_results = {}\n",
        "\n",
        "    for lambda_val in lambda_values:\n",
        "        print(f\"\\nTraining Multi-Task Model with λ = {lambda_val}\")\n",
        "\n",
        "        # Create and compile model\n",
        "        mtl_model = create_multitask_model(train_X.shape[1:], lambda_val)\n",
        "\n",
        "        # Only print summary for the first model\n",
        "        if lambda_val == lambda_values[0]:\n",
        "            mtl_model.summary()\n",
        "\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "        history = mtl_model.fit(\n",
        "            train_X,\n",
        "            {'task1_output': train_y_1, 'task2_output': train_y_2},\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(\n",
        "                test_X,\n",
        "                {'task1_output': test_y_1, 'task2_output': test_y_2}\n",
        "            ),\n",
        "            verbose=1\n",
        "        )\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate model\n",
        "        test_results = mtl_model.evaluate(\n",
        "            test_X,\n",
        "            {'task1_output': test_y_1, 'task2_output': test_y_2},\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Extract test accuracies\n",
        "        # Structure of test_results: [total_loss, task1_loss, task2_loss, task1_acc, task2_acc]\n",
        "        task1_accuracy = test_results[3]\n",
        "        task2_accuracy = test_results[4]\n",
        "\n",
        "        print(f\"λ = {lambda_val}:\")\n",
        "        print(f\"  Task 1 (Item) Test Accuracy: {task1_accuracy:.4f}\")\n",
        "        print(f\"  Task 2 (Group) Test Accuracy: {task2_accuracy:.4f}\")\n",
        "\n",
        "        # Count parameters\n",
        "        param_count = mtl_model.count_params()\n",
        "\n",
        "        # Store results\n",
        "        mtl_results[lambda_val] = {\n",
        "            'model': mtl_model,\n",
        "            'history': history,\n",
        "            'task1_accuracy': task1_accuracy,\n",
        "            'task2_accuracy': task2_accuracy,\n",
        "            'params': param_count,\n",
        "            'train_time': train_time\n",
        "        }\n",
        "\n",
        "    return mtl_results\n",
        "\n",
        "def plot_mtl_results(single_task_results, mtl_results, lambda_values):\n",
        "    \"\"\"\n",
        "    Plot and compare the results of single-task and multi-task models.\n",
        "\n",
        "    Parameters:\n",
        "    single_task_results: Results from single-task models\n",
        "    mtl_results: Results from multi-task models with different lambda values\n",
        "    lambda_values: Lambda values used for MTL models\n",
        "    \"\"\"\n",
        "    # Create a table for comparison\n",
        "    task1_accuracies = [mtl_results[lam]['task1_accuracy'] for lam in lambda_values]\n",
        "    task2_accuracies = [mtl_results[lam]['task2_accuracy'] for lam in lambda_values]\n",
        "\n",
        "    # Add single task accuracies for reference\n",
        "    single_task1_acc = single_task_results['task1']['accuracy']\n",
        "    single_task2_acc = single_task_results['task2']['accuracy']\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.plot(lambda_values, task1_accuracies, 'o-', label='MTL Task 1 (Item)')\n",
        "    plt.plot(lambda_values, task2_accuracies, 's-', label='MTL Task 2 (Group)')\n",
        "\n",
        "    # Add horizontal lines for single task accuracies\n",
        "    plt.axhline(y=single_task1_acc, color='r', linestyle='--',\n",
        "                label=f'Single Task 1 (Item): {single_task1_acc:.4f}')\n",
        "    plt.axhline(y=single_task2_acc, color='g', linestyle='--',\n",
        "                label=f'Single Task 2 (Group): {single_task2_acc:.4f}')\n",
        "\n",
        "    plt.xlabel('Lambda Value (λ)')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Multi-Task Learning Performance vs Lambda Value')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.xticks(lambda_values)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create a comparison table\n",
        "    print(\"\\nResults Comparison Table:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Model':<20} | {'Task 1 Accuracy':<20} | {'Task 2 Accuracy':<20} | {'Parameters':<15}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Single task models\n",
        "    print(f\"{'Single Task 1':<20} | {single_task1_acc:<20.4f} | {'-':<20} | {single_task_results['task1']['params']:<15,}\")\n",
        "    print(f\"{'Single Task 2':<20} | {'-':<20} | {single_task2_acc:<20.4f} | {single_task_results['task2']['params']:<15,}\")\n",
        "\n",
        "    # Total parameters for both single task models\n",
        "    total_single_params = single_task_results['task1']['params'] + single_task_results['task2']['params']\n",
        "    print(f\"{'Single Tasks Total':<20} | {'-':<20} | {'-':<20} | {total_single_params:<15,}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # MTL models\n",
        "    for lam in lambda_values:\n",
        "        model_name = f\"MTL (λ={lam})\"\n",
        "        task1_acc = mtl_results[lam]['task1_accuracy']\n",
        "        task2_acc = mtl_results[lam]['task2_accuracy']\n",
        "        params = mtl_results[lam]['params']\n",
        "        print(f\"{model_name:<20} | {task1_acc:<20.4f} | {task2_acc:<20.4f} | {params:<15,}\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Calculate parameter savings\n",
        "    param_savings = total_single_params - mtl_results[0.5]['params']\n",
        "    param_savings_percent = (param_savings / total_single_params) * 100\n",
        "    print(f\"Parameter savings with MTL: {param_savings:,} ({param_savings_percent:.2f}%)\")\n",
        "\n",
        "    # Analysis for λ=0 and λ=1\n",
        "    print(\"\\nAnalysis of Special Lambda Values:\")\n",
        "    print(f\"λ=0: The model focuses entirely on Task 2 (Group Classification), ignoring Task 1\")\n",
        "    print(f\"λ=1: The model focuses entirely on Task 1 (Item Classification), ignoring Task 2\")\n",
        "\n",
        "    # Overall analysis\n",
        "    print(\"\\nMulti-Task Learning Analysis:\")\n",
        "\n",
        "    # Find best lambda value\n",
        "    best_avg_lambda = max(lambda_values, key=lambda lam: (mtl_results[lam]['task1_accuracy'] +\n",
        "                                                          mtl_results[lam]['task2_accuracy']) / 2)\n",
        "\n",
        "    best_avg_accuracy = (mtl_results[best_avg_lambda]['task1_accuracy'] +\n",
        "                         mtl_results[best_avg_lambda]['task2_accuracy']) / 2\n",
        "\n",
        "    print(f\"Best average performance at λ={best_avg_lambda} with average accuracy: {best_avg_accuracy:.4f}\")\n",
        "\n",
        "    # Compare MTL vs Single Task\n",
        "    avg_single_acc = (single_task1_acc + single_task2_acc) / 2\n",
        "    print(f\"Average single task accuracy: {avg_single_acc:.4f}\")\n",
        "\n",
        "    if best_avg_accuracy > avg_single_acc:\n",
        "        print(\"MTL outperforms the average of single task models!\")\n",
        "    else:\n",
        "        print(\"Single task models outperform MTL on average.\")\n",
        "\n",
        "# Main execution to run the tasks\n",
        "# Load and prepare Fashion MNIST data\n",
        "train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2 = load_fashion_mnist_data()\n",
        "\n",
        "# Task 6.1: Train individual CNN models\n",
        "single_task_results = train_single_task_models(\n",
        "    train_X, train_y_1, train_y_2,\n",
        "    test_X, test_y_1, test_y_2,\n",
        "    batch_size=10, epochs=5\n",
        ")\n",
        "\n",
        "# Task 6.2: Train MTL models with different lambda values\n",
        "lambda_values = [0, 0.25, 0.5, 0.75, 1.0]\n",
        "mtl_results = train_multitask_models(\n",
        "    train_X, train_y_1, train_y_2,\n",
        "    test_X, test_y_1, test_y_2,\n",
        "    lambda_values=lambda_values,\n",
        "    batch_size=10, epochs=5\n",
        ")\n",
        "\n",
        "# Plot and analyze results\n",
        "plot_mtl_results(single_task_results, mtl_results, lambda_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tozgchDzv6Vp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "msc_ai_env",
      "language": "python",
      "name": "aienv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
