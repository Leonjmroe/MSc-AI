{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Normalise images to be in the range [-1, 1]\n",
    "X_train = X_train / 127.5 - 1\n",
    "X_test = X_test / 127.5 - 1\n",
    "\n",
    "# Convert each 28x28 image into a 784 dimensional vector\n",
    "features_count = np.prod(X_train.shape[1:])\n",
    "X_train_flatened = X_train.reshape(n_train, features_count)\n",
    "X_test_flatened = X_test.reshape(n_test, features_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Task 1 --- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and Centroids \n",
    "# The centroid is calculated by averaging the coordinates of all the points in a cluster. This average gives you a single point that best represents the center of that cluster. \n",
    "\n",
    "# Reduce the dimensionality of the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_flatened)\n",
    "\n",
    "# Create a scatter plot of the PCA data, colored by digit\n",
    "fig = px.scatter(X_train_pca, x=0, y=1, color=y_train, title='PCA plot of the MNIST Dataset', width=1000, height=600)\n",
    "fig.update_layout(xaxis_title='Principal Component 1', yaxis_title='Principal Component 2')\n",
    "\n",
    "# Create a DataFrame with the PCA data and digit labels\n",
    "df_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['digit'] = y_train\n",
    "\n",
    "# Compute centroids for each class by taking the mean of PC1 and PC2\n",
    "centroids = df_pca.groupby('digit')[['PC1', 'PC2']].mean()\n",
    "\n",
    "# Colour mapping \n",
    "color_sequence = px.colors.qualitative.Plotly\n",
    "unique_digits = sorted(df_pca['digit'].unique())\n",
    "color_map = {digit: color_sequence[i % len(color_sequence)] for i, digit in enumerate(unique_digits)}\n",
    "\n",
    "# Add centroids as larger markers, each colored according to its class\n",
    "for digit, row in centroids.iterrows():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "                    x=[row['PC1']],\n",
    "                    y=[row['PC2']],\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_map[digit], size=15, symbol='diamond'),\n",
    "                    name=f'Centroid {digit}'\n",
    "                )\n",
    "            )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot - Shows the percentage of variance explained by each principal component\n",
    "\n",
    "pca_full = PCA(n_components=50)\n",
    "pca_full.fit(X_train_flatened)\n",
    "variance_ratios = pca_full.explained_variance_ratio_\n",
    "components = np.arange(1, len(variance_ratios) + 1)\n",
    "\n",
    "df = pd.DataFrame({'Principal Component': components, 'Explained Variance': variance_ratios * 100})\n",
    "df['Cumulative Variance'] = df['Explained Variance'].cumsum()\n",
    "\n",
    "fig = px.bar(df, x='Principal Component', y='Explained Variance', title='Scree Plot & Cumulative Variance', labels={'Explained Variance': 'Percentage of Variance Explained'}, width=1000, height=500)\n",
    "fig.add_scatter(x=df['Principal Component'], y=df['Cumulative Variance'], mode='lines+markers', name='Cumulative Variance', line=dict(color='red'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions / Notes\n",
    "\n",
    "# Why is PCA a good option to visualise data?\n",
    "# PCA is a good option to visualise data because it reduces the dimensionality of the data to 2 dimensions, which makes it easier to plot and understand.\n",
    "\n",
    "# Observations\n",
    "# Clustering of Classes - 784D to 2D space and visually see the differences between the different classes, which is not possible in the original 784D space.\n",
    "# Separation of Certain Classes - Some digits form more isolated clusters. Eg digit 1 has a tight cluster. - Relatively simple and unique shape compared to other digits.\n",
    "# Overlap Among Other Classes - Digit classes, such as 3, 5, and 8, have clusters that overlap considerably. This suggests that their differences may not be well captured by a linear projection onto the first two principal components.\n",
    "\n",
    "# Q - Which classes can be linearly separated?\n",
    "# A - 1 and 0 can be linearly separated. Where 3, 5 and 8 cannot be linearly separated.\n",
    "\n",
    "# PCA Notes\n",
    "# When you have high dimensional data, there are many directions in which the data can vary. The first principal component is the direction along which the data varies the most.\n",
    "# By projecting your data onto the space defined by the top few principal components (often just two for visualisation), you reduce the dimensionality while retaining most of the information (variance) in the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Task 2 --- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "\n",
    "\n",
    "def prepare_data(digit_1, digit_2):\n",
    "\n",
    "    # Convert each 28x28 image into a 784 dimensional vector\n",
    "    features_count = np.prod(X_train.shape[1:])\n",
    "    X_train_flatened = X_train.reshape(n_train, features_count)\n",
    "    X_test_flatened = X_test.reshape(n_test, features_count)\n",
    "\n",
    "    # Filter out for digit_1 and digit_2 for binary classification\n",
    "    cond = (y_train == digit_1) + (y_train == digit_2)\n",
    "    binary_x_train = X_train_flatened[cond, :]\n",
    "    binary_y_train = y_train[cond] * 1.0\n",
    "\n",
    "    # Normalise training labels\n",
    "    binary_y_train[binary_y_train == digit_1] = -1\n",
    "    binary_y_train[binary_y_train == digit_2] = 1\n",
    "\n",
    "    # Filter out for digit_1 and digit_2 for binary classification\n",
    "    cond_test = (y_test == digit_1) + (y_test == digit_2)\n",
    "    binary_x_test = X_test_flatened[cond_test, :]\n",
    "    binary_y_test = y_test[cond_test] * 1.0\n",
    "\n",
    "    # Normalise test labels\n",
    "    binary_y_test[binary_y_test == digit_1] = -1\n",
    "    binary_y_test[binary_y_test == digit_2] = 1\n",
    "\n",
    "    return binary_x_train, binary_y_train, binary_x_test, binary_y_test\n",
    "\n",
    "\n",
    "\n",
    "def predict(x, w, b):\n",
    "\n",
    "        # Compute the linear combination for each sample    \n",
    "        z = np.dot(x, w) + b         \n",
    "\n",
    "        # If z >= 0, predict 1, otherwise predict -1\n",
    "        prediction = np.where(z >= 0, 1, -1)  \n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "def run_epoch_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test, num_epochs=100, learning_rate=0.01):\n",
    "\n",
    "    def train_perceptron(x_train, y_train, num_epochs, learning_rate):\n",
    "\n",
    "        # Get the number of samples and features\n",
    "        n_samples, n_features = x_train.shape\n",
    "\n",
    "        # Initialise weights and bias to zero\n",
    "        w = np.zeros(n_features)  \n",
    "        b = 0.0                   \n",
    "        \n",
    "        # Lists to store accuracy values\n",
    "        train_accuracies = []\n",
    "        epochs = []\n",
    "        \n",
    "        # Batch of stochastic gradient descent\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(n_samples):\n",
    "\n",
    "                # Check if the sample is misclassified\n",
    "                if y_train[i] * (np.dot(x_train[i], w) + b) <= 0:\n",
    "\n",
    "                    # Update weights and bias using the perceptron rule\n",
    "                    w += learning_rate * y_train[i] * x_train[i]\n",
    "                    b += learning_rate * y_train[i]\n",
    "\n",
    "            # Evaluate training progress at each epoch\n",
    "            predictions = predict(x_train, w, b)\n",
    "            accuracy = np.mean(predictions == y_train)\n",
    "            train_accuracies.append(accuracy)\n",
    "            epochs.append(epoch + 1)\n",
    "\n",
    "        # Plot accuracy vs epochs\n",
    "        fig = px.line(x=epochs, y=train_accuracies, title='Training Accuracy vs Epochs', labels={'x': 'Epoch', 'y': 'Accuracy'}, width=1000, height=500)\n",
    "        fig.show()\n",
    "        \n",
    "        return w, b\n",
    "\n",
    "\n",
    "    # Train the perceptron using the binary training data\n",
    "    w, b = train_perceptron(binary_x_train, binary_y_train, num_epochs, learning_rate)\n",
    "\n",
    "    # Predict on the training data\n",
    "    train_predictions = predict(binary_x_train, w, b)\n",
    "    train_accuracy = np.mean(train_predictions == binary_y_train)\n",
    "    print('Final Training Accuracy:', train_accuracy)\n",
    "\n",
    "    # Predict on the test data\n",
    "    test_predictions = predict(binary_x_test, w, b)\n",
    "    test_accuracy = np.mean(test_predictions == binary_y_test)\n",
    "    print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def run_optimisation_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test, max_iters=1000, learning_rate=0.01, tolerance=1e-3):\n",
    "\n",
    "    def optimise_perceptron(x, y, max_iters, learning_rate, tolerance):\n",
    "\n",
    "        # Initialise variables\n",
    "        iter = 0\n",
    "        error = np.inf\n",
    "        error_list = []\n",
    "        n,m = x.shape\n",
    "        rng = np.random.default_rng()\n",
    "        w = rng.random(m)\n",
    "        b = rng.random()\n",
    "\n",
    "        # While the iteration is less than the maximum number of iterations and the error is greater than the tolerance\n",
    "        while (iter <= max_iters) & (error > tolerance):\n",
    "            \n",
    "            # Predict all samples\n",
    "            predictions = predict(x, w, b)\n",
    "            \n",
    "            # Identify misclassified samples\n",
    "            misclassified_indices = np.where(predictions != y)[0]\n",
    "            \n",
    "            # Compute current error (fraction of misclassified samples)\n",
    "            error = len(misclassified_indices) / n\n",
    "            error_list.append(error)\n",
    "            \n",
    "            # If no misclassifications, we can stop early\n",
    "            if len(misclassified_indices) == 0:\n",
    "                break\n",
    "\n",
    "            # Update w, b for each misclassified sample\n",
    "            for i in misclassified_indices:\n",
    "                w += learning_rate * y[i] * x[i]\n",
    "                b += learning_rate * y[i]\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "        return w, b, error_list\n",
    "\n",
    "\n",
    "    # Optimise on the training set\n",
    "    w_opt, b_opt, error_list = optimise_perceptron(binary_x_train, binary_y_train, max_iters, learning_rate, tolerance)\n",
    "\n",
    "    # Evaluate on training\n",
    "    train_pred = predict(binary_x_train, w_opt, b_opt)\n",
    "    train_accuracy = np.mean(train_pred == binary_y_train)\n",
    "    print('Final Training Accuracy:', train_accuracy)\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_pred = predict(binary_x_test, w_opt, b_opt)\n",
    "    test_accuracy = np.mean(test_pred == binary_y_test)\n",
    "    print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "    # Error Curve\n",
    "    df_error = pd.DataFrame({'Iteration': list(range(1, len(error_list) + 1)), 'Misclassification Error': error_list})\n",
    "    fig_error = px.line(df_error, x='Iteration', y='Misclassification Error', title='Perceptron Training Error', markers=True, width=1000, height=500)\n",
    "    fig_error.show()\n",
    "\n",
    "    # Visualise the learned weights as an image\n",
    "    w_image = w_opt.reshape(28, 28)\n",
    "    fig_weights = px.imshow(w_image, color_continuous_scale='RdBu', title='Learned Weight Image', width=1000, height=500)\n",
    "    fig_weights.show()\n",
    "\n",
    "    return test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "\n",
    "digits = {'sample_1': (1, 0), 'sample_2': (8, 3), 'sample_3': (5, 3), 'sample_4': (8, 7), 'sample_5': (2, 9)}\n",
    "results = {}\n",
    "\n",
    "for run, (digit_1, digit_2) in enumerate(digits.values()):\n",
    "    print(f'\\n\\nRun: {run + 1 }: -- Training for digits {digit_1} and {digit_2} --\\n\\n')\n",
    "    print(' -- Epoch Perceptron Training --\\n')\n",
    "    binary_x_train, binary_y_train, binary_x_test, binary_y_test = prepare_data(digit_1, digit_2)\n",
    "    epoch_test_accuracy = run_epoch_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test)\n",
    "    print('\\n -- Optimisation Perceptron Training --\\n')\n",
    "    optimisation_test_accuracy = run_optimisation_perceptron(binary_x_train, binary_y_train, binary_x_test, binary_y_test)  \n",
    "    results[f'run_{run + 1}'] = {'digit_1': round(digit_1, 0), 'digit_2': round(digit_2, 0), 'epoch_test_accuracy': round(epoch_test_accuracy, 2), 'optimisation_test_accuracy': round(optimisation_test_accuracy, 2)}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Task 3 --- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Testing Accuracy Curves\n",
    "\n",
    "def plot(history):\n",
    "\n",
    "    train_acc = history.history['accuracy'][-1] * 100\n",
    "    test_acc = history.history['val_accuracy'][-1] * 100\n",
    "    print(f'Training accuracy: {train_acc:.2f}%')\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Training vs. Testing Accuracy')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def multi_plot(names, histories):\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    for i in range(len(histories)):\n",
    "        plt.plot(histories[i].history['val_accuracy'])\n",
    "        \n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper left')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.98,1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Set up TensorBoard logging\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,  # Log histograms of weights/biases every epoch\n",
    "    write_graph=True,  # Visualise the model graph\n",
    "    write_images=True, # Save images of weights\n",
    "    update_freq='epoch', # Log metrics at the end of each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train/255\n",
    "y_train = np.eye(10)[y_train]\n",
    "X_test = X_test/ 255\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base MLP Model \n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),         # Flatten 28x28 images to 784\n",
    "    Dense(1000, activation='relu'),        # First hidden layer\n",
    "    Dense(1000, activation='relu'),        # Second hidden layer\n",
    "    Dense(10, activation='softmax')        # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second MLP Model \n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),        \n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dense(500, activation='relu'),        \n",
    "    Dense(500, activation='relu'),  \n",
    "    Dense(500, activation='relu'),        \n",
    "    Dense(10, activation='softmax')        \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Task 4 --- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]\n",
    "print(f'Training images: {n_train}, Test images: {n_test}')\n",
    "\n",
    "# Reshape and normalise the data\n",
    "x_train = x_train.reshape((n_train, 28, 28, 1)) / 255.0\n",
    "x_test = x_test.reshape((n_test, 28, 28, 1)) / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base CNN Model \n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(4, 4), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
    "    Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    Conv2D(128, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=50, epochs=10, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimsed CNN Model 1\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(8, kernel_size=(4, 4), strides=(1, 1), activation='relu', input_shape=(28, 28, 1)),\n",
    "    Conv2D(16, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    Conv2D(32, kernel_size=(4, 4), strides=(2, 2), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=50, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "plot(history)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodel Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple models \n",
    "\n",
    "def multi_model_run(models, names):\n",
    "\n",
    "    nets = len(models)\n",
    "\n",
    "    histories = [0] * nets\n",
    "\n",
    "    for j in range(nets):\n",
    "\n",
    "        histories[j] = models[j].fit(x_train, y_train, batch_size=50, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "        print('CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}'.format(\n",
    "            names[j], 10, \n",
    "            max(histories[j].history['accuracy']),  \n",
    "            max(histories[j].history['val_accuracy'])  \n",
    "        ))\n",
    "\n",
    "    return histories    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the optimal CNN depth?\n",
    "\n",
    "nets = 4\n",
    "models = [0] * nets\n",
    "names = ['CNN-1', 'CNN-2', 'CNN-3', 'CNN-4']\n",
    "histories = [0] * nets  \n",
    "\n",
    "for j in range(nets):\n",
    "\n",
    "    models[j] = Sequential()\n",
    "    models[j].add(Conv2D(16, kernel_size=(4,4), strides=(1,1), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "        \n",
    "    if j>0:\n",
    "        models[j].add(Conv2D(32, kernel_size=(4,4), strides=(2,2), padding='same', activation='relu'))\n",
    "        \n",
    "    if j>1:\n",
    "        models[j].add(Conv2D(64, kernel_size=(4,4), strides=(2,2), padding='same', activation='relu'))\n",
    "\n",
    "    if j>2:\n",
    "        models[j].add(Conv2D(128, kernel_size=(4,4), strides=(2,2), padding='same', activation='relu'))\n",
    "\n",
    "    models[j].add(Flatten())\n",
    "    models[j].add(Dense(10, activation='softmax'))\n",
    "\n",
    "    models[j].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "histories = multi_model_run(models, names)\n",
    "\n",
    "multi_plot(names, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the optimal number of feature maps (width)?\n",
    "\n",
    "nets = 6\n",
    "models = [0] * nets\n",
    "names = ['8 maps','16 maps','24 maps','32 maps','48 maps','64 maps']\n",
    "\n",
    "for j in range(6):\n",
    "\n",
    "    models[j] = Sequential([\n",
    "        Conv2D(j*8+8, kernel_size=(4,4), strides=(1,1), activation='relu', input_shape=(28,28,1)),\n",
    "        Conv2D(j*16+16, kernel_size=(4,4), strides=(2,2), activation='relu'),\n",
    "        Conv2D(j*32+32, kernel_size=(4,4), strides=(2,2), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    models[j].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "histories = multi_model_run(models, names)\n",
    "\n",
    "multi_plot(names, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the optimal dense layer size? ( beyond scope of this assignment)\n",
    "\n",
    "nets = 8\n",
    "model = [0] * nets\n",
    "names = ['0N', '32N', '64N', '128N', '256N', '512N', '1024N', '2048N']\n",
    "\n",
    "\n",
    "for j in range(nets):\n",
    "\n",
    "    model[j] = Sequential()\n",
    "    model[j].add(Conv2D(16, kernel_size=(4,4), strides=(1,1), activation='relu', input_shape=(28,28,1)))\n",
    "    model[j].add(Conv2D(32, kernel_size=(4,4), strides=(2,2), activation='relu'))\n",
    "    model[j].add(Flatten())\n",
    "    \n",
    "    if j>0:\n",
    "        model[j].add(Dense(2**(j+4), activation='relu'))\n",
    "\n",
    "    model[j].add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model[j].compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "histories = multi_model_run(models, names)\n",
    "\n",
    "multi_plot(names, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-ai-yPHgmFUp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
