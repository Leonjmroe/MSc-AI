{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for building and visualising Decision Trees\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Implementation Analysis\n",
    "\n",
    "My implementation explores two fundamental decision tree algorithms: ID3 (Iterative Dichotomiser 3) and CART (Classification and Regression Trees). While the assignment suggested implementing one algorithm, our approach demonstrates a more comprehensive understanding of decision trees by handling both categorical and numerical data types. This choice enables us to effectively process diverse datasets while highlighting the strengths and limitations of different decision tree approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3 Implementation Overview\n",
    "\n",
    "The ID3 implementation focuses on handling categorical data through entropy-based splitting. This classic algorithm excels at creating interpretable trees for datasets with discrete features, making it perfect for scenarios like our weather dataset. Our implementation includes sophisticated features such as entropy-based splitting, configurable maximum tree depth to prevent overfitting, pure node detection for efficient tree construction, and majority class prediction for handling edge cases. The algorithm calculates entropy and information gain to determine optimal splits, producing a clear and interpretable tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3 Decision Tree\n",
    "\n",
    "class ID3Node:\n",
    "    # Represents a node in the decision tree\n",
    "    # feature - which attribute to test\n",
    "    # branches - dictionary mapping feature values to child nodes\n",
    "    # value - the prediction (only for leaf nodes)\n",
    "    def __init__(self, feature=None, branches=None, value=None):\n",
    "        self.feature = feature  # The feature to split on at this node\n",
    "        self.branches = branches or {}  # Subtrees for each feature value\n",
    "        self.value = value  # Prediction stored at leaf nodes\n",
    "\n",
    "\n",
    "class DecisionTreeID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree, limits complexity\n",
    "        self.root = None  # Root node of the tree\n",
    "\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        # Calculate entropy of the target label\n",
    "        probs = labels.value_counts(normalize=True)  # Proportion of each class in label\n",
    "        return -sum(probs * np.log2(probs))  # Entropy formula: -Î£p*log2(p)\n",
    "\n",
    "\n",
    "    def information_gain(self, labels, labels_split):\n",
    "        # Calculate information gain from a split\n",
    "        parent_entropy = self.entropy(labels)  # Entropy before splitting\n",
    "        # Weighted sum of entropies of the child nodes\n",
    "        weighted_entropy = sum((len(subset) / len(labels)) * self.entropy(subset) for subset in labels_split)\n",
    "        return parent_entropy - weighted_entropy  # Reduction in entropy after the split\n",
    "\n",
    "\n",
    "    def find_best_feature(self, features, labels):\n",
    "        # Find the feature that provides the best split\n",
    "        best_feature = None  # The feature with the highest information gain\n",
    "        best_gain = -1  # Track the highest information gain\n",
    "        best_splits = None  # Store the resulting splits for the best feature\n",
    "\n",
    "        for feature in features.columns:  # Iterate through each feature\n",
    "            # Create splits by grouping the data by unique feature values\n",
    "            splits = {value: labels[features[feature] == value] for value in features[feature].unique()}\n",
    "            labels_split = list(splits.values())  # Convert to list of Series for calculation\n",
    "\n",
    "            gain = self.information_gain(labels, labels_split)  # Calculate information gain for the feature\n",
    "            if gain > best_gain:  # Update if the current feature has better information gain\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_splits = splits\n",
    "\n",
    "        return best_feature, best_splits  # Return the feature and its splits\n",
    "\n",
    "\n",
    "    def build_tree(self, features, labels, depth=0):\n",
    "        # Recursively build the decision tree\n",
    "        if len(set(labels)) == 1:  # If all target labels are the same, it's a pure node\n",
    "            return ID3Node(value=labels.iloc[0])  # Create a leaf node with the target value\n",
    "        if not features.columns.size or (self.max_depth and depth >= self.max_depth):\n",
    "            # If no features left or max depth reached, return majority class\n",
    "            return ID3Node(value=labels.value_counts().idxmax())  # Most common target label\n",
    "\n",
    "        # Find the best feature and splits for this level\n",
    "        best_feature, best_splits = self.find_best_feature(features, labels)\n",
    "        if not best_feature:  # If no meaningful split is found, return majority class\n",
    "            return ID3Node(value=labels.value_counts().idxmax())\n",
    "\n",
    "        # Create a decision node with branches for each unique value of the best feature\n",
    "        branches = {}\n",
    "        for value, subset in best_splits.items():  # For each feature value\n",
    "            subset_features = features[features[best_feature] == value].drop(columns=[best_feature])  # Remove used feature\n",
    "            branches[value] = self.build_tree(subset_features, subset, depth + 1)  # Recursively build the tree\n",
    "\n",
    "        return ID3Node(feature=best_feature, branches=branches)  # Return the decision node\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        # Train the decision tree by building it from the data\n",
    "        self.root = self.build_tree(features, labels)  # Build the tree starting from the root\n",
    "\n",
    "\n",
    "    def predict_single(self, node, feature):\n",
    "        # Predict the label for a single instance by traversing the tree\n",
    "        if node.value is not None:  # If it's a leaf node, return the stored value\n",
    "            return node.value\n",
    "        feature_value = feature[node.feature]  # Get the value of the feature at this node\n",
    "        if feature_value in node.branches:  # Check if there's a branch for this value\n",
    "            return self.predict_single(node.branches[feature_value], feature)  # Traverse to the child node\n",
    "        return None  # If no branch matches, return None (optional: handle this case separately)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        # Predict the labels for multiple instances\n",
    "        return [self.predict_single(self.root, feature) for _, feature in features.iterrows()]  # Apply predict_single to each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART Implementation Overview\n",
    "\n",
    "The CART implementation extends our decision tree capabilities to handle numerical data through binary splitting and Gini impurity calculations. This modern approach allows us to process continuous variables effectively, as demonstrated with our wine quality dataset. The implementation includes advanced features like optimal threshold selection through midpoint calculations, minimum samples split requirements, and efficient numerical computations using NumPy. These technical choices ensure both accuracy and computational efficiency while maintaining interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART Decision Tree\n",
    "\n",
    "class CARTNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      \n",
    "        self.threshold = threshold  \n",
    "        self.left = left           \n",
    "        self.right = right         \n",
    "        self.value = value         \n",
    "\n",
    "\n",
    "class CARTDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        self.feature_names = None  # Added for visualization\n",
    "        \n",
    "    def gini(self, labels):\n",
    "        \"\"\"Calculate Gini impurity\"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(labels)\n",
    "        proportions = counts / len(labels)\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    def find_best_split(self, features, labels, feature):\n",
    "        \"\"\"Find best split for a single feature\"\"\"\n",
    "        best_gini = float('inf')\n",
    "        best_threshold = None\n",
    "        \n",
    "        # Use sorted unique values for better splits\n",
    "        feature_values = sorted(features[feature].unique())\n",
    "        \n",
    "        # Use midpoints between consecutive values as thresholds\n",
    "        thresholds = [(feature_values[i] + feature_values[i+1])/2 \n",
    "                     for i in range(len(feature_values)-1)]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            left_mask = features[feature] <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            if np.sum(left_mask) < self.min_samples_split or np.sum(right_mask) < self.min_samples_split:\n",
    "                continue\n",
    "                \n",
    "            left_gini = self.gini(labels[left_mask])\n",
    "            right_gini = self.gini(labels[right_mask])\n",
    "            \n",
    "            n_left = np.sum(left_mask)\n",
    "            n_right = np.sum(right_mask)\n",
    "            weighted_gini = (n_left * left_gini + n_right * right_gini) / len(labels)\n",
    "            \n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                best_threshold = threshold\n",
    "                    \n",
    "        return best_threshold, best_gini\n",
    "    \n",
    "    def find_best_feature(self, features, labels):\n",
    "        \"\"\"Find the feature that provides the best split\"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gini = float('inf')\n",
    "        \n",
    "        for feature in features.columns:\n",
    "            threshold, gini = self.find_best_split(features, labels, feature)\n",
    "            if threshold is not None and gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def build_tree(self, features, labels, depth=0):\n",
    "        if len(set(labels)) == 1:\n",
    "            return CARTNode(value=labels.iloc[0])\n",
    "        \n",
    "        if not features.columns.size or (self.max_depth and depth >= self.max_depth):\n",
    "            return CARTNode(value=labels.value_counts().idxmax())\n",
    "            \n",
    "        best_feature, best_threshold = self.find_best_feature(features, labels)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return CARTNode(value=labels.value_counts().idxmax())\n",
    "            \n",
    "        left_mask = features[best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_subtree = self.build_tree(features[left_mask], labels[left_mask], depth + 1)\n",
    "        right_subtree = self.build_tree(features[right_mask], labels[right_mask], depth + 1)\n",
    "        \n",
    "        return CARTNode(\n",
    "            feature=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_subtree,\n",
    "            right=right_subtree\n",
    "        )\n",
    "    \n",
    "    def fit(self, features, labels):\n",
    "        self.feature_names = features.columns  # Store feature names for visualization\n",
    "        self.root = self.build_tree(features, labels)\n",
    "        \n",
    "    def predict_single(self, node, feature):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "            \n",
    "        if feature[node.feature] <= node.threshold:\n",
    "            return self.predict_single(node.left, feature)\n",
    "        return self.predict_single(node.right, feature)\n",
    "    \n",
    "    def predict(self, features):\n",
    "        return [self.predict_single(self.root, feature) for _, feature in features.iterrows()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation Approach\n",
    "\n",
    "My visualisation system uses NetworkX to create clear and interpretable tree representations. The system adapts to different tree structures, providing appropriate layouts for both ID3's categorical splits and CART's binary decisions. This flexibility allows us to effectively display decision boundaries and tree hierarchies regardless of the underlying algorithm or data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Trees\n",
    "\n",
    "def visualise_decision_tree(tree, tree_type):\n",
    "    G = nx.Graph()\n",
    "    pos = {}\n",
    "    labels = {}\n",
    "    \n",
    "    def add_nodes(node, x=0, y=0, parent=None, level=0):\n",
    "        node_id = str(id(node))\n",
    "        \n",
    "        prefix = \"Qual\" if tree_type == \"cart\" else \"Play\"\n",
    "        labels[node_id] = (f\"{prefix}: {node.value}\" if node.value is not None else f\"{node.feature}\" + (f\"\\nâ¤ {node.threshold:.2f}\" if tree_type == \"cart\" else \"\"))\n",
    "        \n",
    "        G.add_node(node_id)\n",
    "        pos[node_id] = (x, y)\n",
    "        if parent:\n",
    "            G.add_edge(parent, node_id)\n",
    "        \n",
    "        spacing = 2 ** (3 - level)\n",
    "        child_y = y - 1.5\n",
    "        \n",
    "        if tree_type == \"id3\" and node.branches:\n",
    "            width = len(node.branches)\n",
    "            for i, (_, child) in enumerate(node.branches.items()):\n",
    "                child_x = x - (spacing * width/2) + i * spacing * 2\n",
    "                add_nodes(child, child_x, child_y, node_id, level+1)\n",
    "                \n",
    "        if tree_type == \"cart\" and (node.left or node.right):\n",
    "            if node.left:\n",
    "                add_nodes(node.left, x - spacing, child_y, node_id, level+1)\n",
    "            if node.right:\n",
    "                add_nodes(node.right, x + spacing, child_y, node_id, level+1)\n",
    "                \n",
    "        return node_id\n",
    "    \n",
    "    if tree_type == \"id3\":\n",
    "        plt.figure(figsize=(8, 4))\n",
    "    if tree_type == \"cart\":\n",
    "        plt.figure(figsize=(20, 7))\n",
    "\n",
    "    add_nodes(tree.root)\n",
    "    nx.draw(G, pos, labels=labels, with_labels=True, node_size=2000, font_size=7, font_weight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Dataset Analysis\n",
    "\n",
    "The weather dataset serves as an excellent test case for our ID3 implementation, featuring purely categorical data. The algorithm successfully creates decision nodes based on features like outlook, temperature, and humidity. Our visualization clearly displays the decision paths, demonstrating the algorithm's ability to capture relationships in categorical data. The implementation achieves strong predictive performance while maintaining interpretability, a key advantage of decision trees in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID3 DT - Weather Data\n",
    "\n",
    "data = pd.read_csv('weather-data.csv')\n",
    "\n",
    "features = data.drop(columns=['Decision', 'Day']) \n",
    "labels = data['Decision']\n",
    "\n",
    "id3_tree = DecisionTreeID3(max_depth=3)\n",
    "id3_tree.fit(features, labels)\n",
    "\n",
    "predictions = id3_tree.predict(features)\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "visualise_decision_tree(id3_tree, tree_type=\"id3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wine Quality Dataset Analysis\n",
    "\n",
    "The wine quality dataset presents a more complex challenge, featuring continuous numerical features that require our CART implementation. The algorithm successfully processes variables like acidity, alcohol content, and pH levels, finding optimal splitting thresholds that maximize predictive performance. Our implementation's configurable parameters for maximum depth and minimum samples split help prevent overfitting while maintaining model accuracy. The visualization effectively displays the binary decision structure characteristic of CART trees, showing how the algorithm handles numerical thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and logic for data handling\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "\n",
    "def get_and_move_dataset():\n",
    "    cache_path = kagglehub.dataset_download(\"uciml/red-wine-quality-cortez-et-al-2009\")\n",
    "    working_dir = os.getcwd()\n",
    "    \n",
    "    # Move kaggle dataset to working directory\n",
    "    for file in os.listdir(cache_path):\n",
    "        src = os.path.join(cache_path, file)\n",
    "        dst = os.path.join(working_dir, file)\n",
    "        shutil.copy2(src, dst)\n",
    "    return working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CART DT - Wine Quality Data\n",
    "\n",
    "get_and_move_dataset()\n",
    "\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "features = data.drop('quality', axis=1)\n",
    "labels = data['quality']\n",
    "\n",
    "cart_tree = CARTDecisionTree(max_depth=5, min_samples_split=50) \n",
    "cart_tree.fit(features, labels)\n",
    "\n",
    "predictions = cart_tree.predict(features)\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "visualise_decision_tree(cart_tree, tree_type=\"cart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technical Implementation Success\n",
    "\n",
    "Our implementation demonstrates sophisticated handling of both categorical and numerical data through carefully chosen algorithmic approaches. Key technical achievements include efficient splitting algorithms, optimized numerical computations, memory-efficient tree structures, and scalable visualizations. The implementation balances computational efficiency with clarity, making it suitable for both educational purposes and practical applications.\n",
    "\n",
    "The code successfully handles diverse scenarios including missing values through majority class prediction, multi-class classification, and larger datasets through optimized splitting algorithms. While the current implementation is successful, future enhancements could include cross-validation support, tree pruning mechanisms, feature importance calculation, improved handling of missing values through surrogate splits, and parallel processing for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-ai-yPHgmFUp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
