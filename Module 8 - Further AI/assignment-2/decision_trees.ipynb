{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Node\n",
    "\n",
    "class Node:\n",
    "    # Represents a node in the decision tree\n",
    "    # feature - which attribute to test\n",
    "    # branches - dictionary mapping feature values to child nodes\n",
    "    # value - the prediction (only for leaf nodes)\n",
    "    def __init__(self, feature=None, branches=None, value=None):\n",
    "        self.feature = feature  # The feature to split on at this node\n",
    "        self.branches = branches or {}  # Subtrees for each feature value\n",
    "        self.value = value  # Prediction stored at leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeID3\n",
    "\n",
    "class DecisionTreeID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree, limits complexity\n",
    "        self.root = None  # Root node of the tree\n",
    "\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        # Calculate entropy of the target label\n",
    "        probs = labels.value_counts(normalize=True)  # Proportion of each class in label\n",
    "        return -sum(probs * np.log2(probs))  # Entropy formula: -Î£p*log2(p)\n",
    "\n",
    "\n",
    "    def information_gain(self, labels, labels_split):\n",
    "        # Calculate information gain from a split\n",
    "        parent_entropy = self.entropy(labels)  # Entropy before splitting\n",
    "        # Weighted sum of entropies of the child nodes\n",
    "        weighted_entropy = sum((len(subset) / len(labels)) * self.entropy(subset) for subset in labels_split)\n",
    "        return parent_entropy - weighted_entropy  # Reduction in entropy after the split\n",
    "\n",
    "\n",
    "    def find_best_feature(self, features, labels):\n",
    "        # Find the feature that provides the best split\n",
    "        best_feature = None  # The feature with the highest information gain\n",
    "        best_gain = -1  # Track the highest information gain\n",
    "        best_splits = None  # Store the resulting splits for the best feature\n",
    "\n",
    "        for feature in features.columns:  # Iterate through each feature\n",
    "            # Create splits by grouping the data by unique feature values\n",
    "            splits = {value: labels[features[feature] == value] for value in features[feature].unique()}\n",
    "            labels_split = list(splits.values())  # Convert to list of Series for calculation\n",
    "\n",
    "            gain = self.information_gain(labels, labels_split)  # Calculate information gain for the feature\n",
    "            if gain > best_gain:  # Update if the current feature has better information gain\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_splits = splits\n",
    "\n",
    "        return best_feature, best_splits  # Return the feature and its splits\n",
    "\n",
    "\n",
    "    def build_tree(self, features, labels, depth=0):\n",
    "        # Recursively build the decision tree\n",
    "        if len(set(labels)) == 1:  # If all target labels are the same, it's a pure node\n",
    "            return Node(value=labels.iloc[0])  # Create a leaf node with the target value\n",
    "        if not features.columns.size or (self.max_depth and depth >= self.max_depth):\n",
    "            # If no features left or max depth reached, return majority class\n",
    "            return Node(value=labels.value_counts().idxmax())  # Most common target label\n",
    "\n",
    "        # Find the best feature and splits for this level\n",
    "        best_feature, best_splits = self.find_best_feature(features, labels)\n",
    "        if not best_feature:  # If no meaningful split is found, return majority class\n",
    "            return Node(value=labels.value_counts().idxmax())\n",
    "\n",
    "        # Create a decision node with branches for each unique value of the best feature\n",
    "        branches = {}\n",
    "        for value, subset in best_splits.items():  # For each feature value\n",
    "            subset_features = features[features[best_feature] == value].drop(columns=[best_feature])  # Remove used feature\n",
    "            branches[value] = self.build_tree(subset_features, subset, depth + 1)  # Recursively build the tree\n",
    "\n",
    "        return Node(feature=best_feature, branches=branches)  # Return the decision node\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        # Train the decision tree by building it from the data\n",
    "        self.root = self.build_tree(features, labels)  # Build the tree starting from the root\n",
    "\n",
    "\n",
    "    def predict_single(self, node, feature):\n",
    "        # Predict the label for a single instance by traversing the tree\n",
    "        if node.value is not None:  # If it's a leaf node, return the stored value\n",
    "            return node.value\n",
    "        feature_value = feature[node.feature]  # Get the value of the feature at this node\n",
    "        if feature_value in node.branches:  # Check if there's a branch for this value\n",
    "            return self.predict_single(node.branches[feature_value], feature)  # Traverse to the child node\n",
    "        return None  # If no branch matches, return None (optional: handle this case separately)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        # Predict the labels for multiple instances\n",
    "        return [self.predict_single(self.root, feature) for _, feature in features.iterrows()]  # Apply predict_single to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulalise Decision Tree \n",
    "\n",
    "def visualise_DT(tree):\n",
    "    \"\"\"\n",
    "    Creates a horizontal tree visualization using NetworkX\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    pos = {}\n",
    "    labels = {}\n",
    "    \n",
    "    def add_nodes(node, x=0, y=0, parent=None, level=0):\n",
    "        node_id = str(id(node))\n",
    "        \n",
    "        # Create node label\n",
    "        if node.value is not None:\n",
    "            label = f\"Predict: {node.value}\"\n",
    "        else:\n",
    "            label = f\"Split: {node.feature}\"\n",
    "            \n",
    "        # Add node\n",
    "        G.add_node(node_id)\n",
    "        pos[node_id] = (x, y)\n",
    "        labels[node_id] = label\n",
    "        \n",
    "        # Add edge from parent\n",
    "        if parent:\n",
    "            G.add_edge(parent, node_id)\n",
    "        \n",
    "        # Process children with vertical spacing\n",
    "        if node.branches:\n",
    "            children = list(node.branches.items())\n",
    "            total_height = len(children)\n",
    "            for i, (value, child) in enumerate(children):\n",
    "                # Calculate position for child\n",
    "                child_y = y - (total_height/2) + i\n",
    "                child_x = x + 2  # Fixed horizontal spacing\n",
    "                \n",
    "                # Add child node\n",
    "                child_id = add_nodes(child, child_x, child_y, node_id, level+1)\n",
    "                \n",
    "                # Add edge label\n",
    "                edge_center = ((x + child_x)/2, (y + child_y)/2)\n",
    "                plt.text(edge_center[0], edge_center[1], value, \n",
    "                        ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "                \n",
    "        return node_id\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Build graph starting from root\n",
    "    add_nodes(tree.root)\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos,\n",
    "            labels=labels,\n",
    "            with_labels=True,\n",
    "            node_color='lightblue',\n",
    "            node_size=3000,\n",
    "            font_size=8,\n",
    "            font_weight='bold',\n",
    "            font_color='black',\n",
    "            width=1,\n",
    "            edge_color='gray')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(\"Decision Tree (Horizontal Layout)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test and Visualise - Weather Data\n",
    "\n",
    "data = pd.read_csv('weather-data.csv')\n",
    "\n",
    "features = data.drop(columns=['Decision', 'Day'])  # Remove both Decision and Day\n",
    "labels = data['Decision']\n",
    "\n",
    "tree = DecisionTreeID3(max_depth=3)\n",
    "tree.fit(features, labels)\n",
    "\n",
    "predictions = tree.predict(features)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "visualise_DT(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test and Visualise - Mushroom Data\n",
    "\n",
    "data = pd.read_csv('mushrooms.csv')\n",
    "\n",
    "features = data.drop(columns=['class']) \n",
    "labels = data['class']\n",
    "\n",
    "tree = DecisionTreeID3(max_depth=3)\n",
    "tree.fit(features, labels)\n",
    "\n",
    "predictions = tree.predict(features)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "visualise_DT(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-ai-yPHgmFUp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
