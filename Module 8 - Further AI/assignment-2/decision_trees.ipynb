{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for building and visualising Decision Trees\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "\n",
    "def visualise_decision_tree(tree, tree_type):\n",
    "\n",
    "    # Use the NetworkX charting package\n",
    "    G = nx.Graph()\n",
    "    pos = {}\n",
    "    labels = {}\n",
    "    \n",
    "    # Recursively add nodes to the graph\n",
    "    def add_nodes(node, x=0, y=0, parent=None, level=0):\n",
    "        node_id = str(id(node))\n",
    "        \n",
    "        # Set the label for the node\n",
    "        label = \"Qual\" if tree_type == \"cart\" else \"Play\"\n",
    "        labels[node_id] = (f\"{label}: {node.value}\" if node.value is not None else f\"{node.feature}\" + (f\"\\n≤ {node.threshold:.2f}\" if tree_type == \"cart\" else \"\"))\n",
    "        \n",
    "        # Add the node to the graph and set the position\n",
    "        G.add_node(node_id)\n",
    "        pos[node_id] = (x, y)\n",
    "        if parent:\n",
    "            G.add_edge(parent, node_id)\n",
    "        \n",
    "        # Calculate the position of the children\n",
    "        spacing = 2 ** (3 - level)\n",
    "        child_y = y - 1.5\n",
    "        \n",
    "        # Recursively add the children\n",
    "\n",
    "        if tree_type == \"id3\" and node.branches:\n",
    "            width = len(node.branches)\n",
    "            for i, (_, child) in enumerate(node.branches.items()):\n",
    "                child_x = x - (spacing * width/2) + i * spacing * 2\n",
    "                add_nodes(child, child_x, child_y, node_id, level+1)\n",
    "                \n",
    "        if tree_type == \"cart\" and (node.left or node.right):\n",
    "            if node.left:\n",
    "                add_nodes(node.left, x - spacing, child_y, node_id, level+1)\n",
    "            if node.right:\n",
    "                add_nodes(node.right, x + spacing, child_y, node_id, level+1)\n",
    "                \n",
    "        # Return the node ID\n",
    "        return node_id\n",
    "    \n",
    "\n",
    "    if tree_type == \"id3\":\n",
    "        plt.figure(figsize=(8, 4))\n",
    "    if tree_type == \"cart\":\n",
    "        plt.figure(figsize=(25, 14))\n",
    "\n",
    "    add_nodes(tree.root)\n",
    "    nx.draw(G, pos, labels=labels, with_labels=True, node_size=3000, font_size=7, font_weight='bold')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_importance(data):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.bar(data.keys(), data.values())\n",
    "    plt.title('Feature Importance in Wine Quality Prediction')\n",
    "    plt.ylabel('Usage Count in Decision Tree')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Approach\n",
    "\n",
    "This implementation provides two complementary approaches: ID3 and CART. This dual implementation strategy was chosen specifically to handle a wider range of real-world datasets while maintaining high performance across different data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3 Implementation Overview\n",
    "\n",
    "The ID3 implementation takes a top-down, recursive approach to handling categorical data using entropy and information gain for split decisions. This version of ID3 enhances the classic algorithm by adding key optimisations: while the basic algorithm looks only at entropy to split data, a configurable maximum depth prevents excessive tree growth and potential overfitting. The implementation includes majority class prediction to handle real-world data challenges like missing values and edge cases. By calculating information gain at each decision point, the algorithm selects the most informative features for splitting. These enhancements allow the implementation to effectively process data with multiple categorical attributes, handle missing information, and work with various classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3 Decision Tree\n",
    "\n",
    "class ID3Node:\n",
    "    def __init__(self, feature=None, branches=None, value=None):\n",
    "\n",
    "        # The feature to split on at this node\n",
    "        self.feature = feature \n",
    "\n",
    "        # Subtrees for each feature value\n",
    "        self.branches = branches or {}  \n",
    "\n",
    "        # Prediction stored at leaf nodes\n",
    "        self.value = value \n",
    "\n",
    "\n",
    "class DecisionTreeID3:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        # Calculates the Shannon entropy of the target labels\n",
    "        # Entropy = -Σ(p(x) * log2(p(x))) where p(x) is probability of each class\n",
    "        # Lower entropy means more homogeneous labels where higher entropy means more mixed labels\n",
    "\n",
    "        # Get probability of each class\n",
    "        probs = labels.value_counts(normalize=True)  \n",
    "\n",
    "        # Apply the shannon entropy formula\n",
    "        return -sum(probs * np.log2(probs)) \n",
    "\n",
    "\n",
    "    def information_gain(self, labels, labels_split):\n",
    "        # Calculates the information gain from a potential split\n",
    "        # Gain = Entropy(parent) - Σ((n_child/n_parent) * Entropy(child))\n",
    "        # The higher gain means a better split where more information is obtained\n",
    "\n",
    "        parent_entropy = self.entropy(labels)\n",
    "        \n",
    "        # Calculate the weighted average entropy of children\n",
    "        weighted_child_entropy = sum((len(child) / len(labels)) * self.entropy(child) for child in labels_split)\n",
    "\n",
    "        # Take weighted entropy from parent entropy to get information gain\n",
    "        return parent_entropy - weighted_child_entropy\n",
    "\n",
    "\n",
    "    def find_best_feature(self, features, labels):\n",
    "        # For each feature, group data by feature values then calculate information gain and then choose feature with highest gain\n",
    "        # Returns the feature with the highest information gain and the corresponding label subsets\n",
    "\n",
    "        best_feature = None\n",
    "        best_splits = None\n",
    "\n",
    "        # Any gain will be better than -1\n",
    "        best_gain = -1  \n",
    "\n",
    "        for feature in features.columns:\n",
    "\n",
    "            # Group labels by feature values\n",
    "            splits = {value: labels[features[feature] == value] for value in features[feature].unique()}\n",
    "            labels_split = list(splits.values())\n",
    "            \n",
    "            # Calculate information gain and update best feature if better\n",
    "            gain = self.information_gain(labels, labels_split)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_splits = splits\n",
    "\n",
    "        # Return the feature with the highest gain and the corresponding splits\n",
    "        return best_feature, best_splits\n",
    "\n",
    "\n",
    "    def build_tree(self, features, labels, depth=0):\n",
    "\n",
    "        # Recursively build the decision tree and create a leaf node with the target value\n",
    "        if len(set(labels)) == 1:  \n",
    "            return ID3Node(value=labels.iloc[0]) \n",
    "        \n",
    "         # If no features left or max depth reached, return majority class\n",
    "        if not features.columns.size or (self.max_depth and depth >= self.max_depth):\n",
    "            return ID3Node(value=labels.value_counts().idxmax())  \n",
    "\n",
    "        # Find the best feature and splits for this level. If no meaningful split is found, return majority class\n",
    "        best_feature, best_splits = self.find_best_feature(features, labels)\n",
    "        if not best_feature:  \n",
    "            return ID3Node(value=labels.value_counts().idxmax())\n",
    "\n",
    "        # Create a decision node with branches for each unique value of the best feature\n",
    "        branches = {}\n",
    "        for value, subset in best_splits.items():  \n",
    "            subset_features = features[features[best_feature] == value].drop(columns=[best_feature]) \n",
    "            branches[value] = self.build_tree(subset_features, subset, depth + 1)  \n",
    "\n",
    "        # Return the decision node\n",
    "        return ID3Node(feature=best_feature, branches=branches)  \n",
    "\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        # Train the decision tree by building it from the data, starting from the root\n",
    "        self.root = self.build_tree(features, labels)  \n",
    "\n",
    "\n",
    "    def predict_single(self, node, feature):\n",
    "        # Predict the label for a single instance by following the tree, if it's a leaf node, return the stored value\n",
    "        if node.value is not None:  \n",
    "            return node.value\n",
    "        \n",
    "        # Get the value of the feature at this node and check if there's a branch for this value\n",
    "        feature_value = feature[node.feature]  \n",
    "        if feature_value in node.branches:  \n",
    "            return self.predict_single(node.branches[feature_value], feature) \n",
    "        \n",
    "        # If no branch matche\n",
    "        return None  \n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        # Predict the labels for multiple instances\n",
    "        return [self.predict_single(self.root, feature) for _, feature in features.iterrows()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ID3 DT - Weather Data\n",
    "\n",
    "data = pd.read_csv('weather-data.csv')\n",
    "\n",
    "features = data.drop(columns=['Decision', 'Day']) \n",
    "labels = data['Decision']\n",
    "\n",
    "id3_tree = DecisionTreeID3(max_depth=3)\n",
    "id3_tree.fit(features, labels)\n",
    "\n",
    "predictions = id3_tree.predict(features)\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "visualise_decision_tree(id3_tree, tree_type=\"id3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Analysis on Weather Dataset\n",
    "\n",
    "The ID3 implementation achieved a 100% accuracy on the weather dataset. With only 14 rows of data, this perfect accuracy indicates an overfitting behavior. The decision tree has effectively created a lookup table for the training data.\n",
    "\n",
    "Each decision node can split the already small dataset into even smaller subsets, eventually leading to leaf nodes that may represent just one or two samples. This level of granularity, while producing perfect training accuracy, typically indicates that the model has learned patterns specific to this exact dataset rather than general relationships between weather conditions and decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------\n",
    "\n",
    "#### Wine Quality Dataset\n",
    "This implementation uses the \"Wine Quality\" dataset from the UCI Machine Learning Repository, accessed via Kaggle:\n",
    "\n",
    "**Dataset Source:**  \n",
    "- Title: Red Wine Quality (Cortez et al., 2009)\n",
    "- URL: https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009\n",
    "- Original Publication: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- Features:\n",
    "  - Fixed acidity\n",
    "  - Volatile acidity\n",
    "  - Citric acid\n",
    "  - Residual sugar\n",
    "  - Chlorides\n",
    "  - Free sulfur dioxide\n",
    "  - Total sulfur dioxide\n",
    "  - Density\n",
    "  - pH\n",
    "  - Sulphates\n",
    "  - Alcohol\n",
    "- Output Variable: Quality (score between 0 and 10)\n",
    "- Data Count: 1,599\n",
    "- Feature Types: All numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART Implementation Overview\n",
    "\n",
    "The CART implementation builds on ID3's capabilities by handling both numerical and categorical data through binary splits. While ID3 creates multiple branches at each node, CART makes binary decisions based on optimised thresholds - a design choice that proved effective with the wine quality dataset's numerical features. The implementation uses Gini impurity to evaluate potential splits, which tends to work well for numerical data. It includes practical safeguards like minimum sample requirements for splits and configurable tree depth to prevent overfitting. Through efficient midpoint threshold calculations, the algorithm processes both continuous and categorical variables effectively, making it versatile enough for complex, real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART Decision Tree\n",
    "\n",
    "class CARTNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        # The feature to split on at this node\n",
    "        self.feature = feature      \n",
    "\n",
    "        # Threshold for binary split\n",
    "        self.threshold = threshold  \n",
    "\n",
    "        # Subtrees for each branch\n",
    "        self.left = left           \n",
    "        self.right = right        \n",
    "\n",
    "        # Prediction stored at leaf nodes \n",
    "        self.value = value         \n",
    "\n",
    "\n",
    "class CARTDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        self.feature_names = None\n",
    "        self.feature_importance = {}\n",
    "\n",
    "    def gini(self, labels):\n",
    "        # Calculate the Gini impurity for a set of labels\n",
    "        # Gini = 1 - Σ(p_i²) where p_i is probability of class i\n",
    "        # Gini = 0 - (Perfect purity - all samples same class)\n",
    "        # Gini aprox 0.5 - (Maximum impurity for binary classification)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Get the counts and probabilities of each class\n",
    "        counts = np.bincount(labels)\n",
    "        proportions = counts / len(labels)\n",
    "\n",
    "        # Calculate the Gini impurity\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "\n",
    "    def find_best_split(self, features, labels, feature):\n",
    "        # Find best binary split for a numerical feature and return the threshold and Gini impurity by trying midpoints between consecutive values\n",
    "\n",
    "        # Initialise best Gini impurity and threshold\n",
    "        best_gini = float('inf')\n",
    "        best_threshold = None\n",
    "        \n",
    "        feature_values = sorted(features[feature].unique())\n",
    "        \n",
    "        # Use midpoints between consecutive values as potential thresholds\n",
    "        thresholds = [(feature_values[i] + feature_values[i+1])/2 for i in range(len(feature_values)-1)]\n",
    "        \n",
    "        # Try each threshold and keep track of the best one\n",
    "        for threshold in thresholds:\n",
    "            left_mask = features[feature] <= threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            # Ensure minimum samples in both children\n",
    "            if (np.sum(left_mask) < self.min_samples_split or \n",
    "                np.sum(right_mask) < self.min_samples_split):\n",
    "                continue\n",
    "            \n",
    "            # Calculate Gini impurity for both children\n",
    "            left_gini = self.gini(labels[left_mask])\n",
    "            right_gini = self.gini(labels[right_mask])\n",
    "            \n",
    "            # Weighted average of children's Gini impurity\n",
    "            n_left = np.sum(left_mask)\n",
    "            n_right = np.sum(right_mask)\n",
    "            weighted_gini = (n_left * left_gini + n_right * right_gini) / len(labels)\n",
    "            \n",
    "            # Update best Gini impurity and threshold\n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                best_threshold = threshold\n",
    "                    \n",
    "        return best_threshold, best_gini\n",
    "    \n",
    "\n",
    "    def find_best_feature(self, features, labels):\n",
    "        # Find the feature that provides the best split and return the feature and threshold\n",
    "\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gini = float('inf')\n",
    "        \n",
    "        # Try each feature and find the best threshold\n",
    "        for feature in features.columns:\n",
    "            threshold, gini = self.find_best_split(features, labels, feature)\n",
    "            if threshold is not None and gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "                # Update feature importance\n",
    "                if best_feature is not None:\n",
    "                    if best_feature not in self.feature_importance:\n",
    "                        self.feature_importance[best_feature] = 1\n",
    "                    else:\n",
    "                        self.feature_importance[best_feature] += 1\n",
    "                \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "\n",
    "    def build_tree(self, features, labels, depth=0):\n",
    "\n",
    "        # Recursively build the decision tree and create a leaf node with the target value\n",
    "        if len(set(labels)) == 1:\n",
    "            return CARTNode(value=labels.iloc[0])\n",
    "        \n",
    "        # If no features left or max depth reached, return majority class\n",
    "        if not features.columns.size or (self.max_depth and depth >= self.max_depth):\n",
    "            return CARTNode(value=labels.value_counts().idxmax())\n",
    "            \n",
    "        # Find the best feature and threshold for this level. If no meaningful split is found, return majority class\n",
    "        best_feature, best_threshold = self.find_best_feature(features, labels)\n",
    "        \n",
    "        # If no split found, return majority class\n",
    "        if best_feature is None:\n",
    "            return CARTNode(value=labels.value_counts().idxmax())\n",
    "            \n",
    "        # Split data\n",
    "        left_mask = features[best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Ensure minimum samples in both children\n",
    "        left_subtree = self.build_tree(features[left_mask], labels[left_mask], depth + 1)\n",
    "        right_subtree = self.build_tree(features[right_mask], labels[right_mask], depth + 1)\n",
    "        \n",
    "        # Return the decision node\n",
    "        return CARTNode(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "    \n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        # Train the decision tree by building it from the data, starting from the root\n",
    "        self.feature_names = features.columns  \n",
    "        self.root = self.build_tree(features, labels)\n",
    "        \n",
    "\n",
    "    def predict_single(self, node, feature):\n",
    "\n",
    "        # If leaf node, return the stored value\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "            \n",
    "        # Get the value of the feature at this node and check if there's a branch for this value\n",
    "        if feature[node.feature] <= node.threshold:\n",
    "            return self.predict_single(node.left, feature)\n",
    "        \n",
    "        # If no branch matches\n",
    "        return self.predict_single(node.right, feature)\n",
    "    \n",
    "\n",
    "    def predict(self, features):\n",
    "        # Predict the labels for multiple instances\n",
    "        return [self.predict_single(self.root, feature) for _, feature in features.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and logic for data handling of the Red Wine Quality dataset\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Function to download and move dataset to working directory\n",
    "import shutil\n",
    "\n",
    "\n",
    "def get_and_move_dataset():\n",
    "\n",
    "    # Download dataset from Kaggle\n",
    "    cache_path = kagglehub.dataset_download(\"uciml/red-wine-quality-cortez-et-al-2009\")\n",
    "    working_dir = os.getcwd()\n",
    "    \n",
    "    # Move dataset to working directory\n",
    "    for file in os.listdir(cache_path):\n",
    "        src = os.path.join(cache_path, file)\n",
    "        dst = os.path.join(working_dir, file)\n",
    "        shutil.copy2(src, dst)\n",
    "    return working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CART DT - Wine Quality Data\n",
    "\n",
    "get_and_move_dataset()\n",
    "\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "features = data.drop('quality', axis=1)\n",
    "labels = data['quality']\n",
    "\n",
    "cart_tree = CARTDecisionTree(max_depth=5, min_samples_split=50) \n",
    "cart_tree.fit(features, labels)\n",
    "\n",
    "predictions = cart_tree.predict(features)\n",
    "accuracy = sum(pred == actual for pred, actual in zip(predictions, labels)) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "visualise_decision_tree(cart_tree, tree_type=\"cart\")\n",
    "print(\"\\n\")\n",
    "\n",
    "feature_importance = dict(sorted(cart_tree.feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "plot_importance(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-ai-yPHgmFUp-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
