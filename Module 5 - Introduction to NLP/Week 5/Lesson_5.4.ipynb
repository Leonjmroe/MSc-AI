{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lesson 4, Activity 8: LDA for Topic Modelling\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(updated to newer library versions: Nadejda Roubtsova, February 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement topic modelling approach using Latent Dirichlet Allocation and apply it to the `20 Newsgroups` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data\n",
    "\n",
    "First, let's import the libraries that we are going to use in this notebook. Then, let's define a method to load *training* and *test* subsets using a predefined list of categories. Note that you are working with the same dataset as last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(a_set, cats):\n",
    "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return dataset\n",
    "\n",
    "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
    "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
    "\n",
    "newsgroups_all = load_dataset(# load_dataset using 'all' as a_set and the list of categories from above\n",
    "                              )\n",
    "print(len(newsgroups_all.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the data\n",
    "\n",
    "Convert word forms to stems to get concise representations for the documents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem(text):\n",
    "    # return stemmed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `gensim` and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS as stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text, min_len=4):\n",
    "        # if token is not in stopwords, stem it and append to the result list\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how each document is represented. For example, look into the very first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample = newsgroups_all.data[0]\n",
    "print('Original document: ')\n",
    "print(doc_sample)\n",
    "\n",
    "print('\\n\\nTokenized document: ')\n",
    "words = []\n",
    "for token in gensim.utils.tokenize(doc_sample):\n",
    "    words.append(token)\n",
    "print(words)\n",
    "\n",
    "print('\\n\\nPreprocessed document: ')\n",
    "# print preprocessed document for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the first 10 look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(str(i) + \"\\t\" + \", \".join(preprocess(newsgroups_all.data[i])[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's represent each document as a dictionary of relevant words. Each word (*value* in the dictionary) has a unique identifier (*key*): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "for i in range(0, len(newsgroups_all.data)):\n",
    "    processed_docs.append(preprocess(newsgroups_all.data[i]))\n",
    "\n",
    "print(len(processed_docs))\n",
    "    \n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print(len(dictionary))\n",
    "\n",
    "index = 0\n",
    "# print(key, value) for the first 10 items in dictionary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put some constraints on the dictionary of terms: for instance, keep up to $100000$ words that occur more frequently than $10$ times (`no_below`) and less frequently than in $50\\%$ of the documents (`no_above`). This should help you extract the most useful terms, while still keeping a reasonable number of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a particular document is represented in this dictionary: for example, look into the very first post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# take a look into the very first post in bow_corpus â€“ print the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode what each index (key) in this dictionary points to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc = bow_corpus[0]\n",
    "\n",
    "for i in range(len(bow_doc)):\n",
    "    print(f\"Key {bow_doc[i][0]} =\\\"{dictionary[bow_doc[i][0]]}\\\":\\\n",
    "    occurrences={bow_doc[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train an LDA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary\n",
    "id2word = dictionary\n",
    "\n",
    "# Create the corpus with word frequencies\n",
    "corpus = bow_corpus\n",
    "\n",
    "# Build the LDA model\n",
    "# Check gensim documentation and familiarise yourself with LdaModel functionality\n",
    "# (see https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "# Experiment with other LDA model settings\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "\n",
    "for index, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {index} \\nWords: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyse the results\n",
    "\n",
    "What is the most representative topic in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_topics(ldamodel, corpus, texts):\n",
    "    main_topic = {}\n",
    "    percentage = {}\n",
    "    keywords = {}\n",
    "    text_snippets = {}\n",
    "    # Get main topic in each document\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        topic = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        topic = sorted(topic, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the main topic, contribution (%) and keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(topic):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp[:5]])\n",
    "                main_topic[i] = int(topic_num)\n",
    "                percentage[i] = round(prop_topic,4)\n",
    "                keywords[i] = topic_keywords\n",
    "                text_snippets[i] = texts[i][:8]\n",
    "            else:\n",
    "                break\n",
    "    return main_topic, percentage, keywords, text_snippets\n",
    "\n",
    "\n",
    "main_topic, percentage, keywords, text_snippets = analyse_topics(# apply to the relevant structures\n",
    "                                                                 )\n",
    "\n",
    "indexes = []\n",
    "rows = []\n",
    "for i in range(0, 10):\n",
    "    indexes.append(i)\n",
    "rows.append(['ID', 'Main Topic', 'Contribution (%)', 'Keywords', 'Snippet'])\n",
    "\n",
    "for idx in indexes:\n",
    "    rows.append([str(idx), f\"{main_topic.get(idx)}\", \n",
    "                f\"{percentage.get(idx):.4f}\",\n",
    "                f\"{keywords.get(idx)}\\n\",\n",
    "                f\"{text_snippets.get(idx)}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], width=column_widths[i]) \n",
    "                  for i in range(0, len(row)))) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is how you can explore words and topics with `pyLDAvis` (for installation instructions, see https://pyldavis.readthedocs.io/en/latest/readme.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
