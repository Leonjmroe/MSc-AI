{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdffcf5-7db6-4399-8121-54e5985d5fa5",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "My opinion miner project is designed to process a broad spectrum of consumer reviews across various product categories, employing a sophisticated NLP pipeline that addresses the complexities of unstructured text data. This extensive pipeline begins with the parsing and preprocessing of data from diverse sources, where the raw text is organized into a structured pandas DataFrame. Reviews are segmented based on embedded sentiment tags, and the text analysis phase involves Noun Phrase Chunking to isolate relevant phrases and Concrete Noun Filters to identify nouns that substantively relate to physical product attributes.\n",
    "\n",
    "The system manages the diversity of content and style, blending technical specifications with personal experiences and incorporating colloquial terminologies from reviews that range from single to multiline texts. To refine the extracted features, a Similarity Buffer assesses semantic proximity to the identified product name, ensuring the features are directly related to specific products. This is crucial especially as the dataset often includes product models in the titles of the reviews, but inconsistently, adding complexity to the task of accurately identifying and associating reviews with specific products.\n",
    "\n",
    "For handling the diverse and sometimes ambiguous nature of product identification, I employ sophisticated text classification models that are trained to detect and categorize product mentions accurately. Each identified feature then undergoes sentiment classification where the context surrounding each feature is analyzed to determine sentiment scores, which are subsequently aggregated into a product-feature dictionary.\n",
    "\n",
    "However, the evaluation process faces significant challenges due to the lack of explicit sentiment tags in much of the data, and where tags exist, they often display inconsistency in format and correlation with textual sentiment. This discrepancy necessitates the development of advanced models and custom rules to align and calibrate the sentiment analysis, ensuring accuracy and reliability in results. Through continuous learning mechanisms that update models with new data and user feedback, the system remains adaptable and current with evolving consumer language and product features.\n",
    "\n",
    "Overall, by integrating advanced NLP techniques and machine learning models, my opinion miner is capable of handling the complexities of a multi-product review environment, providing detailed and actionable insights that aid businesses in understanding consumer sentiments across a broad spectrum of products. This facilitates strategic decision-making based on robust data-driven analytics, enhancing product design, marketing strategies, and customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore', message='Discarded redundant search for Synset')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import deaccent\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c6f52-7a94-47d6-8b75-25a17df3d803",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "#### Data Ingestion and Initial Processing\n",
    "\n",
    "The process begins with the reading of data files, where each file potentially contains multiple reviews with varying structures. A Python function, `read_file`, is employed to open and read the content of these files. Reviews are often separated by new lines and may begin with a distinctive marker of asterisks indicating metadata or headers that are not part of the actual review content. Such lines are programmatically identified and skipped, ensuring that only relevant text is processed further.\n",
    "\n",
    "Reviews within these files are then split using '##' as a delimiter to segregate tags from the main content, which allows for the extraction of embedded metadata or sentiment tags when present. Each piece of the review, along with its associated tags, is stored in a structured format within a pandas DataFrame, facilitating subsequent manipulations and analyses.\n",
    "\n",
    "#### Advanced Text Processing Techniques\n",
    "\n",
    "Once the initial ingestion is complete, the reviews undergo a series of sophisticated text processing steps encapsulated within the `pre_process_review` function. This function is designed to handle various nuances of the text, including title concatenation where necessary and preservation of the textual integrity for reviews that continue across multiple lines.\n",
    "\n",
    "##### Preservation of Semantic Structures\n",
    "\n",
    "To maintain the semantic integrity of phrases within the reviews, the `preserve_compound_phrases` function is applied. This function utilizes spaCy, an advanced NLP library, to parse the text and identify compound nouns and adjectival modifiers, which are crucial for understanding the context and sentiment related to specific product features. These phrases are then reconstructed with underscores replacing spaces to ensure they are treated as single tokens in subsequent analyses, preventing the loss of their semantic unity.\n",
    "\n",
    "##### Enhancement of Tokenization and Filtering\n",
    "\n",
    "Post semantic preservation, the `pre_processing_controller` function orchestrates several layers of tokenization and filtering. The text is first tokenized, ensuring that each word or phrase is individually analyzed. This tokenization feeds into a dual filtering process where:\n",
    "1. A 'Soft Filtered Review' captures tokens that either form part of the identified compound phrases or are standalone alphabetic words.\n",
    "2. A 'Filtered Review' applies a more stringent filter, additionally removing common stop words to focus on the more meaningful terms relevant to sentiment analysis.\n",
    "\n",
    "These tokens are then reassembled into coherent strings, forming the basis for deeper linguistic analysis, including lemmatization and stemming. Lemmatization is performed to reduce words to their base or dictionary form, whereas stemming further strips down the words to their root forms, often leading to a more generalized but powerful analysis of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f903c8-1605-447c-a89e-14604a75cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    # Initialise an empty list to store reviews and their associated tags\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    # Open the specified file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the entire content of the file\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        # Pre-process the reviews for further handling (assuming a function 'pre_process_review' exists)\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        # Loop over each review in the processed list\n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                # Split the first part by commas to get tags and strip spaces\n",
    "                tags = parts[0].strip().split(',')\n",
    "                # Take the second part as the review content and strip spaces\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        # Convert the list of tagged reviews into a pandas DataFrame\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        # Return the DataFrame containing the tagged reviews\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    doc = nlp(text)  # Process text with NLP model\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = token.text + \"_\" + token.head.text\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue  # Avoids duplicating compound nouns\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def chuncking_post_process(text):\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if '_' in words[i]:  # Handles compound phrases with underscores\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Skips to next after processing a compound phrase\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "            \n",
    "    return ' '.join(processed_words)  # Returns the processed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings in the 'Review' column, if necessary\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Apply compound phrase preservation to the 'Tokenised_Review' column\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Create a 'Soft_Filtered_Review' column with tokens filtered by conditions\n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings in the 'Soft_Filtered_Review_String' column\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Apply chunking post-process to the soft filtered review strings\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chuncking_post_process)\n",
    "    \n",
    "    # Create a 'Filtered_Review' column applying a stricter filtering with stop words check\n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    \n",
    "    # Convert lists of filtered tokens back to strings in the 'Filtered_Review_String' column\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatize the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    \n",
    "    # Tokenize the lemmatized and filtered review strings\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    # Apply stemming to the tokenized words\n",
    "    df['Stemmed_Review'] = df['Lemmatised_Tokenised_Filtered_Review'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    \n",
    "    # Convert lists of stemmed tokens back to strings in the 'Stemmed_Review_String' column\n",
    "    df['Stemmed_Review_String'] = df['Stemmed_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeceaeb-d6eb-4ccf-a4b9-6a69c0f8432e",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "#### Feature Extraction Process\n",
    "\n",
    "The feature extraction begins with the identification of key nouns and phrases that potentially represent product features. This is achieved through two main processes: **POS Noun Tagging** and **Noun Phrase Chunking**. POS (Part of Speech) Noun Tagging involves parsing reviews to tag parts of speech and extracting nouns, as these often represent features. This process not only identifies individual nouns but also tags them according to their grammatical types, enhancing the accuracy of feature identification.\n",
    "\n",
    "Noun Phrase Chunking goes a step further by extracting coherent noun phrases using Natural Language Processing (NLP) techniques. This process involves analyzing the syntactic patterns in the text to capture phrases that are likely to represent product attributes. These extracted noun phrases are then normalized to lower case to maintain consistency across the dataset.\n",
    "\n",
    "#### Refinement and Semantic Analysis\n",
    "\n",
    "Following the initial extraction, the features undergo a refinement process using two distinct filters: the **Concrete Noun Filter** and the **Semantic Filter**. The Concrete Noun Filter assesses each noun or phrase to determine if it pertains to tangible product attributes. This is done by checking if the nouns align with concrete categories such as objects, devices, or artifacts, using semantic networks like WordNet to understand their hypernym relationships.\n",
    "\n",
    "The Semantic Filter is designed to ensure the relevance of the features to the product being reviewed. It involves a similarity analysis where features are compared against a list of product-related terms to ascertain their pertinence. Features that are semantically related to the identified product categories are retained for further analysis.\n",
    "\n",
    "#### Feature-Sentiment Linkage and Dictionary Construction\n",
    "\n",
    "Each identified and refined feature is then linked with its respective sentiment scores derived from the sentiment analysis phase of the opinion miner. This linkage is crucial as it allows for the aggregation of sentiments specifically associated with individual features. The result is a structured product-feature dictionary where each feature is associated with a compiled sentiment score, facilitating an organized review of consumer opinions.\n",
    "\n",
    "#### Data Structuring and Review Aggregation\n",
    "\n",
    "To enhance the usability of the extracted data, the features and their corresponding sentiments are structured into a DataFrame, which organizes the data by product and feature. This structured format is essential for conducting detailed analysis and supports the aggregation of data across multiple reviews.\n",
    "\n",
    "Additionally, the DataFrame is filtered and refined to ensure that only the most relevant and accurately tagged data is presented. This involves filtering out redundancies and ensuring that the data is presented in a clear and concise manner, making it easy for stakeholders to interpret and make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Extraction - Method 1  \n",
    "\n",
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "\n",
    "    return common_features\n",
    "\n",
    "\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    \"\"\"Check if a noun is concrete by examining its categories.\"\"\"\n",
    "    # Categories that indicate a concrete noun\n",
    "    concrete_indicators = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    # Get all noun meanings of the word\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    # Check each meaning for relevant categories\n",
    "    for synset in synsets:\n",
    "        # Explore each category hierarchy for the word\n",
    "        for hypernym in synset.closure(lambda s: s.hypernyms()):\n",
    "            # Check if any category names indicate a concrete noun\n",
    "            if concrete_indicators.intersection(set(hypernym.lemma_names())):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def Noun_Phrase_Chuncking(string_list):\n",
    "    \n",
    "#     all_noun_phrases = []\n",
    "    \n",
    "#     for review in string_list:\n",
    "#         doc = nlp(review)\n",
    "#         noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "#         all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "#     # Count the occurrences of each noun phrase\n",
    "\n",
    "#     phrase_counts = Counter(all_noun_phrases).most_common(15)\n",
    "    \n",
    "#     # Display most common noun phrases\n",
    "#     common_phrases = phrase_counts\n",
    "#     # display(common_phrases)\n",
    "    \n",
    "#     return common_phrases\n",
    "\n",
    "\n",
    "\n",
    "# def context_based_filter(nouns):\n",
    "#     \"\"\"Filter nouns based on enhanced concrete checks and contextual usage.\"\"\"\n",
    "#     filtered_features = []\n",
    "#     for noun in nouns:\n",
    "#         if is_concrete_noun(noun):\n",
    "#             filtered_features.append(noun)\n",
    "#     return filtered_features\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Concrete noun filter 2 ---------------------\n",
    "\n",
    "# # Define a list of concrete domains (as WordNet synsets)\n",
    "# concrete_domains = [\n",
    "#     wn.synset('artifact.n.01'),  # Artifacts, objects made by humans\n",
    "#     wn.synset('device.n.01'),    # Devices, tools or instruments\n",
    "#     wn.synset('instrumentality.n.03'),  # Instrumentalities, means of achieving an end\n",
    "#     # Add more domains as necessary\n",
    "# ]\n",
    "\n",
    "# def is_related_to_concrete_domain(word):\n",
    "#     \"\"\"\n",
    "#     Check if a word is semantically related to predefined concrete domains.\n",
    "#     \"\"\"\n",
    "#     synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "#     for synset in synsets:\n",
    "#         for domain in concrete_domains:\n",
    "#             if domain in synset.closure(lambda s: s.hypernyms()):\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "# def filter_nouns_semantically(nouns):\n",
    "#     \"\"\"\n",
    "#     Filter a list of nouns, keeping only those related to concrete domains.\n",
    "#     \"\"\"\n",
    "#     return [noun for noun in nouns if is_related_to_concrete_domain(noun)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c0fe2e0-7dad-4c72-acdc-f51a1f2dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    \n",
    "    # Define a threshold for filtering; this is arbitrary and might need adjustment\n",
    "    similarity_threshold = 0.2\n",
    "    \n",
    "    # Extract just the words for similarity comparison\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    # Calculate average similarities and filter\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Get the top N similar words using GloVe and convert to a dictionary for easier access\n",
    "            glove_similar_words = dict(glove_model.most_similar(words[0], topn=10))\n",
    "            # Get the similarity score for the current word from the GloVe model\n",
    "            glove_similarity = glove_similar_words.get(word, 0)  # Default to 0 if word is not found\n",
    "            # Average the similarities\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            # Filter based on the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                filtered_words.append(word)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def further_similar_filter(noun_list):\n",
    "    \n",
    "    main_word = 'picture'\n",
    "    word = 'pic'\n",
    "    \n",
    "    w2v_similarity = Word2Vec_model.wv.similarity(main_word, word)\n",
    "    glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "    glove_similarity = glove_similar_words.get(word, 0) \n",
    "    \n",
    "    items_to_remove = []\n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_list = [item for item in noun_list if item not in items_to_remove]\n",
    "    # print(filtered_list)\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ad8a4c-83a1-49ae-9043-10f271c6aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features_dict(product_info):\n",
    "    # Extract the product title\n",
    "    product_title = product_info[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = product_info[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def build_featured_df(df, feature_list_pos_noun):\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_list = feature_list_pos_noun[1:]\n",
    "    \n",
    "    featured_items_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Convert the review Series to a DataFrame with one row\n",
    "            review_df = review.to_frame().transpose()\n",
    "            \n",
    "            # New: Append the found features as a string (or you can keep it as list)\n",
    "            review_df['Featured_Items'] = [', '.join(featured_items)]  # As a single string of items\n",
    "    \n",
    "            review_df['Main_Index'] = idx\n",
    "            \n",
    "            # Append this review to the feature_df\n",
    "            feature_df = pd.concat([review_df, feature_df], axis=0)\n",
    "            \n",
    "            # Additionally, append the found features to the featured_items_list\n",
    "            featured_items_list.append(featured_items)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def feature_dict_count(feature_df, product_dict):\n",
    "    \n",
    "    sentiment_class_list = feature_df[['Featured_Items', 'Sentiment']]\n",
    "    \n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "    \n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        elif sentiment == -1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "        else:\n",
    "            # print('More than two features, cannot classify')\n",
    "            pass\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def df_filter(df):\n",
    "    df = df[['Main_Index', 'Featured_Items', 'Sentiment', 'Tags', 'Review', 'Tokenised_Review', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String', 'Filtered_Review', 'Filtered_Review_String', 'Lemmatised_Review_String', 'Lemmatised_Tokenised_Filtered_Review', 'Stemmed_Review', 'Stemmed_Review_String']]\n",
    "    df = df.set_index('Main_Index', drop=True) \n",
    "    df.index.name = 'Main Index'\n",
    "    df = df.iloc[::-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989f6a-3ba0-4c77-a556-db97096cacdc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "#### Overview of Sentiment Analysis Process\n",
    "\n",
    "The sentiment analysis process starts with the sentiment controller function, which systematically processes each review to determine sentiment orientation. The function parses the DataFrame containing reviews, where each review is analyzed to extract and evaluate sentiments associated with specific product features. This begins by segmenting the review text into individual features, followed by sentiment classification for each segment.\n",
    "\n",
    "#### Detailed Sentiment Classification Approach\n",
    "\n",
    "The core of sentiment classification is executed through two primary models: a basic rule-based model and a more refined contextual analysis model. The rule-based model, `sentiment_classifier_1`, integrates two distinct sentiment evaluation methods. It first applies a direct sentiment assessment method to compound terms, leveraging the SentiWordNet lexicon for immediate sentiment scores. If direct scores are available, they are considered; otherwise, the model splits the phrase into individual words to calculate an average sentiment score based on the cumulative sentiment values of the constituent words.\n",
    "\n",
    "For a more nuanced interpretation, especially in contexts with negations or intensifiers such as \"not\" or \"very,\" the second method adjusts the sentiment scores accordingly. This involves parsing the phrase, tagging each word with its part of speech, and then adjusting the sentiment scores if modifiers that change sentiment polarity or intensity are detected. This dual-method approach ensures a balanced sentiment evaluation, accounting for both overt and subtle linguistic cues that might influence sentiment perception.\n",
    "\n",
    "#### Handling Complex Sentiment Dynamics\n",
    "\n",
    "Recognizing the limitations of rule-based methods in capturing the full spectrum of human emotions, the sentiment analysis also includes mechanisms to handle complex sentiment dynamics. Adjustments for negations and intensifiers are crucial, as they can significantly alter the sentiment conveyed by a phrase. For instance, the phrase \"not good\" has a different sentiment implication compared to \"good,\" and the system is calibrated to recognize and adjust for such nuances.\n",
    "\n",
    "The sentiment analysis framework is also designed to be robust against the variability in sentiment expression across different reviews. It includes a comprehensive sentiment scoring system that aggregates individual word and phrase scores to form an overall sentiment score for each reviewed feature. This aggregate score is then used to update the sentiment attributes in the feature DataFrame, providing a holistic view of consumer sentiment towards specific product features.\n",
    "\n",
    "#### Integration with Machine Learning Models\n",
    "\n",
    "[Space for future development and integration of machine learning models]\n",
    "\n",
    "This section will detail the incorporation of advanced machine learning models to enhance sentiment analysis capabilities. These models, based on neural networks or other machine learning techniques, will be trained on domain-specific corpora to improve accuracy in sentiment classification, especially in interpreting complex sentence structures and sarcasm, which are often challenging for rule-based systems.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "In conclusion, the sentiment analysis component of my opinion miner is a sophisticated amalgamation of rule-based and potentially machine learning-enhanced methodologies designed to accurately interpret and quantify sentiments expressed in consumer reviews. By effectively addressing the intricacies of language used in consumer feedback, this system not only provides insights into consumer sentiments but also aids businesses in refining their products and strategies based on reliable, data-driven sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ee806a9-15c0-4f67-b2b2-ff82b55d67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(feature_df):\n",
    "    sentiment_list = []\n",
    "    \n",
    "    for idx, entry in feature_df.iterrows():\n",
    "        review = entry['Soft_Filtered_Review_String']\n",
    "\n",
    "        features = entry['Featured_Items'].split(',')\n",
    "        features = [feature.strip() for feature in features] \n",
    "        \n",
    "        if len(features) == 1:\n",
    "            \n",
    "            sentiment = sentiment_classifier_1(review)\n",
    "            sentiment_list.append(sentiment)\n",
    "            \n",
    "        else:\n",
    "            sentiment_list.append(0)\n",
    "\n",
    "    feature_df['Sentiment'] = sentiment_list\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_classifier_1(review):\n",
    "    # Average sentiment method 1 and 2 \n",
    "    pos_score_1, neg_score_1 = get_phrase_sentiment_1(review)\n",
    "    pos_score_2, neg_score_2 = get_phrase_sentiment_2(review)\n",
    "    avg_pos_score = (pos_score_1 + pos_score_2) / 2\n",
    "    avg_neg_score = (neg_score_1 + neg_score_2) / 2\n",
    "\n",
    "    if avg_pos_score > avg_neg_score:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = -1 \n",
    "\n",
    "    # Single model\n",
    "    # pos_score, neg_score = get_phrase_sentiment_2(review)\n",
    "\n",
    "    # if pos_score > neg_score:\n",
    "    #     sentiment = 1\n",
    "    # else:\n",
    "    #     sentiment = -1 \n",
    "\n",
    "    return sentiment \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "- This method assumes equal weighting of words in calculating the average sentiment, which might not reflect the actual sentiment conveyed by the phrase.\n",
    "- Handling negations and intensifiers (e.g., \"not\" in \"not good\", \"very\" in \"very good\") requires more sophisticated logic, as they can significantly alter the sentiment.\n",
    "- Advanced models designed for sentiment analysis at the sentence or document level (like BERT-based models) may provide more accurate sentiment assessments for phrases and sentences by considering the broader context.\n",
    "This approach gives a basic approximation but is limited by the nuances of natural language. For more accurate sentiment analysis on phrases or sentences, consider using pre-trained sentiment analysis models or services.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def get_phrase_sentiment_1(phrase):\n",
    "    # Try the phrase directly (useful for compound terms recognized by SWN)\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # Split the phrase into individual words and average their sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This version attempts to address negations directly before a word and could be extended to consider intensifiers (like \"very\") by further\n",
    "modifying the sentiment scores. For even more nuanced sentiment analysis, exploring deep learning models trained specifically for sentiment \n",
    "analysis is recommended.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"Converts Penn Treebank tags to WordNet tags.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_word_sentiment_2(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(swn.senti_synsets(word, wn_tag))\n",
    "        if synsets:\n",
    "            return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    return 0, 0\n",
    "\n",
    "def adjust_scores_for_negation_and_intensifiers(scores, words, i):\n",
    "    \"\"\"Adjusts sentiment scores based on negations and intensifiers around the i-th word.\"\"\"\n",
    "    if i > 0 and words[i-1].lower() in [\"not\", \"no\"]:\n",
    "        return -scores[0], -scores[1]  # Inverting the sentiment\n",
    "    # Further adjustments for intensifiers (like \"very\") can be added here\n",
    "    return scores\n",
    "\n",
    "def get_phrase_sentiment_2(phrase):\n",
    "    words = word_tokenize(phrase)\n",
    "    tagged = pos_tag(words)\n",
    "    total_pos, total_neg = 0, 0\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        scores = get_word_sentiment_2(word, tag)\n",
    "        scores = adjust_scores_for_negation_and_intensifiers(scores, words, i)\n",
    "        total_pos += scores[0]\n",
    "        total_neg += scores[1]\n",
    "    \n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a16dec-cd09-4680-8d8f-e2c62753d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera': {'picture': {'positive': 8, 'negative': 10},\n",
       "  'shoot': {'positive': 17, 'negative': 5},\n",
       "  'flash': {'positive': 8, 'negative': 10},\n",
       "  'photo': {'positive': 1, 'negative': 0}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def opinion_miner_controller(file):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    nouns = POS_Noun_Tagging(df['Lemmatised_Review_String'])\n",
    "    similar_nouns = similarity_filter(nouns)\n",
    "    features = further_similar_filter(similar_nouns)\n",
    "\n",
    "    if len(features) == 1:\n",
    "        return print(f'Only item name extracted: {features[0]} - No other features')\n",
    "    else:\n",
    "        feature_df = build_featured_df(df, features)\n",
    "        feature_df = sentiment_controller(feature_df)\n",
    "        feature_df = df_filter(feature_df)\n",
    "        product_dict = create_product_features_dict(features)\n",
    "        feature_dict = feature_dict_count(feature_df, copy.deepcopy(product_dict))\n",
    "        return feature_dict, feature_dfA\n",
    "\n",
    "\n",
    "# for file in files:\n",
    "#     feature_dict = opinion_miner_controller(file)\n",
    "#     print(feature_dict)\n",
    "#     print('\\n')\n",
    "\n",
    "feature_dict, feature_df = opinion_miner_controller(files[1])\n",
    "display(feature_dict)\n",
    "# display(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf323b1-e89a-4d76-8cec-5108a353c10e",
   "metadata": {},
   "source": [
    "# Evaluation and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5076e0-3cd3-4b30-96f2-bed28e28ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2738c3b-4b4a-48a0-a722-5e8369a19b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22c114d5-dee0-4319-9522-21da74ef41c1",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3edc5101-3432-407c-b0f9-a60b5caf7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: \n",
      "AlbertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "\n",
    "try:\n",
    "    # Replace 'albert-base-v2' with any model you actually want to test\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "    model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7304dd5-eaec-47e9-a798-a43557454b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262f7cb0f9334994b6725ff797c505c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3152bfd6a234f2884ac4ba183a0fbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d93721b6a04a1fa7e3a04fcef7f58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import spacy\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def extract_features_and_analyze_sentiment(text, features):\n",
    "    # Analyze the text with spaCy for NER\n",
    "    doc = nlp(text)\n",
    "    feature_sentiments = {}\n",
    "\n",
    "    # Identify provided features in text\n",
    "    for ent in doc.ents:\n",
    "        if ent.text.lower() in [feature.lower() for feature in features]:\n",
    "            # Perform sentiment analysis on the sentence containing the feature\n",
    "            sentiment = sentiment_analysis(ent.sent.text)\n",
    "            feature_sentiments[ent.text] = sentiment\n",
    "\n",
    "    return feature_sentiments\n",
    "\n",
    "features = [\"battery life\", \"camera\", \"display\"]\n",
    "text = \"The phone has a great camera and display but the battery life could be better.\"\n",
    "\n",
    "sentiments = extract_features_and_analyze_sentiment(text, features)\n",
    "print(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdfe3f-025f-4074-8aa2-37e563fdffd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb078941-c4c6-4881-9edf-0d0d2199e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = feature_df[feature_df.index == 214]\n",
    "\n",
    "sample_features = sample['Featured_Items'].iloc[0].split(',')\n",
    "sample_review = sample['Review'].iloc[0]\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_feature_sentiment(sentence, features):\n",
    "    # Dictionary to store sentiment results for each feature\n",
    "    sentiment_results = {}\n",
    "\n",
    "    features = [feature.strip() for feature in features]  \n",
    "    \n",
    "    # Analyze sentiment for each feature\n",
    "    for feature in features:\n",
    "        # Extract context around the feature if needed (optional improvement)\n",
    "        start_index = sentence.lower().find(feature.lower())\n",
    "        if start_index != -1:\n",
    "            # Extract a sub-sentence for context-based sentiment analysis\n",
    "            sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "            # Get sentiment using TextBlob\n",
    "            tb_sentiment = TextBlob(sub_sentence).sentiment.polarity\n",
    "            # Get sentiment using VADER\n",
    "            vader_sentiment = analyzer.polarity_scores(sub_sentence)['compound']\n",
    "\n",
    "            # Store results\n",
    "            sentiment_results[feature] = {\n",
    "                'TextBlob Sentiment': 'Positive' if tb_sentiment > 0 else 'Negative' if tb_sentiment < 0 else 'Neutral',\n",
    "                'VADER Sentiment': 'Positive' if vader_sentiment > 0.05 else 'Negative' if vader_sentiment < -0.05 else 'Neutral'\n",
    "            }\n",
    "        else:\n",
    "            sentiment_results[feature] = {\n",
    "                'TextBlob Sentiment': 'Not Found',\n",
    "                'VADER Sentiment': 'Not Found'\n",
    "            }\n",
    "\n",
    "    return sentiment_results\n",
    "\n",
    "results = analyze_feature_sentiment(sample_review, sample_features)\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d61341-dc5f-4724-bc4d-0a4987e986fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca724af2-3702-4e17-8d4a-838f492d777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94d9a2-517e-4bab-aa42-2f1724827fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "59601c91-4553-4363-95eb-ccf1249e53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_freq_dist(df):\n",
    "#     all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "#     freq_dist = FreqDist(all_words)\n",
    "#     top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "#     words, frequencies = zip(*top_items)\n",
    "#     plt.figure(figsize=(6, 3))  \n",
    "#     plt.bar(words, frequencies, color='skyblue')  \n",
    "#     plt.xlabel('Words') \n",
    "#     plt.ylabel('Frequency') \n",
    "#     plt.title('Top Words Frequency Distribution')  \n",
    "#     plt.xticks(rotation=45) \n",
    "#     plt.show()\n",
    "\n",
    "# display_freq_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5708071f-356a-4d42-b0f0-cb283608abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def k_means(string_list):\n",
    "\n",
    "#     num_clusters = 3\n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "#     km = KMeans(n_clusters=num_clusters, n_init=10)\n",
    "#     km.fit(tfidf_matrix)\n",
    "#     clusters = km.labels_.tolist()\n",
    "    \n",
    "#     order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "#     terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "#     for i in range(num_clusters):\n",
    "#         top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "#         print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "#     pca = PCA(n_components=2)\n",
    "#     reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "#     # Get the cluster labels for each data point\n",
    "#     cluster_labels = km.labels_\n",
    "    \n",
    "#     plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "#     # Scatter plot of the reduced data, colored by cluster labels\n",
    "#     plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "#     # Adding labels for axes\n",
    "#     plt.xlabel('PCA 1')\n",
    "#     plt.ylabel('PCA 2')\n",
    "    \n",
    "#     # Title of the plot\n",
    "#     plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "#     # Display the plot\n",
    "#     print('\\n')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# df = read_file(files[1])\n",
    "# df = pre_processing_controller(df)\n",
    "# # k_means(df['Filtered_Review_String'])\n",
    "# # k_means(df['Lemmatised_Review_String'])\n",
    "# # k_means(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c1962340-a817-4beb-a57e-ed1665c3da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "#     # Create a dictionary representation of the documents\n",
    "#     dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "#     # Convert dictionary to a bag of words corpus\n",
    "#     corpus = [dictionary.doc2bow(text) for text in tokenised_reviews]\n",
    "    \n",
    "#     # Number of topics\n",
    "#     num_topics = 5\n",
    "    \n",
    "#     # Generate LDA model\n",
    "#     lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "#     # Print the topics\n",
    "#     topics = lda_model.print_topics(num_words=5)\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "\n",
    "# # LDA_Model(df['Stemmed_Review'])\n",
    "# # print('\\n')\n",
    "# # LDA_Model(df['Filtered_Review'])\n",
    "# # print('\\n')\n",
    "# # LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "85ebafc8-576a-4a39-b57b-9bcc3f7353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA_Model_2(string_reviews):\n",
    "    \n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "#     num_topics = 5\n",
    "#     lda = LDA(n_components=num_topics)\n",
    "#     lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#     # Explore the topics\n",
    "#     terms = tfidf_vectorizer.get_feature_names_out()\n",
    "#     for topic_idx, topic in enumerate(lda.components_):\n",
    "#         print(f\"Topic #{topic_idx+1}:\")\n",
    "#         print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "#         print('\\n')\n",
    "\n",
    "# # LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "cce48f0e-fdf7-4791-9a29-dda9356e2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def disambiguate_word_sense(sentence, word):\n",
    "#     # Use Lesk algorithm for WSD\n",
    "#     sense = lesk(nltk.word_tokenize(sentence), word)\n",
    "#     if not sense:\n",
    "#         return None\n",
    "    \n",
    "#     # Get sentiment scores\n",
    "#     senti_synset = swn.senti_synset(sense.name())\n",
    "#     return {\n",
    "#         'word': word,\n",
    "#         'synset_name': sense.name(),\n",
    "#         'definition': sense.definition(),\n",
    "#         'examples': sense.examples(),\n",
    "#         'positivity_score': senti_synset.pos_score(),\n",
    "#         'negativity_score': senti_synset.neg_score(),\n",
    "#         'objectivity_score': senti_synset.obj_score()\n",
    "#     }\n",
    "\n",
    "\n",
    "# # review = df.iloc[200]\n",
    "# # for word in review['Filtered_Review']:\n",
    "# #     disambiguated_sense = disambiguate_word_sense(review['Review'], word)\n",
    "#     # print(disambiguated_sense)\n",
    "#     # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6dd1ad5f-dca4-44ec-9a8b-723aa88e3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_and_ner(tokens):\n",
    "    \n",
    "#     tagged = pos_tag(tokens)\n",
    "#     named_entities = ne_chunk(tagged)\n",
    "#     return named_entities\n",
    "\n",
    "# # tokenised_text = df['Filtered_Review'].iloc[106]\n",
    "# # named_entities = preprocess_and_ner(tokenised_text)\n",
    "# # print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fd50c8c5-1174-4ce7-8811-144d3db421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf(reviews):\n",
    "#     # Initialize the TF-IDF Vectorizer\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "#     # Transform the reviews into a TF-IDF matrix\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    \n",
    "#     # Extract the feature names/terms from the TF-IDF Vectorizer\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "#     # Calculate the average TF-IDF score for each term across all documents\n",
    "#     scores = tfidf_matrix.mean(axis=0)\n",
    "#     term_scores = {feature_names[col]: scores[0, col] for col in range(scores.shape[1])}\n",
    "    \n",
    "#     # Sort the terms by their average TF-IDF score in descending order\n",
    "#     sorted_term_scores = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # Optionally: Display the top 10 terms with the highest average TF-IDF scores\n",
    "#     print(\"Top 15 terms by average TF-IDF score:\")\n",
    "#     for term, score in sorted_term_scores[:15]:\n",
    "#         print(f\"Term: {term}, Score: {round(score, 4)}\")\n",
    "    \n",
    "#     # Calculate cosine similarity among the documents using the TF-IDF matrix\n",
    "#     cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "#     # return tfidf_matrix, feature_names, cos_sim_matrix\n",
    "\n",
    "# # tf_idf(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "dd663a77-0eb2-48eb-bc2e-0ec273f85e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# # Define custom patterns\n",
    "# patterns = [\n",
    "#     [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}],  # Adjective followed by one or more nouns\n",
    "#     [{\"POS\": \"NOUN\", \"OP\": \"+\"}, {\"LOWER\": \"mode\"}]  # One or more nouns followed by \"mode\"\n",
    "# ]\n",
    "# matcher.add(\"CUSTOM_PATTERNS\", patterns)\n",
    "\n",
    "\n",
    "# def POS_Chuck_Parser_Matcher(review):\n",
    "#     doc = nlp(review)\n",
    "#     matches = matcher(doc)\n",
    "\n",
    "#     extracted_phrases = []\n",
    "#     for match_id, start, end in matches:\n",
    "#         span = doc[start:end]\n",
    "#         extracted_phrases.append(span.text)\n",
    "\n",
    "#     print(extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d5be75d1-de50-40ea-bcb6-54e5ffddbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def POS_Chuck_Parser(review):\n",
    "\n",
    "#     doc = nlp(review)\n",
    "    \n",
    "#     # Initialize a list to hold our extracted phrases\n",
    "#     extracted_phrases = []\n",
    "    \n",
    "#     # Iterate over tokens in the doc\n",
    "#     for token in doc:\n",
    "#         # Look for an adverb modifying an adjective and check the adjective doesn't have a noun child\n",
    "#         if token.pos_ == \"ADV\" and token.head.pos_ == \"ADJ\":\n",
    "#             is_adj_modified = False\n",
    "#             for child in token.head.children:\n",
    "#                 if child.dep_ in [\"attr\", \"dobj\", \"pobj\"]:  # The adjective is modifying a noun\n",
    "#                     is_adj_modified = True\n",
    "#                     break\n",
    "#             if not is_adj_modified:\n",
    "#                 # Capture the adverb-adjective pair \"rather heavy\"\n",
    "#                 extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "    \n",
    "#         # Look for an adjective modifying a noun and check if it's in a prepositional phrase\n",
    "#         if token.pos_ == \"ADJ\" and token.head.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#             is_in_prep_phrase = False\n",
    "#             for ancestor in token.head.ancestors:\n",
    "#                 if ancestor.dep_ == \"prep\":\n",
    "#                     is_in_prep_phrase = True\n",
    "#                     breakg\n",
    "#             if not is_in_prep_phrase:\n",
    "#                 # Capture the adjective-noun pair \"great camera\"\n",
    "#                 extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "\n",
    "#     print(extracted_phrases)\n",
    "\n",
    "# # index = 24\n",
    "\n",
    "# # print(df['Tags'].iloc[index])\n",
    "# # print(df['Review'].iloc[index])\n",
    "\n",
    "# # POS_Chuck_Parser(df['Review'].iloc[index])\n",
    "# # POS_Chuck_Parser(df['Filtered_Review_String'].iloc[index])\n",
    "# # POS_Chuck_Parser(df['Lemmatised_Review_String'].iloc[index])\n",
    "\n",
    "# # POS_Chuck_Parser_Matcher(df['Review'].iloc[index])\n",
    "# # POS_Chuck_Parser_Matcher(df['Filtered_Review_String'].iloc[index])\n",
    "# # POS_Chuck_Parser_Matcher(df['Lemmatised_Review_String'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9f25a-1ee5-4cee-86e9-e08b1620ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
