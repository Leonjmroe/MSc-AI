{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        if reviews[0] == '*****************************************************************************':\n",
    "            reviews = reviews[11:]\n",
    "        \n",
    "        for review in reviews:\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "            \n",
    "\n",
    "df = read_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) \n",
    "df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: word_tokenize(review))\n",
    "df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words])\n",
    "df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8b091620-4c70-40c6-a91d-ae7740aa4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "def display_freq_dist(freq_dist):\n",
    "    top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words, frequencies = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))  \n",
    "    plt.bar(words, frequencies, color='skyblue')  \n",
    "    plt.xlabel('Words') \n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Top Words Frequency Distribution')  \n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "# display_freq_dist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "30dcda76-34ae-4551-9a1a-d339bbdb8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(string_list):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "    km = KMeans(n_clusters=5, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "        print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get the cluster labels for each data point\n",
    "    cluster_labels = km.labels_\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "    # Scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    # Adding labels for axes\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "    # Title of the plot\n",
    "    plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "    # Display the plot\n",
    "    print('\\n')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# k_means(df['Lemmatized_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    print(common_features)\n",
    "\n",
    "# POS_Noun_Tagging(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "1da73d6b-eae7-40f1-a3fe-c593c1b2c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Noun_Phrase_Chuncking(string_list):\n",
    "\n",
    "    all_noun_phrases = []\n",
    "    \n",
    "    for review in string_list:\n",
    "        doc = nlp(review)\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "        all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "    # Count the occurrences of each noun phrase\n",
    "    from collections import Counter\n",
    "    phrase_counts = Counter(all_noun_phrases)\n",
    "    \n",
    "    # Display most common noun phrases\n",
    "    common_phrases = phrase_counts.most_common(15)\n",
    "    print(common_phrases)\n",
    "    \n",
    "# Noun_Phrase_Chuncking(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "216c365d-9bd2-489e-b263-2a3e41bd1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "    # Convert dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Number of topics\n",
    "    num_topics = 5\n",
    "    \n",
    "    # Generate LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "    # Print the topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "\n",
    "# LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a7507d87-5db0-4c2c-bafc-a2aefb58a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model_2(string_reviews):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "    num_topics = 5\n",
    "    lda = LDA(n_components=num_topics)\n",
    "    lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Explore the topics\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "        print('\\n')\n",
    "\n",
    "# LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649b587-2093-4f91-aab2-6ffc8efbf89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
