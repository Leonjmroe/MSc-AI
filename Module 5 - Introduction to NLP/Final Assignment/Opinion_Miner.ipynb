{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        if reviews[0] == '*****************************************************************************':\n",
    "            reviews = reviews[11:]\n",
    "        \n",
    "        for review in reviews:\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "            \n",
    "\n",
    "df = read_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) \n",
    "df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: word_tokenize(review))\n",
    "df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words])\n",
    "df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "df['Tokenised_Review_String'] = df['Tokenised_Review'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8b091620-4c70-40c6-a91d-ae7740aa4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "def display_freq_dist(freq_dist):\n",
    "    top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words, frequencies = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))  \n",
    "    plt.bar(words, frequencies, color='skyblue')  \n",
    "    plt.xlabel('Words') \n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Top Words Frequency Distribution')  \n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "# display_freq_dist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "30dcda76-34ae-4551-9a1a-d339bbdb8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(string_list):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "    km = KMeans(n_clusters=5, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "        print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get the cluster labels for each data point\n",
    "    cluster_labels = km.labels_\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "    # Scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    # Adding labels for axes\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "    # Title of the plot\n",
    "    plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "    # Display the plot\n",
    "    print('\\n')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# k_means(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 293), ('dvd', 155), ('player', 144), ('t', 99), (']', 87), ('apex', 52), ('dvds', 34), ('picture', 32), ('problems', 30), ('price', 30)]\n"
     ]
    }
   ],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(10)\n",
    "    print(common_features)\n",
    "\n",
    "POS_Noun_Tagging(df['Tokenised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1da73d6b-eae7-40f1-a3fe-c593c1b2c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The battery life', 'the screen']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample review text\n",
    "review = \"The battery life is amazing but the screen could be brighter.\"\n",
    "\n",
    "# Process the review text with SpaCy\n",
    "doc = nlp(review)\n",
    "\n",
    "# Extract noun phrases (chunking)\n",
    "noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "print(noun_phrases)\n",
    "\n",
    "\n",
    "reviews = [\n",
    "    \"[t] troubleshooting ad-2500 and ad-2600 no picture scrolling...\",\n",
    "    # Add more reviews as necessary\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store noun phrases from all reviews\n",
    "all_noun_phrases = []\n",
    "\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "    all_noun_phrases.extend(noun_phrases)\n",
    "\n",
    "# Count the occurrences of each noun phrase\n",
    "from collections import Counter\n",
    "phrase_counts = Counter(all_noun_phrases)\n",
    "\n",
    "# Display most common noun phrases\n",
    "common_phrases = phrase_counts.most_common(10)\n",
    "print(common_phrases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49d4d2-a420-417c-8372-9e3caef23bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
