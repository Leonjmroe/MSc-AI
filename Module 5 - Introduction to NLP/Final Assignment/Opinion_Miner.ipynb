{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message='Discarded redundant search for Synset')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import deaccent\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f903c8-1605-447c-a89e-14604a75cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        if reviews[0] == '*' * 77:\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "\n",
    "    processed_reviews = []\n",
    "    title_switch = False\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):\n",
    "            title = review[3:]\n",
    "            title_switch = True\n",
    "        elif title_switch:\n",
    "            appended_review = review + title \n",
    "            processed_reviews.append(appended_review)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = token.text + \"_\" + token.head.text\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "    \n",
    "    return processed_tokens\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def chuncking_post_process(text):\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        current_word = words[i]\n",
    "        processed_words.append(current_word)\n",
    "        \n",
    "        if '_' in current_word:\n",
    "            i += 1\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            if i < len(words):\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1  \n",
    "            \n",
    "    return ' '.join(processed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) \n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chuncking_post_process)\n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    df['Stemmed_Review'] = df['Lemmatised_Tokenised_Filtered_Review'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    df['Stemmed_Review_String'] = df['Stemmed_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b091620-4c70-40c6-a91d-ae7740aa4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_freq_dist(df):\n",
    "    all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words, frequencies = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))  \n",
    "    plt.bar(words, frequencies, color='skyblue')  \n",
    "    plt.xlabel('Words') \n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Top Words Frequency Distribution')  \n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "# display_freq_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30dcda76-34ae-4551-9a1a-d339bbdb8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(string_list):\n",
    "\n",
    "    num_clusters = 2\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "        print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get the cluster labels for each data point\n",
    "    cluster_labels = km.labels_\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "    # Scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    # Adding labels for axes\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "    # Title of the plot\n",
    "    plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "    # Display the plot\n",
    "    print('\\n')\n",
    "    plt.show()\n",
    "\n",
    "# k_means(df['Filtered_Review_String'])\n",
    "# k_means(df['Lemmatised_Review_String'])\n",
    "# k_means(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    # display(common_features)\n",
    "\n",
    "    return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da73d6b-eae7-40f1-a3fe-c593c1b2c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Noun_Phrase_Chuncking(string_list):\n",
    "    \n",
    "    all_noun_phrases = []\n",
    "    \n",
    "    for review in string_list:\n",
    "        doc = nlp(review)\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "        all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "    # Count the occurrences of each noun phrase\n",
    "    from collections import Counter\n",
    "    phrase_counts = Counter(all_noun_phrases).most_common(15)\n",
    "    \n",
    "    # Display most common noun phrases\n",
    "    common_phrases = phrase_counts\n",
    "    # display(common_phrases)\n",
    "    \n",
    "    return common_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00936935-8ff4-4d9d-9c8f-1b29dc9adbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Concrete noun filter 1 ---------------------\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    \"\"\"Enhanced check if a noun is concrete based on its hypernyms and usage context.\"\"\"\n",
    "    concrete_clues = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        for hyper in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_clues.intersection(set(hyper.lemma_names())):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def context_based_filter(nouns):\n",
    "    \"\"\"Filter nouns based on enhanced concrete checks and contextual usage.\"\"\"\n",
    "    filtered_features = []\n",
    "    for noun in nouns:\n",
    "        if is_concrete_noun(noun):\n",
    "            filtered_features.append(noun)\n",
    "    return filtered_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Concrete noun filter 2 ---------------------\n",
    "\n",
    "# Define a list of concrete domains (as WordNet synsets)\n",
    "concrete_domains = [\n",
    "    wn.synset('artifact.n.01'),  # Artifacts, objects made by humans\n",
    "    wn.synset('device.n.01'),    # Devices, tools or instruments\n",
    "    wn.synset('instrumentality.n.03'),  # Instrumentalities, means of achieving an end\n",
    "    # Add more domains as necessary\n",
    "]\n",
    "\n",
    "def is_related_to_concrete_domain(word):\n",
    "    \"\"\"\n",
    "    Check if a word is semantically related to predefined concrete domains.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        for domain in concrete_domains:\n",
    "            if domain in synset.closure(lambda s: s.hypernyms()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_nouns_semantically(nouns):\n",
    "    \"\"\"\n",
    "    Filter a list of nouns, keeping only those related to concrete domains.\n",
    "    \"\"\"\n",
    "    return [noun for noun in nouns if is_related_to_concrete_domain(noun)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0fe2e0-7dad-4c72-acdc-f51a1f2dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    \n",
    "    # Define a threshold for filtering; this is arbitrary and might need adjustment\n",
    "    similarity_threshold = 0.2\n",
    "    \n",
    "    # Extract just the words for similarity comparison\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    # Calculate average similarities and filter\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Get the top N similar words using GloVe and convert to a dictionary for easier access\n",
    "            glove_similar_words = dict(glove_model.most_similar(words[0], topn=10))\n",
    "            # Get the similarity score for the current word from the GloVe model\n",
    "            glove_similarity = glove_similar_words.get(word, 0)  # Default to 0 if word is not found\n",
    "            # Average the similarities\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            # Filter based on the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                # print(word, w2v_similarity, glove_similarity)\n",
    "                filtered_words.append(word)\n",
    "        except KeyError:\n",
    "            # Word not in vocabulary\n",
    "            # print(f\"Word '{word}' not found in one of the model's vocabulary.\")\n",
    "            pass\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45844c88-e4b8-4d65-bbb1-86d735268e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_similar_filter(noun_list):\n",
    "    \n",
    "    main_word = 'picture'\n",
    "    word = 'pic'\n",
    "    \n",
    "    w2v_similarity = Word2Vec_model.wv.similarity(main_word, word)\n",
    "    glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "    glove_similarity = glove_similar_words.get(word, 0) \n",
    "    \n",
    "    items_to_remove = []\n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_list = [item for item in noun_list if item not in items_to_remove]\n",
    "    # print(filtered_list)\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8aca312-1e90-4263-bf44-87049995b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features_dict(product_info):\n",
    "    # Extract the product title\n",
    "    product_title = product_info[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = product_info[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8ad8a4c-83a1-49ae-9043-10f271c6aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_featured_df(df, feature_list_pos_noun):\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_list = feature_list_pos_noun[1:]\n",
    "    \n",
    "    featured_items_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Convert the review Series to a DataFrame with one row\n",
    "            review_df = review.to_frame().transpose()\n",
    "            \n",
    "            # New: Append the found features as a string (or you can keep it as list)\n",
    "            review_df['Featured_Items'] = [', '.join(featured_items)]  # As a single string of items\n",
    "    \n",
    "            review_df['Main_Index'] = idx\n",
    "            /\n",
    "            # Append this review to the feature_df\n",
    "            feature_df = pd.concat([review_df, feature_df], axis=0)\n",
    "            \n",
    "            # Additionally, append the found features to the featured_items_list\n",
    "            featured_items_list.append(featured_items)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ee806a9-15c0-4f67-b2b2-ff82b55d67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_classifier_1(feature_df):\n",
    "    sentiment_list = []\n",
    "    \n",
    "    for idx, entry in feature_df.iterrows():\n",
    "        review = entry['Soft_Filtered_Review_String']\n",
    "        \n",
    "        if len(entry['Featured_Items'].split(',')) == 1:\n",
    "            \n",
    "            pos_score_1, neg_score_1 = get_phrase_sentiment_1(review)\n",
    "            pos_score_2, neg_score_2 = get_phrase_sentiment_2(review)\n",
    "            avg_pos_score = (pos_score_1 + pos_score_2) / 2\n",
    "            avg_neg_score = (neg_score_1 + neg_score_2) / 2\n",
    "        \n",
    "            if avg_pos_score > avg_neg_score:\n",
    "                sentiment = 1\n",
    "            else:\n",
    "                sentiment = -1 \n",
    "\n",
    "            sentiment_list.append(sentiment)\n",
    "        else:\n",
    "            sentiment_list.append(0)\n",
    "\n",
    "    feature_df['Sentiment'] = sentiment_list\n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73df1ccb-8044-42d4-a0fe-f04edbeceb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dict_count(feature_df, product_dict):\n",
    "    \n",
    "    sentiment_class_list = feature_df[['Featured_Items', 'Sentiment']]\n",
    "    \n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "    \n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        elif sentiment == -1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "        else:\n",
    "            # print('More than two features, cannot classify')\n",
    "            pass\n",
    "\n",
    "    return product_dict\n",
    "    \n",
    "\n",
    "# feature_dict = feature_dict_count(feature_df, product_dict)\n",
    "# display(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c4b99a2-fe9a-48cb-8f55-d86d57381078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "216c365d-9bd2-489e-b263-2a3e41bd1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "    # Convert dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenised_reviews]\n",
    "    \n",
    "    # Number of topics\n",
    "    num_topics = 5\n",
    "    \n",
    "    # Generate LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "    # Print the topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "\n",
    "# LDA_Model(df['Stemmed_Review'])\n",
    "# print('\\n')\n",
    "# LDA_Model(df['Filtered_Review'])\n",
    "# print('\\n')\n",
    "# LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7507d87-5db0-4c2c-bafc-a2aefb58a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model_2(string_reviews):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "    num_topics = 5\n",
    "    lda = LDA(n_components=num_topics)\n",
    "    lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Explore the topics\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "        print('\\n')\n",
    "\n",
    "# LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30a1b452-8838-4abb-9c1f-8a291f68b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting the sentiment of multiword phrases using SentiWordNet (SWN) through the NLTK library requires a more nuanced approach compared to single words because SentiWordNet does not directly provide sentiment scores for phrases. Instead, you can average the sentiment scores of individual words in the phrase, adjust the methodology to consider the phrase's context, or use compound term lookup techniques where applicable. Hereâ€™s an approach to approximate sentiment for multiword phrases:\n",
    "\n",
    "### Step 1: Handle Compound Terms\n",
    "\n",
    "Some compound terms might be recognized by SentiWordNet if connected by underscores (e.g., \"well_known\"). Before splitting the phrase into individual words, check if the compound term has an entry in SentiWordNet.\n",
    "\n",
    "### Step 2: Average Sentiment Scores of Individual Words\n",
    "\n",
    "For phrases not recognized as compound terms, calculate the average sentiment scores of the individual words. Consideration of part-of-speech tags can enhance accuracy, but for simplicity, this example ignores POS tags.\n",
    "\n",
    "### Note:\n",
    "\n",
    "- This method assumes equal weighting of words in calculating the average sentiment, which might not reflect the actual sentiment conveyed by the phrase.\n",
    "- Handling negations and intensifiers (e.g., \"not\" in \"not good\", \"very\" in \"very good\") requires more sophisticated logic, as they can significantly alter the sentiment.\n",
    "- Advanced models designed for sentiment analysis at the sentence or document level (like BERT-based models) may provide more accurate sentiment assessments for phrases and sentences by considering the broader context.\n",
    "\n",
    "This approach gives a basic approximation but is limited by the nuances of natural language. For more accurate sentiment analysis on phrases or sentences, consider using pre-trained sentiment analysis models or services.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def get_phrase_sentiment_1(phrase):\n",
    "    # Try the phrase directly (useful for compound terms recognized by SWN)\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # Split the phrase into individual words and average their sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This version attempts to address negations directly before a word and could be extended to consider intensifiers (like \"very\") by further\n",
    "modifying the sentiment scores. For even more nuanced sentiment analysis, exploring deep learning models trained specifically for sentiment \n",
    "analysis is recommended.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"Converts Penn Treebank tags to WordNet tags.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_word_sentiment_2(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(swn.senti_synsets(word, wn_tag))\n",
    "        if synsets:\n",
    "            return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    return 0, 0\n",
    "\n",
    "def adjust_scores_for_negation_and_intensifiers(scores, words, i):\n",
    "    \"\"\"Adjusts sentiment scores based on negations and intensifiers around the i-th word.\"\"\"\n",
    "    if i > 0 and words[i-1].lower() in [\"not\", \"no\"]:\n",
    "        return -scores[0], -scores[1]  # Inverting the sentiment\n",
    "    # Further adjustments for intensifiers (like \"very\") can be added here\n",
    "    return scores\n",
    "\n",
    "def get_phrase_sentiment_2(phrase):\n",
    "    words = word_tokenize(phrase)\n",
    "    tagged = pos_tag(words)\n",
    "    total_pos, total_neg = 0, 0\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        scores = get_word_sentiment_2(word, tag)\n",
    "        scores = adjust_scores_for_negation_and_intensifiers(scores, words, i)\n",
    "        total_pos += scores[0]\n",
    "        total_neg += scores[1]\n",
    "    \n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52e8c9bc-bdef-4e00-9ee8-82efafa87da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_word_sense(sentence, word):\n",
    "    # Use Lesk algorithm for WSD\n",
    "    sense = lesk(nltk.word_tokenize(sentence), word)\n",
    "    if not sense:\n",
    "        return None\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    senti_synset = swn.senti_synset(sense.name())\n",
    "    return {\n",
    "        'word': word,\n",
    "        'synset_name': sense.name(),\n",
    "        'definition': sense.definition(),\n",
    "        'examples': sense.examples(),\n",
    "        'positivity_score': senti_synset.pos_score(),\n",
    "        'negativity_score': senti_synset.neg_score(),\n",
    "        'objectivity_score': senti_synset.obj_score()\n",
    "    }\n",
    "\n",
    "\n",
    "# review = df.iloc[200]\n",
    "# for word in review['Filtered_Review']:\n",
    "#     disambiguated_sense = disambiguate_word_sense(review['Review'], word)\n",
    "    # print(disambiguated_sense)\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e0fb0d2-93ff-4477-84ce-f0d12a786506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_ner(tokens):\n",
    "    \n",
    "    tagged = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    return named_entities\n",
    "\n",
    "# tokenised_text = df['Filtered_Review'].iloc[106]\n",
    "# named_entities = preprocess_and_ner(tokenised_text)\n",
    "# print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d74f075-44ca-41a1-9f91-10dd36ffef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(reviews):\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Transform the reviews into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    # Extract the feature names/terms from the TF-IDF Vectorizer\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate the average TF-IDF score for each term across all documents\n",
    "    scores = tfidf_matrix.mean(axis=0)\n",
    "    term_scores = {feature_names[col]: scores[0, col] for col in range(scores.shape[1])}\n",
    "    \n",
    "    # Sort the terms by their average TF-IDF score in descending order\n",
    "    sorted_term_scores = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Optionally: Display the top 10 terms with the highest average TF-IDF scores\n",
    "    print(\"Top 15 terms by average TF-IDF score:\")\n",
    "    for term, score in sorted_term_scores[:15]:\n",
    "        print(f\"Term: {term}, Score: {round(score, 4)}\")\n",
    "    \n",
    "    # Calculate cosine similarity among the documents using the TF-IDF matrix\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # return tfidf_matrix, feature_names, cos_sim_matrix\n",
    "\n",
    "# tf_idf(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c0c8c0e-9282-4752-81f2-80a1f52ad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define custom patterns\n",
    "patterns = [\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}],  # Adjective followed by one or more nouns\n",
    "    [{\"POS\": \"NOUN\", \"OP\": \"+\"}, {\"LOWER\": \"mode\"}]  # One or more nouns followed by \"mode\"\n",
    "]\n",
    "matcher.add(\"CUSTOM_PATTERNS\", patterns)\n",
    "\n",
    "\n",
    "def POS_Chuck_Parser_Matcher(review):\n",
    "    doc = nlp(review)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    extracted_phrases = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        extracted_phrases.append(span.text)\n",
    "\n",
    "    print(extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a873b5ed-1ecc-4178-9428-6ed206e7ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Chuck_Parser(review):\n",
    "\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    # Initialize a list to hold our extracted phrases\n",
    "    extracted_phrases = []\n",
    "    \n",
    "    # Iterate over tokens in the doc\n",
    "    for token in doc:\n",
    "        # Look for an adverb modifying an adjective and check the adjective doesn't have a noun child\n",
    "        if token.pos_ == \"ADV\" and token.head.pos_ == \"ADJ\":\n",
    "            is_adj_modified = False\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in [\"attr\", \"dobj\", \"pobj\"]:  # The adjective is modifying a noun\n",
    "                    is_adj_modified = True\n",
    "                    break\n",
    "            if not is_adj_modified:\n",
    "                # Capture the adverb-adjective pair \"rather heavy\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "    \n",
    "        # Look for an adjective modifying a noun and check if it's in a prepositional phrase\n",
    "        if token.pos_ == \"ADJ\" and token.head.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            is_in_prep_phrase = False\n",
    "            for ancestor in token.head.ancestors:\n",
    "                if ancestor.dep_ == \"prep\":\n",
    "                    is_in_prep_phrase = True\n",
    "                    break\n",
    "            if not is_in_prep_phrase:\n",
    "                # Capture the adjective-noun pair \"great camera\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "\n",
    "    print(extracted_phrases)\n",
    "\n",
    "# index = 24\n",
    "\n",
    "# print(df['Tags'].iloc[index])\n",
    "# print(df['Review'].iloc[index])\n",
    "\n",
    "# POS_Chuck_Parser(df['Review'].iloc[index])\n",
    "# POS_Chuck_Parser(df['Filtered_Review_String'].iloc[index])\n",
    "# POS_Chuck_Parser(df['Lemmatised_Review_String'].iloc[index])\n",
    "\n",
    "# POS_Chuck_Parser_Matcher(df['Review'].iloc[index])\n",
    "# POS_Chuck_Parser_Matcher(df['Filtered_Review_String'].iloc[index])\n",
    "# POS_Chuck_Parser_Matcher(df['Lemmatised_Review_String'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09a16dec-cd09-4680-8d8f-e2c62753d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_miner_controller(file):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    nouns = POS_Noun_Tagging(df['Lemmatised_Review_String'])\n",
    "    similar_nouns = similarity_filter(nouns)\n",
    "    features = further_similar_filter(similar_nouns)\n",
    "\n",
    "    if len(features) == 1:\n",
    "        return print(f'Only item name extracted: {features[0]} - No other features')\n",
    "    else:\n",
    "        feature_df = build_featured_df(df, features)\n",
    "        feature_df = sentiment_classifier_1(feature_df)\n",
    "        product_dict = create_product_features_dict(features)\n",
    "        feature_dict = feature_dict_count(feature_df, product_dict)\n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de60fda3-f5e6-4746-a358-e0110c944480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'play': {'player': {'positive': 29, 'negative': 26}, 'feature': {'positive': 4, 'negative': 5}}}\n",
      "\n",
      "\n",
      "{'camera': {'picture': {'positive': 8, 'negative': 10}, 'shoot': {'positive': 18, 'negative': 4}, 'flash': {'positive': 10, 'negative': 8}, 'photo': {'positive': 1, 'negative': 0}}}\n",
      "\n",
      "\n",
      "Only item name extracted: player - No other features\n",
      "None\n",
      "\n",
      "\n",
      "{'camera': {'picture': {'positive': 6, 'negative': 6}, 'battery': {'positive': 4, 'negative': 12}, 'mode': {'positive': 7, 'negative': 3}, 'zoom': {'positive': 5, 'negative': 2}}}\n",
      "\n",
      "\n",
      "{'phone': {'service': {'positive': 8, 'negative': 5}, 'radio': {'positive': 10, 'negative': 1}}}\n",
      "\n",
      "\n",
      "{'monitor': {'computer': {'positive': 14, 'negative': 8}, 'display': {'positive': 8, 'negative': 3}, 'screen': {'positive': 5, 'negative': 10}}}\n",
      "\n",
      "\n",
      "{'router': {'network': {'positive': 17, 'negative': 14}, 'device': {'positive': 11, 'negative': 5}, 'setup': {'positive': 14, 'negative': 7}, 'firmware': {'positive': 10, 'negative': 12}}}\n",
      "\n",
      "\n",
      "{'speaker': {'unit': {'positive': 11, 'negative': 5}}}\n",
      "\n",
      "\n",
      "{'camera': {'picture': {'positive': 1, 'negative': 0}, 'shoot': {'positive': 4, 'negative': 0}, 'shot': {'positive': 4, 'negative': 2}, 'flash': {'positive': 5, 'negative': 4}}}\n",
      "\n",
      "\n",
      "{'camera': {'picture': {'positive': 1, 'negative': 4}, 'battery': {'positive': 8, 'negative': 7}, 'zoom': {'positive': 2, 'negative': 4}, 'shot': {'positive': 1, 'negative': 2}}}\n",
      "\n",
      "\n",
      "{'diaper': {'champ': {'positive': 26, 'negative': 12}, 'bag': {'positive': 8, 'negative': 6}, 'odor': {'positive': 11, 'negative': 6}, 'genie': {'positive': 11, 'negative': 4}, 'smell': {'positive': 7, 'negative': 6}}}\n",
      "\n",
      "\n",
      "{'router': {'bit': {'positive': 8, 'negative': 8}, 'hitachi': {'positive': 14, 'negative': 10}}}\n",
      "\n",
      "\n",
      "{'ipod': {'apple': {'positive': 15, 'negative': 18}, 'gb': {'positive': 20, 'negative': 10}}}\n",
      "\n",
      "\n",
      "{'router': {'network': {'positive': 12, 'negative': 6}, 'setup': {'positive': 13, 'negative': 4}, 'cd': {'positive': 10, 'negative': 11}, 'wireless': {'positive': 12, 'negative': 5}}}\n",
      "\n",
      "\n",
      "Only item name extracted: player - No other features\n",
      "None\n",
      "\n",
      "\n",
      "{'phone': {'camera': {'positive': 15, 'negative': 6}, 'call': {'positive': 3, 'negative': 0}, 'computer': {'positive': 8, 'negative': 3}}}\n",
      "\n",
      "\n",
      "{'product': {'program': {'positive': 10, 'negative': 11}, 'software': {'positive': 5, 'negative': 7}}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    feature_dict = opinion_miner_controller(file)\n",
    "    print(feature_dict)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324e491-2c88-4f31-be58-7cb9b9c90983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
