{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdffcf5-7db6-4399-8121-54e5985d5fa5",
   "metadata": {},
   "source": [
    "# Task and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore', message='Discarded redundant search for Synset')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import deaccent\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c6f52-7a94-47d6-8b75-25a17df3d803",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "90f903c8-1605-447c-a89e-14604a75cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    # Initialise an empty list to store reviews and their associated tags\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    # Open the specified file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the entire content of the file\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        # Pre-process the reviews for further handling (assuming a function 'pre_process_review' exists)\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        # Loop over each review in the processed list\n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                # Split the first part by commas to get tags and strip spaces\n",
    "                tags = parts[0].strip().split(',')\n",
    "                # Take the second part as the review content and strip spaces\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        # Convert the list of tagged reviews into a pandas DataFrame\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        # Return the DataFrame containing the tagged reviews\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    doc = nlp(text)  # Process text with NLP model\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = token.text + \"_\" + token.head.text\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue  # Avoids duplicating compound nouns\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def chuncking_post_process(text):\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if '_' in words[i]:  # Handles compound phrases with underscores\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Skips to next after processing a compound phrase\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "            \n",
    "    return ' '.join(processed_words)  # Returns the processed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings in the 'Review' column, if necessary\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Apply compound phrase preservation to the 'Tokenised_Review' column\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Create a 'Soft_Filtered_Review' column with tokens filtered by conditions\n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings in the 'Soft_Filtered_Review_String' column\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Apply chunking post-process to the soft filtered review strings\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chuncking_post_process)\n",
    "    \n",
    "    # Create a 'Filtered_Review' column applying a stricter filtering with stop words check\n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    \n",
    "    # Convert lists of filtered tokens back to strings in the 'Filtered_Review_String' column\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatize the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    \n",
    "    # Tokenize the lemmatized and filtered review strings\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    # Apply stemming to the tokenized words\n",
    "    df['Stemmed_Review'] = df['Lemmatised_Tokenised_Filtered_Review'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    \n",
    "    # Convert lists of stemmed tokens back to strings in the 'Stemmed_Review_String' column\n",
    "    df['Stemmed_Review_String'] = df['Stemmed_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeceaeb-d6eb-4ccf-a4b9-6a69c0f8432e",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "* POS_Noun_Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:m\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "\n",
    "    return common_features\n",
    "\n",
    "\n",
    "\n",
    "def Noun_Phrase_Chuncking(string_list):\n",
    "    \n",
    "    all_noun_phrases = []\n",
    "    \n",
    "    for review in string_list:\n",
    "        doc = nlp(review)\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "        all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "    # Count the occurrences of each noun phrase\n",
    "\n",
    "    phrase_counts = Counter(all_noun_phrases).most_common(15)\n",
    "    \n",
    "    # Display most common noun phrases\n",
    "    common_phrases = phrase_counts\n",
    "    # display(common_phrases)\n",
    "    \n",
    "    return common_phrases\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Concrete noun filter 1 ---------------------\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    \"\"\"Enhanced check if a noun is concrete based on its hypernyms and usage context.\"\"\"\n",
    "    concrete_clues = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        for hyper in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_clues.intersection(set(hyper.lemma_names())):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def context_based_filter(nouns):\n",
    "    \"\"\"Filter nouns based on enhanced concrete checks and contextual usage.\"\"\"\n",
    "    filtered_features = []\n",
    "    for noun in nouns:\n",
    "        if is_concrete_noun(noun):\n",
    "            filtered_features.append(noun)\n",
    "    return filtered_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Concrete noun filter 2 ---------------------\n",
    "\n",
    "# Define a list of concrete domains (as WordNet synsets)\n",
    "concrete_domains = [\n",
    "    wn.synset('artifact.n.01'),  # Artifacts, objects made by humans\n",
    "    wn.synset('device.n.01'),    # Devices, tools or instruments\n",
    "    wn.synset('instrumentality.n.03'),  # Instrumentalities, means of achieving an end\n",
    "    # Add more domains as necessary\n",
    "]\n",
    "\n",
    "def is_related_to_concrete_domain(word):\n",
    "    \"\"\"\n",
    "    Check if a word is semantically related to predefined concrete domains.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        for domain in concrete_domains:\n",
    "            if domain in synset.closure(lambda s: s.hypernyms()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_nouns_semantically(nouns):\n",
    "    \"\"\"\n",
    "    Filter a list of nouns, keeping only those related to concrete domains.\n",
    "    \"\"\"\n",
    "    return [noun for noun in nouns if is_related_to_concrete_domain(noun)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "8c0fe2e0-7dad-4c72-acdc-f51a1f2dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    \n",
    "    # Define a threshold for filtering; this is arbitrary and might need adjustment\n",
    "    similarity_threshold = 0.2\n",
    "    \n",
    "    # Extract just the words for similarity comparison\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    # Calculate average similarities and filter\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Get the top N similar words using GloVe and convert to a dictionary for easier access\n",
    "            glove_similar_words = dict(glove_model.most_similar(words[0], topn=10))\n",
    "            # Get the similarity score for the current word from the GloVe model\n",
    "            glove_similarity = glove_similar_words.get(word, 0)  # Default to 0 if word is not found\n",
    "            # Average the similarities\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            # Filter based on the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                # print(word, w2v_similarity, glove_similarity)\n",
    "                filtered_words.append(word)\n",
    "        except KeyError:\n",
    "            # Word not in vocabulary\n",
    "            # print(f\"Word '{word}' not found in one of the model's vocabulary.\")\n",
    "            pass\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def further_similar_filter(noun_list):\n",
    "    \n",
    "    main_word = 'picture'\n",
    "    word = 'pic'\n",
    "    \n",
    "    w2v_similarity = Word2Vec_model.wv.similarity(main_word, word)\n",
    "    glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "    glove_similarity = glove_similar_words.get(word, 0) \n",
    "    \n",
    "    items_to_remove = []\n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(main_word, topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_list = [item for item in noun_list if item not in items_to_remove]\n",
    "    # print(filtered_list)\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e8ad8a4c-83a1-49ae-9043-10f271c6aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features_dict(product_info):\n",
    "    # Extract the product title\n",
    "    product_title = product_info[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = product_info[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def build_featured_df(df, feature_list_pos_noun):\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_list = feature_list_pos_noun[1:]\n",
    "    \n",
    "    featured_items_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Convert the review Series to a DataFrame with one row\n",
    "            review_df = review.to_frame().transpose()\n",
    "            \n",
    "            # New: Append the found features as a string (or you can keep it as list)\n",
    "            review_df['Featured_Items'] = [', '.join(featured_items)]  # As a single string of items\n",
    "    \n",
    "            review_df['Main_Index'] = idx\n",
    "            \n",
    "            # Append this review to the feature_df\n",
    "            feature_df = pd.concat([review_df, feature_df], axis=0)\n",
    "            \n",
    "            # Additionally, append the found features to the featured_items_list\n",
    "            featured_items_list.append(featured_items)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def feature_dict_count(feature_df, product_dict):\n",
    "    \n",
    "    sentiment_class_list = feature_df[['Featured_Items', 'Sentiment']]\n",
    "    \n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "    \n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        elif sentiment == -1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "        else:\n",
    "            # print('More than two features, cannot classify')\n",
    "            pass\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def df_filter(df):\n",
    "    df = df[['Main_Index', 'Featured_Items', 'Sentiment', 'Tags', 'Review', 'Tokenised_Review', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String', 'Filtered_Review', 'Filtered_Review_String', 'Lemmatised_Review_String', 'Lemmatised_Tokenised_Filtered_Review', 'Stemmed_Review', 'Stemmed_Review_String']]\n",
    "    df = df.set_index('Main_Index', drop=True) \n",
    "    df.index.name = 'Main Index'\n",
    "    df = df.iloc[::-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989f6a-3ba0-4c77-a556-db97096cacdc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "7ee806a9-15c0-4f67-b2b2-ff82b55d67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(feature_df):\n",
    "    sentiment_list = []\n",
    "    \n",
    "    for idx, entry in feature_df.iterrows():\n",
    "        review = entry['Soft_Filtered_Review_String']\n",
    "\n",
    "        features = entry['Featured_Items'].split(',')\n",
    "        features = [feature.strip() for feature in features] \n",
    "        \n",
    "        if len(features) == 1:\n",
    "            \n",
    "            sentiment = sentiment_classifier_1(review)\n",
    "            sentiment_list.append(sentiment)\n",
    "            \n",
    "        else:\n",
    "            sentiment_list.append(0)\n",
    "\n",
    "    feature_df['Sentiment'] = sentiment_list\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_classifier_1(review):\n",
    "    # Average sentiment method 1 and 2 \n",
    "    pos_score_1, neg_score_1 = get_phrase_sentiment_1(review)\n",
    "    pos_score_2, neg_score_2 = get_phrase_sentiment_2(review)\n",
    "    avg_pos_score = (pos_score_1 + pos_score_2) / 2\n",
    "    avg_neg_score = (neg_score_1 + neg_score_2) / 2\n",
    "\n",
    "    if avg_pos_score > avg_neg_score:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = -1 \n",
    "\n",
    "    # Single model\n",
    "    # pos_score, neg_score = get_phrase_sentiment_2(review)\n",
    "\n",
    "    # if pos_score > neg_score:\n",
    "    #     sentiment = 1\n",
    "    # else:\n",
    "    #     sentiment = -1 \n",
    "\n",
    "    return sentiment \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "- This method assumes equal weighting of words in calculating the average sentiment, which might not reflect the actual sentiment conveyed by the phrase.\n",
    "- Handling negations and intensifiers (e.g., \"not\" in \"not good\", \"very\" in \"very good\") requires more sophisticated logic, as they can significantly alter the sentiment.\n",
    "- Advanced models designed for sentiment analysis at the sentence or document level (like BERT-based models) may provide more accurate sentiment assessments for phrases and sentences by considering the broader context.\n",
    "This approach gives a basic approximation but is limited by the nuances of natural language. For more accurate sentiment analysis on phrases or sentences, consider using pre-trained sentiment analysis models or services.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def get_phrase_sentiment_1(phrase):\n",
    "    # Try the phrase directly (useful for compound terms recognized by SWN)\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # Split the phrase into individual words and average their sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This version attempts to address negations directly before a word and could be extended to consider intensifiers (like \"very\") by further\n",
    "modifying the sentiment scores. For even more nuanced sentiment analysis, exploring deep learning models trained specifically for sentiment \n",
    "analysis is recommended.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"Converts Penn Treebank tags to WordNet tags.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_word_sentiment_2(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(swn.senti_synsets(word, wn_tag))\n",
    "        if synsets:\n",
    "            return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    return 0, 0\n",
    "\n",
    "def adjust_scores_for_negation_and_intensifiers(scores, words, i):\n",
    "    \"\"\"Adjusts sentiment scores based on negations and intensifiers around the i-th word.\"\"\"\n",
    "    if i > 0 and words[i-1].lower() in [\"not\", \"no\"]:\n",
    "        return -scores[0], -scores[1]  # Inverting the sentiment\n",
    "    # Further adjustments for intensifiers (like \"very\") can be added here\n",
    "    return scores\n",
    "\n",
    "def get_phrase_sentiment_2(phrase):\n",
    "    words = word_tokenize(phrase)\n",
    "    tagged = pos_tag(words)\n",
    "    total_pos, total_neg = 0, 0\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        scores = get_word_sentiment_2(word, tag)\n",
    "        scores = adjust_scores_for_negation_and_intensifiers(scores, words, i)\n",
    "        total_pos += scores[0]\n",
    "        total_neg += scores[1]\n",
    "    \n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "09a16dec-cd09-4680-8d8f-e2c62753d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_miner_controller(file):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    nouns = POS_Noun_Tagging(df['Lemmatised_Review_String'])\n",
    "    similar_nouns = similarity_filter(nouns)\n",
    "    features = further_similar_filter(similar_nouns)\n",
    "\n",
    "    if len(features) == 1:\n",
    "        return print(f'Only item name extracted: {features[0]} - No other features')\n",
    "    else:\n",
    "        feature_df = build_featured_df(df, features)\n",
    "        feature_df = sentiment_controller(feature_df)\n",
    "        feature_df = df_filter(feature_df)\n",
    "        product_dict = create_product_features_dict(features)\n",
    "        feature_dict = feature_dict_count(feature_df, copy.deepcopy(product_dict))\n",
    "        return feature_dict, feature_df\n",
    "\n",
    "\n",
    "# for file in files:\n",
    "#     feature_dict = opinion_miner_controller(file)\n",
    "#     print(feature_dict)\n",
    "#     print('\\n')\n",
    "\n",
    "feature_dict, feature_df = opinion_miner_controller(files[1])\n",
    "display(feature_dict)\n",
    "# display(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5deb3-05de-4250-8482-4eeabea74f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5076e0-3cd3-4b30-96f2-bed28e28ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22c114d5-dee0-4319-9522-21da74ef41c1",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d6a9e0f8-b74a-442f-83d6-fe3ca9c87dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = feature_df[feature_df.index == 214]\n",
    "\n",
    "# sample_features = sample['Featured_Items'].iloc[0].split(',')\n",
    "# sample_review = sample['Review'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "cb078941-c4c6-4881-9edf-0d0d2199e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize VADER sentiment analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def analyze_feature_sentiment(sentence, features):\n",
    "#     # Dictionary to store sentiment results for each feature\n",
    "#     sentiment_results = {}\n",
    "\n",
    "#     features = [feature.strip() for feature in features]  \n",
    "    \n",
    "#     # Analyze sentiment for each feature\n",
    "#     for feature in features:\n",
    "#         # Extract context around the feature if needed (optional improvement)\n",
    "#         start_index = sentence.lower().find(feature.lower())\n",
    "#         if start_index != -1:\n",
    "#             # Extract a sub-sentence for context-based sentiment analysis\n",
    "#             sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "#             # Get sentiment using TextBlob\n",
    "#             tb_sentiment = TextBlob(sub_sentence).sentiment.polarity\n",
    "#             # Get sentiment using VADER\n",
    "#             vader_sentiment = analyzer.polarity_scores(sub_sentence)['compound']\n",
    "\n",
    "#             # Store results\n",
    "#             sentiment_results[feature] = {\n",
    "#                 'TextBlob Sentiment': 'Positive' if tb_sentiment > 0 else 'Negative' if tb_sentiment < 0 else 'Neutral',\n",
    "#                 'VADER Sentiment': 'Positive' if vader_sentiment > 0.05 else 'Negative' if vader_sentiment < -0.05 else 'Neutral'\n",
    "#             }\n",
    "#         else:\n",
    "#             sentiment_results[feature] = {\n",
    "#                 'TextBlob Sentiment': 'Not Found',\n",
    "#                 'VADER Sentiment': 'Not Found'\n",
    "#             }\n",
    "\n",
    "#     return sentiment_results\n",
    "\n",
    "# results = analyze_feature_sentiment(sample_review, sample_features)\n",
    "# # display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d61341-dc5f-4724-bc4d-0a4987e986fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca724af2-3702-4e17-8d4a-838f492d777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94d9a2-517e-4bab-aa42-2f1724827fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "59601c91-4553-4363-95eb-ccf1249e53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_freq_dist(df):\n",
    "#     all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "#     freq_dist = FreqDist(all_words)\n",
    "#     top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "#     words, frequencies = zip(*top_items)\n",
    "#     plt.figure(figsize=(6, 3))  \n",
    "#     plt.bar(words, frequencies, color='skyblue')  \n",
    "#     plt.xlabel('Words') \n",
    "#     plt.ylabel('Frequency') \n",
    "#     plt.title('Top Words Frequency Distribution')  \n",
    "#     plt.xticks(rotation=45) \n",
    "#     plt.show()\n",
    "\n",
    "# # display_freq_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5708071f-356a-4d42-b0f0-cb283608abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def k_means(string_list):\n",
    "\n",
    "#     num_clusters = 3\n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "#     km = KMeans(n_clusters=num_clusters, n_init=10)\n",
    "#     km.fit(tfidf_matrix)\n",
    "#     clusters = km.labels_.tolist()\n",
    "    \n",
    "#     order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "#     terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "#     for i in range(num_clusters):\n",
    "#         top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "#         print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "#     pca = PCA(n_components=2)\n",
    "#     reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "#     # Get the cluster labels for each data point\n",
    "#     cluster_labels = km.labels_\n",
    "    \n",
    "#     plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "#     # Scatter plot of the reduced data, colored by cluster labels\n",
    "#     plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "#     # Adding labels for axes\n",
    "#     plt.xlabel('PCA 1')\n",
    "#     plt.ylabel('PCA 2')\n",
    "    \n",
    "#     # Title of the plot\n",
    "#     plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "#     # Display the plot\n",
    "#     print('\\n')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# df = read_file(files[1])\n",
    "# df = pre_processing_controller(df)\n",
    "# # k_means(df['Filtered_Review_String'])\n",
    "# # k_means(df['Lemmatised_Review_String'])\n",
    "# # k_means(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c1962340-a817-4beb-a57e-ed1665c3da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "#     # Create a dictionary representation of the documents\n",
    "#     dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "#     # Convert dictionary to a bag of words corpus\n",
    "#     corpus = [dictionary.doc2bow(text) for text in tokenised_reviews]\n",
    "    \n",
    "#     # Number of topics\n",
    "#     num_topics = 5\n",
    "    \n",
    "#     # Generate LDA model\n",
    "#     lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "#     # Print the topics\n",
    "#     topics = lda_model.print_topics(num_words=5)\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "\n",
    "# # LDA_Model(df['Stemmed_Review'])\n",
    "# # print('\\n')\n",
    "# # LDA_Model(df['Filtered_Review'])\n",
    "# # print('\\n')\n",
    "# # LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "85ebafc8-576a-4a39-b57b-9bcc3f7353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LDA_Model_2(string_reviews):\n",
    "    \n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "#     num_topics = 5\n",
    "#     lda = LDA(n_components=num_topics)\n",
    "#     lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "#     # Explore the topics\n",
    "#     terms = tfidf_vectorizer.get_feature_names_out()\n",
    "#     for topic_idx, topic in enumerate(lda.components_):\n",
    "#         print(f\"Topic #{topic_idx+1}:\")\n",
    "#         print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "#         print('\\n')\n",
    "\n",
    "# # LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "cce48f0e-fdf7-4791-9a29-dda9356e2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def disambiguate_word_sense(sentence, word):\n",
    "#     # Use Lesk algorithm for WSD\n",
    "#     sense = lesk(nltk.word_tokenize(sentence), word)\n",
    "#     if not sense:\n",
    "#         return None\n",
    "    \n",
    "#     # Get sentiment scores\n",
    "#     senti_synset = swn.senti_synset(sense.name())\n",
    "#     return {\n",
    "#         'word': word,\n",
    "#         'synset_name': sense.name(),\n",
    "#         'definition': sense.definition(),\n",
    "#         'examples': sense.examples(),\n",
    "#         'positivity_score': senti_synset.pos_score(),\n",
    "#         'negativity_score': senti_synset.neg_score(),\n",
    "#         'objectivity_score': senti_synset.obj_score()\n",
    "#     }\n",
    "\n",
    "\n",
    "# # review = df.iloc[200]\n",
    "# # for word in review['Filtered_Review']:\n",
    "# #     disambiguated_sense = disambiguate_word_sense(review['Review'], word)\n",
    "#     # print(disambiguated_sense)\n",
    "#     # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6dd1ad5f-dca4-44ec-9a8b-723aa88e3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_and_ner(tokens):\n",
    "    \n",
    "#     tagged = pos_tag(tokens)\n",
    "#     named_entities = ne_chunk(tagged)\n",
    "#     return named_entities\n",
    "\n",
    "# # tokenised_text = df['Filtered_Review'].iloc[106]\n",
    "# # named_entities = preprocess_and_ner(tokenised_text)\n",
    "# # print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fd50c8c5-1174-4ce7-8811-144d3db421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf(reviews):\n",
    "#     # Initialize the TF-IDF Vectorizer\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "#     # Transform the reviews into a TF-IDF matrix\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    \n",
    "#     # Extract the feature names/terms from the TF-IDF Vectorizer\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "#     # Calculate the average TF-IDF score for each term across all documents\n",
    "#     scores = tfidf_matrix.mean(axis=0)\n",
    "#     term_scores = {feature_names[col]: scores[0, col] for col in range(scores.shape[1])}\n",
    "    \n",
    "#     # Sort the terms by their average TF-IDF score in descending order\n",
    "#     sorted_term_scores = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # Optionally: Display the top 10 terms with the highest average TF-IDF scores\n",
    "#     print(\"Top 15 terms by average TF-IDF score:\")\n",
    "#     for term, score in sorted_term_scores[:15]:\n",
    "#         print(f\"Term: {term}, Score: {round(score, 4)}\")\n",
    "    \n",
    "#     # Calculate cosine similarity among the documents using the TF-IDF matrix\n",
    "#     cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "#     # return tfidf_matrix, feature_names, cos_sim_matrix\n",
    "\n",
    "# # tf_idf(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "dd663a77-0eb2-48eb-bc2e-0ec273f85e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# # Define custom patterns\n",
    "# patterns = [\n",
    "#     [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}],  # Adjective followed by one or more nouns\n",
    "#     [{\"POS\": \"NOUN\", \"OP\": \"+\"}, {\"LOWER\": \"mode\"}]  # One or more nouns followed by \"mode\"\n",
    "# ]\n",
    "# matcher.add(\"CUSTOM_PATTERNS\", patterns)\n",
    "\n",
    "\n",
    "# def POS_Chuck_Parser_Matcher(review):\n",
    "#     doc = nlp(review)\n",
    "#     matches = matcher(doc)\n",
    "\n",
    "#     extracted_phrases = []\n",
    "#     for match_id, start, end in matches:\n",
    "#         span = doc[start:end]\n",
    "#         extracted_phrases.append(span.text)\n",
    "\n",
    "#     print(extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d5be75d1-de50-40ea-bcb6-54e5ffddbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def POS_Chuck_Parser(review):\n",
    "\n",
    "#     doc = nlp(review)\n",
    "    \n",
    "#     # Initialize a list to hold our extracted phrases\n",
    "#     extracted_phrases = []\n",
    "    \n",
    "#     # Iterate over tokens in the doc\n",
    "#     for token in doc:\n",
    "#         # Look for an adverb modifying an adjective and check the adjective doesn't have a noun child\n",
    "#         if token.pos_ == \"ADV\" and token.head.pos_ == \"ADJ\":\n",
    "#             is_adj_modified = False\n",
    "#             for child in token.head.children:\n",
    "#                 if child.dep_ in [\"attr\", \"dobj\", \"pobj\"]:  # The adjective is modifying a noun\n",
    "#                     is_adj_modified = True\n",
    "#                     break\n",
    "#             if not is_adj_modified:\n",
    "#                 # Capture the adverb-adjective pair \"rather heavy\"\n",
    "#                 extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "    \n",
    "#         # Look for an adjective modifying a noun and check if it's in a prepositional phrase\n",
    "#         if token.pos_ == \"ADJ\" and token.head.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "#             is_in_prep_phrase = False\n",
    "#             for ancestor in token.head.ancestors:\n",
    "#                 if ancestor.dep_ == \"prep\":\n",
    "#                     is_in_prep_phrase = True\n",
    "#                     breakg\n",
    "#             if not is_in_prep_phrase:\n",
    "#                 # Capture the adjective-noun pair \"great camera\"\n",
    "#                 extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "\n",
    "#     print(extracted_phrases)\n",
    "\n",
    "# # index = 24\n",
    "\n",
    "# # print(df['Tags'].iloc[index])\n",
    "# # print(df['Review'].iloc[index])\n",
    "\n",
    "# # POS_Chuck_Parser(df['Review'].iloc[index])\n",
    "# # POS_Chuck_Parser(df['Filtered_Review_String'].iloc[index])\n",
    "# # POS_Chuck_Parser(df['Lemmatised_Review_String'].iloc[index])\n",
    "\n",
    "# # POS_Chuck_Parser_Matcher(df['Review'].iloc[index])\n",
    "# # POS_Chuck_Parser_Matcher(df['Filtered_Review_String'].iloc[index])\n",
    "# # POS_Chuck_Parser_Matcher(df['Lemmatised_Review_String'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9f25a-1ee5-4cee-86e9-e08b1620ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
