{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message='Discarded redundant search for Synset')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import deaccent\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_review(reviews):\n",
    "\n",
    "    processed_reviews = []\n",
    "    title_switch = False\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):\n",
    "            title = review[3:]\n",
    "            title_switch = True\n",
    "        elif title_switch:\n",
    "            appended_review = review + title \n",
    "            processed_reviews.append(appended_review)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = token.text + \"_\" + token.head.text\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "    \n",
    "    return processed_tokens\n",
    "    \n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        if reviews[0] == '*' * 77:\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "        \n",
    "\n",
    "df = read_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) \n",
    "df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "df['Stemmed_Review'] = df['Lemmatised_Tokenised_Filtered_Review'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "df['Stemmed_Review_String'] = df['Stemmed_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b091620-4c70-40c6-a91d-ae7740aa4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "def display_freq_dist(freq_dist):\n",
    "    top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words, frequencies = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))  \n",
    "    plt.bar(words, frequencies, color='skyblue')  \n",
    "    plt.xlabel('Words') \n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Top Words Frequency Distribution')  \n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "# display_freq_dist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcda76-34ae-4551-9a1a-d339bbdb8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(string_list):\n",
    "\n",
    "    num_clusters = 3\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "    km = KMeans(n_clusters=num_clusters, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "        print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get the cluster labels for each data point\n",
    "    cluster_labels = km.labels_\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "    # Scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    # Adding labels for axes\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "    # Title of the plot\n",
    "    plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "    # Display the plot\n",
    "    print('\\n')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# k_means(df['Filtered_Review_String'])\n",
    "k_means(df['Lemmatised_Review_String'])\n",
    "# k_means(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    print(common_features)\n",
    "\n",
    "\n",
    "\n",
    "# POS_Noun_Tagging(df['Filtered_Review_String'])\n",
    "# print('\\n')\n",
    "POS_Noun_Tagging(df['Lemmatised_Review_String'])\n",
    "# print('\\n')\n",
    "# POS_Noun_Tagging(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da73d6b-eae7-40f1-a3fe-c593c1b2c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Noun_Phrase_Chuncking(string_list):\n",
    "\n",
    "    all_noun_phrases = []\n",
    "    \n",
    "    for review in string_list:\n",
    "        doc = nlp(review)\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "        all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "    # Count the occurrences of each noun phrase\n",
    "    from collections import Counter\n",
    "    phrase_counts = Counter(all_noun_phrases).most_common(50)\n",
    "    \n",
    "    # Display most common noun phrases\n",
    "    common_phrases = phrase_counts\n",
    "    print(common_phrases)\n",
    "    \n",
    "    return common_phrases\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Noun_Phrase_Chuncking(df['Filtered_Review_String'])\n",
    "# print('\\n')\n",
    "noun_chucked_phrases = Noun_Phrase_Chuncking(df['Lemmatised_Review_String'])\n",
    "# print('\\n')\n",
    "# Noun_Phrase_Chuncking(df['Stemmed_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12547c-b4c8-4ca5-a6d2-9df6b2ceee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_concrete_noun(word):\n",
    "    \"\"\"Enhanced check if a noun is concrete based on its hypernyms and usage context.\"\"\"\n",
    "    concrete_clues = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        for hyper in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_clues.intersection(set(hyper.lemma_names())):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def context_based_filter(noun_tuples):\n",
    "    \"\"\"Filter nouns based on enhanced concrete checks and contextual usage.\"\"\"\n",
    "    filtered_features = []\n",
    "    for noun, count in noun_tuples:\n",
    "        if is_concrete_noun(noun):\n",
    "            filtered_features.append((noun, count))\n",
    "    return filtered_features\n",
    "\n",
    "\n",
    "\n",
    "potential_nouns = context_based_filter(noun_chucked_phrases)\n",
    "potential_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fe2e0-7dad-4c72-acdc-f51a1f2dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_words = Word2Vec_model.wv.most_similar('dvds')\n",
    "glove_words = glove_model.wv.most_similar('dvds')\n",
    "\n",
    "print(Word2Vec_words)\n",
    "print('\\n')\n",
    "print(glove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119f45e-9989-41fb-abc3-c27d256e7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def semantic_similarity_check(noun, model, seed_features):\n",
    "    noun_vector = average_feature_vector([noun], model, model.vector_size)\n",
    "    \n",
    "    # Check if noun_vector is all zeros which indicates noun was not in the model's vocabulary\n",
    "    if np.all(noun_vector == 0):\n",
    "        return False\n",
    "    \n",
    "    max_similarity = 0\n",
    "    for feature in seed_features:\n",
    "        feature_vector = average_feature_vector([feature], model, model.vector_size)\n",
    "        \n",
    "        # Check if feature_vector is all zeros\n",
    "        if np.all(feature_vector == 0):\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity, safely handling division by zero\n",
    "        denominator = (np.linalg.norm(noun_vector) * np.linalg.norm(feature_vector))\n",
    "        if denominator == 0:\n",
    "            continue  # Avoid division by zero\n",
    "        \n",
    "        similarity = np.dot(noun_vector, feature_vector) / denominator\n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "    \n",
    "    return max_similarity > 0.1  # Adjust threshold as necessary\n",
    "\n",
    "\n",
    "# Example: dynamically selecting seed features that are present in the model\n",
    "model_vocab = set(model.wv.index_to_key)\n",
    "seed_features = [feature for feature in ['player', 'dvd', 'button'] if feature in model_vocab]\n",
    "\n",
    "# Now proceed with your filtered nouns calculation\n",
    "filtered_nouns = [noun for noun in potential_nouns if semantic_similarity_check(noun, Word2Vec_model, seed_features)]\n",
    "\n",
    "filtered_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c365d-9bd2-489e-b263-2a3e41bd1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "    # Convert dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenised_reviews]\n",
    "    \n",
    "    # Number of topics\n",
    "    num_topics = 5\n",
    "    \n",
    "    # Generate LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "    # Print the topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "\n",
    "# LDA_Model(df['Stemmed_Review'])\n",
    "# print('\\n')\n",
    "# LDA_Model(df['Filtered_Review'])\n",
    "# print('\\n')\n",
    "# LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7507d87-5db0-4c2c-bafc-a2aefb58a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model_2(string_reviews):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "    num_topics = 5\n",
    "    lda = LDA(n_components=num_topics)\n",
    "    lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Explore the topics\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "        print('\\n')\n",
    "\n",
    "# LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "30a1b452-8838-4abb-9c1f-8a291f68b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'not very well known', Pos score: 0.125, Neg score: 0.15625\n",
      "Phrase: 'not very well known', Pos score: 0.03125, Neg score: 0.09375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Getting the sentiment of multiword phrases using SentiWordNet (SWN) through the NLTK library requires a more nuanced approach compared to single words because SentiWordNet does not directly provide sentiment scores for phrases. Instead, you can average the sentiment scores of individual words in the phrase, adjust the methodology to consider the phrase's context, or use compound term lookup techniques where applicable. Hereâ€™s an approach to approximate sentiment for multiword phrases:\n",
    "\n",
    "### Step 1: Handle Compound Terms\n",
    "\n",
    "Some compound terms might be recognized by SentiWordNet if connected by underscores (e.g., \"well_known\"). Before splitting the phrase into individual words, check if the compound term has an entry in SentiWordNet.\n",
    "\n",
    "### Step 2: Average Sentiment Scores of Individual Words\n",
    "\n",
    "For phrases not recognized as compound terms, calculate the average sentiment scores of the individual words. Consideration of part-of-speech tags can enhance accuracy, but for simplicity, this example ignores POS tags.\n",
    "\n",
    "### Note:\n",
    "\n",
    "- This method assumes equal weighting of words in calculating the average sentiment, which might not reflect the actual sentiment conveyed by the phrase.\n",
    "- Handling negations and intensifiers (e.g., \"not\" in \"not good\", \"very\" in \"very good\") requires more sophisticated logic, as they can significantly alter the sentiment.\n",
    "- Advanced models designed for sentiment analysis at the sentence or document level (like BERT-based models) may provide more accurate sentiment assessments for phrases and sentences by considering the broader context.\n",
    "\n",
    "This approach gives a basic approximation but is limited by the nuances of natural language. For more accurate sentiment analysis on phrases or sentences, consider using pre-trained sentiment analysis models or services.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def get_phrase_sentiment(phrase):\n",
    "    # Try the phrase directly (useful for compound terms recognized by SWN)\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # Split the phrase into individual words and average their sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "# Example usage\n",
    "phrase = 'not very well known'\n",
    "pos_score, neg_score = get_phrase_sentiment(phrase)\n",
    "print(f\"Phrase: '{phrase}', Pos score: {pos_score}, Neg score: {neg_score}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This version attempts to address negations directly before a word and could be extended to consider intensifiers (like \"very\") by further\n",
    "modifying the sentiment scores. For even more nuanced sentiment analysis, exploring deep learning models trained specifically for sentiment \n",
    "analysis is recommended.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"Converts Penn Treebank tags to WordNet tags.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def get_word_sentiment(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag:\n",
    "        synsets = list(swn.senti_synsets(word, wn_tag))\n",
    "        if synsets:\n",
    "            return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    return 0, 0\n",
    "\n",
    "def adjust_scores_for_negation_and_intensifiers(scores, words, i):\n",
    "    \"\"\"Adjusts sentiment scores based on negations and intensifiers around the i-th word.\"\"\"\n",
    "    if i > 0 and words[i-1].lower() in [\"not\", \"no\"]:\n",
    "        return -scores[0], -scores[1]  # Inverting the sentiment\n",
    "    # Further adjustments for intensifiers (like \"very\") can be added here\n",
    "    return scores\n",
    "\n",
    "def get_phrase_sentiment(phrase):\n",
    "    words = word_tokenize(phrase)\n",
    "    tagged = pos_tag(words)\n",
    "    total_pos, total_neg = 0, 0\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        scores = get_word_sentiment(word, tag)\n",
    "        scores = adjust_scores_for_negation_and_intensifiers(scores, words, i)\n",
    "        total_pos += scores[0]\n",
    "        total_neg += scores[1]\n",
    "    \n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "# Example usage\n",
    "phrase = 'not very well known'\n",
    "pos_score, neg_score = get_phrase_sentiment(phrase)\n",
    "print(f\"Phrase: '{phrase}', Pos score: {pos_score}, Neg score: {neg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52e8c9bc-bdef-4e00-9ee8-82efafa87da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_word_sense(sentence, word):\n",
    "    # Use Lesk algorithm for WSD\n",
    "    sense = lesk(nltk.word_tokenize(sentence), word)\n",
    "    if not sense:\n",
    "        return None\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    senti_synset = swn.senti_synset(sense.name())\n",
    "    return {\n",
    "        'word': word,\n",
    "        'synset_name': sense.name(),\n",
    "        'definition': sense.definition(),\n",
    "        'examples': sense.examples(),\n",
    "        'positivity_score': senti_synset.pos_score(),\n",
    "        'negativity_score': senti_synset.neg_score(),\n",
    "        'objectivity_score': senti_synset.obj_score()\n",
    "    }\n",
    "\n",
    "\n",
    "review = df.iloc[200]\n",
    "\n",
    "for word in review['Filtered_Review']:\n",
    "    disambiguated_sense = disambiguate_word_sense(review['Review'], word)\n",
    "    # print(disambiguated_sense)\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e0fb0d2-93ff-4477-84ce-f0d12a786506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_ner(tokens):\n",
    "    \n",
    "    tagged = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "tokenised_text = df['Filtered_Review'].iloc[106]\n",
    "named_entities = preprocess_and_ner(tokenised_text)\n",
    "# print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d74f075-44ca-41a1-9f91-10dd36ffef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(reviews):\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Transform the reviews into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    # Extract the feature names/terms from the TF-IDF Vectorizer\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate the average TF-IDF score for each term across all documents\n",
    "    scores = tfidf_matrix.mean(axis=0)\n",
    "    term_scores = {feature_names[col]: scores[0, col] for col in range(scores.shape[1])}\n",
    "    \n",
    "    # Sort the terms by their average TF-IDF score in descending order\n",
    "    sorted_term_scores = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Optionally: Display the top 10 terms with the highest average TF-IDF scores\n",
    "    print(\"Top 15 terms by average TF-IDF score:\")\n",
    "    for term, score in sorted_term_scores[:15]:\n",
    "        print(f\"Term: {term}, Score: {round(score, 4)}\")\n",
    "    \n",
    "    # Calculate cosine similarity among the documents using the TF-IDF matrix\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # return tfidf_matrix, feature_names, cos_sim_matrix\n",
    "\n",
    "# tf_idf(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1c0c8c0e-9282-4752-81f2-80a1f52ad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define custom patterns\n",
    "patterns = [\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}],  # Adjective followed by one or more nouns\n",
    "    [{\"POS\": \"NOUN\", \"OP\": \"+\"}, {\"LOWER\": \"mode\"}]  # One or more nouns followed by \"mode\"\n",
    "]\n",
    "matcher.add(\"CUSTOM_PATTERNS\", patterns)\n",
    "\n",
    "\n",
    "def POS_Chuck_Parser_Matcher(review):\n",
    "    doc = nlp(review)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    extracted_phrases = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        extracted_phrases.append(span.text)\n",
    "\n",
    "    print(extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a873b5ed-1ecc-4178-9428-6ed206e7ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Chuck_Parser(review):\n",
    "\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    # Initialize a list to hold our extracted phrases\n",
    "    extracted_phrases = []\n",
    "    \n",
    "    # Iterate over tokens in the doc\n",
    "    for token in doc:\n",
    "        # Look for an adverb modifying an adjective and check the adjective doesn't have a noun child\n",
    "        if token.pos_ == \"ADV\" and token.head.pos_ == \"ADJ\":\n",
    "            is_adj_modified = False\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in [\"attr\", \"dobj\", \"pobj\"]:  # The adjective is modifying a noun\n",
    "                    is_adj_modified = True\n",
    "                    break\n",
    "            if not is_adj_modified:\n",
    "                # Capture the adverb-adjective pair \"rather heavy\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "    \n",
    "        # Look for an adjective modifying a noun and check if it's in a prepositional phrase\n",
    "        if token.pos_ == \"ADJ\" and token.head.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            is_in_prep_phrase = False\n",
    "            for ancestor in token.head.ancestors:\n",
    "                if ancestor.dep_ == \"prep\":\n",
    "                    is_in_prep_phrase = True\n",
    "                    break\n",
    "            if not is_in_prep_phrase:\n",
    "                # Capture the adjective-noun pair \"great camera\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "\n",
    "    print(extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ff8c14d7-ab08-4053-a304-53b87e38e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price[+2]']\n",
      "for the price it is a well spent investment !\n",
      "\n",
      "\n",
      "[]\n",
      "['spent_investment investment']\n",
      "['spent_investment investment']\n",
      "\n",
      "\n",
      "[]\n",
      "['spent_investment investment']\n",
      "['spent_investment investment']\n"
     ]
    }
   ],
   "source": [
    "index = 24\n",
    "\n",
    "print(df['Tags'].iloc[index])\n",
    "print(df['Review'].iloc[index])\n",
    "print('\\n')\n",
    "\n",
    "POS_Chuck_Parser(df['Review'].iloc[index])\n",
    "POS_Chuck_Parser(df['Filtered_Review_String'].iloc[index])\n",
    "POS_Chuck_Parser(df['Lemmatised_Review_String'].iloc[index])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "POS_Chuck_Parser_Matcher(df['Review'].iloc[index])\n",
    "POS_Chuck_Parser_Matcher(df['Filtered_Review_String'].iloc[index])\n",
    "POS_Chuck_Parser_Matcher(df['Lemmatised_Review_String'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0013d09-6b60-4261-ba3c-0da3ed6c6470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
