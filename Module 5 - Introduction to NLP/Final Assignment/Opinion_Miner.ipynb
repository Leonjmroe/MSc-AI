{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad95280-3e1d-4ef7-a504-a1d108bb51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467bfe9b-39e2-4c80-a037-39f0a5bdfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fcb3c68-cf03-4b66-a8fc-303f26569e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_review(reviews):\n",
    "\n",
    "    processed_reviews = []\n",
    "    title_switch = False\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):\n",
    "            title = review[3:]\n",
    "            title_switch = True\n",
    "        elif title_switch:\n",
    "            appended_review = review + title \n",
    "            processed_reviews.append(appended_review)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)\n",
    "\n",
    "    return processed_reviews\n",
    "    \n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        if reviews[0] == '*' * 77:\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "            \n",
    "\n",
    "df = read_file(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a59b1b5e-04b0-4a5a-852d-cfc678027543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Review</th>\n",
       "      <th>Tokenised_Review</th>\n",
       "      <th>Filtered_Review</th>\n",
       "      <th>Filtered_Review_String</th>\n",
       "      <th>Lemmatised_Review_String</th>\n",
       "      <th>Lemmatised_Tokenised_Filtered_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[canon powershot g3[+3]]</td>\n",
       "      <td>i recently purchased the canon powershot g3 an...</td>\n",
       "      <td>[i, recently, purchased, the, canon, powershot...</td>\n",
       "      <td>[recently, purchased, canon, powershot, extrem...</td>\n",
       "      <td>recently purchased canon powershot extremely s...</td>\n",
       "      <td>recently purchase canon powershot extremely sa...</td>\n",
       "      <td>[recently, purchased, canon, powershot, extrem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[use[+2]]</td>\n",
       "      <td>the camera is very easy to use , in fact on a ...</td>\n",
       "      <td>[the, camera, is, very, easy, to, use, ,, in, ...</td>\n",
       "      <td>[camera, easy, use, fact, recent, trip, past, ...</td>\n",
       "      <td>camera easy use fact recent trip past week ask...</td>\n",
       "      <td>camera easy use fact recent trip past week ask...</td>\n",
       "      <td>[camera, easy, use, fact, recent, trip, past, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>after i took their picture with their camera ,...</td>\n",
       "      <td>[after, i, took, their, picture, with, their, ...</td>\n",
       "      <td>[took, picture, camera, offered, take, picture...</td>\n",
       "      <td>took picture camera offered take picture us</td>\n",
       "      <td>take picture camera offer take picture we</td>\n",
       "      <td>[took, picture, camera, offered, take, picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>i just told them , press halfway , wait for th...</td>\n",
       "      <td>[i, just, told, them, ,, press, halfway, ,, wa...</td>\n",
       "      <td>[told, press, halfway, wait, box, turn, green,...</td>\n",
       "      <td>told press halfway wait box turn green press r...</td>\n",
       "      <td>tell press halfway wait box turn green press r...</td>\n",
       "      <td>[told, press, halfway, wait, box, turn, green,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[picture[+2]]</td>\n",
       "      <td>they fired away and the picture turned out qui...</td>\n",
       "      <td>[they, fired, away, and, the, picture, turned,...</td>\n",
       "      <td>[fired, away, picture, turned, quite, nicely, ...</td>\n",
       "      <td>fired away picture turned quite nicely picture...</td>\n",
       "      <td>fire away picture turn quite nicely picture th...</td>\n",
       "      <td>[fired, away, picture, turned, quite, nicely, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>[camera[+3]]</td>\n",
       "      <td>even with these shortcomings , i still think i...</td>\n",
       "      <td>[even, with, these, shortcomings, ,, i, still,...</td>\n",
       "      <td>[even, shortcomings, still, think, best, digit...</td>\n",
       "      <td>even shortcomings still think best digital cam...</td>\n",
       "      <td>even shortcoming still think good digital came...</td>\n",
       "      <td>[even, shortcomings, still, think, best, digit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>[camera[+3]]</td>\n",
       "      <td>definetely a great camera . great camera but g...</td>\n",
       "      <td>[definetely, a, great, camera, ., great, camer...</td>\n",
       "      <td>[definetely, great, camera, great, camera, less]</td>\n",
       "      <td>definetely great camera great camera less</td>\n",
       "      <td>definetely great camera great camera less</td>\n",
       "      <td>[definetely, great, camera, great, camera, less]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>[quality[+2], lens[+2]]</td>\n",
       "      <td>proven canon built quality and lens .</td>\n",
       "      <td>[proven, canon, built, quality, and, lens, .]</td>\n",
       "      <td>[proven, canon, built, quality, lens]</td>\n",
       "      <td>proven canon built quality lens</td>\n",
       "      <td>prove canon build quality len</td>\n",
       "      <td>[proven, canon, built, quality, lens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>[feel[+2]]</td>\n",
       "      <td>feels solid in hand .</td>\n",
       "      <td>[feels, solid, in, hand, .]</td>\n",
       "      <td>[feels, solid, hand]</td>\n",
       "      <td>feels solid hand</td>\n",
       "      <td>feel solid hand</td>\n",
       "      <td>[feels, solid, hand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>[weight[-1], camera[+2]]</td>\n",
       "      <td>rather heavy for point and shoot but a great c...</td>\n",
       "      <td>[rather, heavy, for, point, and, shoot, but, a...</td>\n",
       "      <td>[rather, heavy, point, shoot, great, camera, s...</td>\n",
       "      <td>rather heavy point shoot great camera semi pros</td>\n",
       "      <td>rather heavy point shoot great camera semi pro</td>\n",
       "      <td>[rather, heavy, point, shoot, great, camera, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Tags  \\\n",
       "0    [canon powershot g3[+3]]   \n",
       "1                   [use[+2]]   \n",
       "2                          []   \n",
       "3                          []   \n",
       "4               [picture[+2]]   \n",
       "..                        ...   \n",
       "592              [camera[+3]]   \n",
       "593              [camera[+3]]   \n",
       "594   [quality[+2], lens[+2]]   \n",
       "595                [feel[+2]]   \n",
       "596  [weight[-1], camera[+2]]   \n",
       "\n",
       "                                                Review  \\\n",
       "0    i recently purchased the canon powershot g3 an...   \n",
       "1    the camera is very easy to use , in fact on a ...   \n",
       "2    after i took their picture with their camera ,...   \n",
       "3    i just told them , press halfway , wait for th...   \n",
       "4    they fired away and the picture turned out qui...   \n",
       "..                                                 ...   \n",
       "592  even with these shortcomings , i still think i...   \n",
       "593  definetely a great camera . great camera but g...   \n",
       "594              proven canon built quality and lens .   \n",
       "595                              feels solid in hand .   \n",
       "596  rather heavy for point and shoot but a great c...   \n",
       "\n",
       "                                      Tokenised_Review  \\\n",
       "0    [i, recently, purchased, the, canon, powershot...   \n",
       "1    [the, camera, is, very, easy, to, use, ,, in, ...   \n",
       "2    [after, i, took, their, picture, with, their, ...   \n",
       "3    [i, just, told, them, ,, press, halfway, ,, wa...   \n",
       "4    [they, fired, away, and, the, picture, turned,...   \n",
       "..                                                 ...   \n",
       "592  [even, with, these, shortcomings, ,, i, still,...   \n",
       "593  [definetely, a, great, camera, ., great, camer...   \n",
       "594      [proven, canon, built, quality, and, lens, .]   \n",
       "595                        [feels, solid, in, hand, .]   \n",
       "596  [rather, heavy, for, point, and, shoot, but, a...   \n",
       "\n",
       "                                       Filtered_Review  \\\n",
       "0    [recently, purchased, canon, powershot, extrem...   \n",
       "1    [camera, easy, use, fact, recent, trip, past, ...   \n",
       "2    [took, picture, camera, offered, take, picture...   \n",
       "3    [told, press, halfway, wait, box, turn, green,...   \n",
       "4    [fired, away, picture, turned, quite, nicely, ...   \n",
       "..                                                 ...   \n",
       "592  [even, shortcomings, still, think, best, digit...   \n",
       "593   [definetely, great, camera, great, camera, less]   \n",
       "594              [proven, canon, built, quality, lens]   \n",
       "595                               [feels, solid, hand]   \n",
       "596  [rather, heavy, point, shoot, great, camera, s...   \n",
       "\n",
       "                                Filtered_Review_String  \\\n",
       "0    recently purchased canon powershot extremely s...   \n",
       "1    camera easy use fact recent trip past week ask...   \n",
       "2          took picture camera offered take picture us   \n",
       "3    told press halfway wait box turn green press r...   \n",
       "4    fired away picture turned quite nicely picture...   \n",
       "..                                                 ...   \n",
       "592  even shortcomings still think best digital cam...   \n",
       "593          definetely great camera great camera less   \n",
       "594                    proven canon built quality lens   \n",
       "595                                   feels solid hand   \n",
       "596    rather heavy point shoot great camera semi pros   \n",
       "\n",
       "                              Lemmatised_Review_String  \\\n",
       "0    recently purchase canon powershot extremely sa...   \n",
       "1    camera easy use fact recent trip past week ask...   \n",
       "2            take picture camera offer take picture we   \n",
       "3    tell press halfway wait box turn green press r...   \n",
       "4    fire away picture turn quite nicely picture th...   \n",
       "..                                                 ...   \n",
       "592  even shortcoming still think good digital came...   \n",
       "593          definetely great camera great camera less   \n",
       "594                      prove canon build quality len   \n",
       "595                                    feel solid hand   \n",
       "596     rather heavy point shoot great camera semi pro   \n",
       "\n",
       "                  Lemmatised_Tokenised_Filtered_Review  \n",
       "0    [recently, purchased, canon, powershot, extrem...  \n",
       "1    [camera, easy, use, fact, recent, trip, past, ...  \n",
       "2    [took, picture, camera, offered, take, picture...  \n",
       "3    [told, press, halfway, wait, box, turn, green,...  \n",
       "4    [fired, away, picture, turned, quite, nicely, ...  \n",
       "..                                                 ...  \n",
       "592  [even, shortcomings, still, think, best, digit...  \n",
       "593   [definetely, great, camera, great, camera, less]  \n",
       "594              [proven, canon, built, quality, lens]  \n",
       "595                               [feels, solid, hand]  \n",
       "596  [rather, heavy, point, shoot, great, camera, s...  \n",
       "\n",
       "[597 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x) \n",
    "df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: word_tokenize(review))\n",
    "df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words])\n",
    "df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b091620-4c70-40c6-a91d-ae7740aa4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for review in df['Filtered_Review'] for word in review]\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "def display_freq_dist(freq_dist):\n",
    "    top_items = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    words, frequencies = zip(*top_items)\n",
    "    plt.figure(figsize=(6, 3))  \n",
    "    plt.bar(words, frequencies, color='skyblue')  \n",
    "    plt.xlabel('Words') \n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Top Words Frequency Distribution')  \n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "# display_freq_dist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30dcda76-34ae-4551-9a1a-d339bbdb8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(string_list):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), max_df=0.85, min_df=2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_list)\n",
    "    \n",
    "    km = KMeans(n_clusters=5, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :10]]  # Get top 10 terms for each cluster\n",
    "        print(f\"Cluster {i}: {top_terms}\")\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Get the cluster labels for each data point\n",
    "    cluster_labels = km.labels_\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))  # Set figure size\n",
    "    \n",
    "    # Scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    # Adding labels for axes\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    \n",
    "    # Title of the plot\n",
    "    plt.title('2D Visualization of K-Means Clusters')\n",
    "    \n",
    "    # Display the plot\n",
    "    print('\\n')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# k_means(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "900c0599-a382-4ce9-bbaa-35aa801cd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    \n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Extracts nouns from POS tagged text as nouns likely features names\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    print(common_features)\n",
    "\n",
    "# POS_Noun_Tagging(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1da73d6b-eae7-40f1-a3fe-c593c1b2c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Noun_Phrase_Chuncking(string_list):\n",
    "\n",
    "    all_noun_phrases = []\n",
    "    \n",
    "    for review in string_list:\n",
    "        doc = nlp(review)\n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "        all_noun_phrases.extend(noun_phrases)\n",
    "    \n",
    "    # Count the occurrences of each noun phrase\n",
    "    from collections import Counter\n",
    "    phrase_counts = Counter(all_noun_phrases)\n",
    "    \n",
    "    # Display most common noun phrases\n",
    "    common_phrases = phrase_counts.most_common(15)\n",
    "    print(common_phrases)\n",
    "    \n",
    "# Noun_Phrase_Chuncking(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "216c365d-9bd2-489e-b263-2a3e41bd1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model(tokenised_reviews):\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(tokenised_reviews)\n",
    "    \n",
    "    # Convert dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Number of topics\n",
    "    num_topics = 5\n",
    "    \n",
    "    # Generate LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, passes=10, alpha='auto')\n",
    "    \n",
    "    # Print the topics\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        \n",
    "\n",
    "# LDA_Model(df['Lemmatised_Tokenised_Filtered_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7507d87-5db0-4c2c-bafc-a2aefb58a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Model_2(string_reviews):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_reviews)\n",
    "    \n",
    "    num_topics = 5\n",
    "    lda = LDA(n_components=num_topics)\n",
    "    lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Explore the topics\n",
    "    terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\" \".join([terms[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "        print('\\n')\n",
    "\n",
    "# LDA_Model_2(df['Lemmatised_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "54191043-75c0-4c9b-aa39-dc3f5f56912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Chuck_Parser(review_data):\n",
    "\n",
    "    review = review_data['Review']\n",
    "    tags = review_data['Tags']\n",
    "    doc = nlp(review)\n",
    "    \n",
    "    # Initialize a list to hold our extracted phrases\n",
    "    extracted_phrases = []\n",
    "    \n",
    "    # Iterate over tokens in the doc\n",
    "    for token in doc:\n",
    "        # Look for an adverb modifying an adjective and check the adjective doesn't have a noun child\n",
    "        if token.pos_ == \"ADV\" and token.head.pos_ == \"ADJ\":\n",
    "            is_adj_modified = False\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in [\"attr\", \"dobj\", \"pobj\"]:  # The adjective is modifying a noun\n",
    "                    is_adj_modified = True\n",
    "                    break\n",
    "            if not is_adj_modified:\n",
    "                # Capture the adverb-adjective pair \"rather heavy\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "    \n",
    "        # Look for an adjective modifying a noun and check if it's in a prepositional phrase\n",
    "        if token.pos_ == \"ADJ\" and token.head.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            is_in_prep_phrase = False\n",
    "            for ancestor in token.head.ancestors:\n",
    "                if ancestor.dep_ == \"prep\":\n",
    "                    is_in_prep_phrase = True\n",
    "                    break\n",
    "            if not is_in_prep_phrase:\n",
    "                # Capture the adjective-noun pair \"great camera\"\n",
    "                extracted_phrases.append(token.text + \" \" + token.head.text)\n",
    "\n",
    "    print(tags)\n",
    "    print(review)\n",
    "    print(extracted_phrases)\n",
    "\n",
    "    return review, extracted_phrases\n",
    "\n",
    "\n",
    "# 641/118/117/115/110/107\n",
    "# review, extracted_phrases = POS_Chuck_Parser(df.iloc[129])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "30a1b452-8838-4abb-9c1f-8a291f68b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "# get_sentiment('easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52e8c9bc-bdef-4e00-9ee8-82efafa87da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_word_sense(sentence, word):\n",
    "    # Use Lesk algorithm for WSD\n",
    "    sense = lesk(nltk.word_tokenize(sentence), word)\n",
    "    if not sense:\n",
    "        return None\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    senti_synset = swn.senti_synset(sense.name())\n",
    "    return {\n",
    "        'word': word,\n",
    "        'synset_name': sense.name(),\n",
    "        'definition': sense.definition(),\n",
    "        'examples': sense.examples(),\n",
    "        'positivity_score': senti_synset.pos_score(),\n",
    "        'negativity_score': senti_synset.neg_score(),\n",
    "        'objectivity_score': senti_synset.obj_score()\n",
    "    }\n",
    "\n",
    "\n",
    "review = df.iloc[641]\n",
    "\n",
    "for word in review['Filtered_Review']:\n",
    "    disambiguated_sense = disambiguate_word_sense(review['Review'], word)\n",
    "    # print(disambiguated_sense)\n",
    "    # print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e0fb0d2-93ff-4477-84ce-f0d12a786506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_ner(tokens):\n",
    "    \n",
    "    tagged = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged)\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "tokenised_text = df['Filtered_Review'].iloc[106]\n",
    "named_entities = preprocess_and_ner(tokenised_text)\n",
    "# print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4d74f075-44ca-41a1-9f91-10dd36ffef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(reviews):\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Transform the reviews into a TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    # Extract the feature names/terms from the TF-IDF Vectorizer\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate the average TF-IDF score for each term across all documents\n",
    "    scores = tfidf_matrix.mean(axis=0)\n",
    "    term_scores = {feature_names[col]: scores[0, col] for col in range(scores.shape[1])}\n",
    "    \n",
    "    # Sort the terms by their average TF-IDF score in descending order\n",
    "    sorted_term_scores = sorted(term_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Optionally: Display the top 10 terms with the highest average TF-IDF scores\n",
    "    print(\"Top 15 terms by average TF-IDF score:\")\n",
    "    for term, score in sorted_term_scores[:15]:\n",
    "        print(f\"Term: {term}, Score: {round(score, 4)}\")\n",
    "    \n",
    "    # Calculate cosine similarity among the documents using the TF-IDF matrix\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # return tfidf_matrix, feature_names, cos_sim_matrix\n",
    "\n",
    "# tf_idf(df['Filtered_Review_String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1c0c8c0e-9282-4752-81f2-80a1f52ad8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['camera[+2]']\n",
      "this is a great camera for you !\n",
      "\n",
      "\n",
      "['great camera']\n",
      "['great camera']\n",
      "['great camera']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define custom patterns\n",
    "patterns = [\n",
    "    [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}],  # Adjective followed by one or more nouns\n",
    "    [{\"POS\": \"NOUN\", \"OP\": \"+\"}, {\"LOWER\": \"mode\"}]  # One or more nouns followed by \"mode\"\n",
    "]\n",
    "matcher.add(\"CUSTOM_PATTERNS\", patterns)\n",
    "\n",
    "\n",
    "def POS_Chuck_Parser_Matcher(review):\n",
    "    doc = nlp(review)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    extracted_phrases = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        extracted_phrases.append(span.text)\n",
    "\n",
    "    print(extracted_phrases)\n",
    "\n",
    "\n",
    "index = 109\n",
    "print(df['Tags'].iloc[index])\n",
    "print(df['Review'].iloc[index])\n",
    "print('\\n')\n",
    "POS_Chuck_Parser_Matcher(df['Review'].iloc[index])\n",
    "POS_Chuck_Parser_Matcher(df['Filtered_Review_String'].iloc[index])\n",
    "POS_Chuck_Parser_Matcher(df['Lemmatised_Review_String'].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e75a97-beee-44c4-834f-936321b0fa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
