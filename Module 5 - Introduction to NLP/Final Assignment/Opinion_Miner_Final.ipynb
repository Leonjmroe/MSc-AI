{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61263af9-60d2-4c0d-bbd6-fa7c022e7af0",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "#### Overview of the Data\n",
    "\n",
    "The dataset consists of product reviews from Amazon, segmented into features with annotated sentiment scores. Reviews include multiple aspects such as ease of use, picture quality, and additional functionalities, each tagged with a corresponding sentiment score. These annotations provide a rich basis for sentiment analysis but require precise parsing to effectively utilize the structured format in which they are presented.\n",
    "\n",
    "#### System-Level Outline\n",
    "\n",
    "##### Data Parsing and Pre-processing\n",
    "\n",
    "Using a custom `read_file` function, the system initially parses the structured reviews from text files. This function handles the dataset's specific format, which includes initial metadata and reviewer comments split by a unique delimiter ('##'). This is crucial for separating feature tags from review content, facilitating subsequent analysis.\n",
    "\n",
    "The `pre_process_review` function further refines this by extracting titles and adjusting reviews accordingly, ensuring that the context provided by review headers is not lost. Additionally, it preserves the integrity of the review flow, which is vital for understanding the nuances of each review.\n",
    "\n",
    "##### Enhancing NLP with Custom Processing\n",
    "\n",
    "To deepen the analysis, the `preserve_compound_phrases` function is employed. This function utilizes an NLP model to identify and preserve compound nouns and adjectives directly linked to nouns, which are often critical in understanding the specific features discussed. By preserving these compounds, the system maintains the granular detail necessary for precise feature extraction.\n",
    "\n",
    "Following this, the `chunking_post_process` method reassembles the text from tokenized forms back into a structured format conducive to further analysis, ensuring that compound phrases are treated as single entities within the dataset.\n",
    "\n",
    "##### Comprehensive Review Filtering\n",
    "\n",
    "The `pre_processing_controller` function orchestrates the entire preprocessing pipeline. It transforms raw review texts into a tokenized format, applies compound preservation, and executes two levels of filtering: soft filtering (preserving basic structure and some stopwords) and hard filtering (removing all non-alphabetic characters and stopwords). This dual approach allows for flexibility in analysis, from high-level sentiment trends to detailed feature-specific sentiments.\n",
    "\n",
    "##### Sentiment Analysis and Feature Extraction\n",
    "\n",
    "Once preprocessed, the data is ripe for sentiment analysis. Leveraging the structured format of feature tags and sentiment scores, the system can map sentiments directly to product features, allowing for an aggregated sentiment score for each feature. This quantification is pivotal in determining which features are most appreciated or criticized by users.\n",
    "\n",
    "##### Leveraging Data for Business Insights\n",
    "\n",
    "The final step involves synthesizing the analyzed data into actionable business insights. By understanding which features correlate strongly with positive or negative sentiments, companies can prioritize product improvements or highlight successful aspects in marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97caa3c8-0c02-48d4-abe7-98b05163312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "sentiment_intensity_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2b03ea-3191-44bf-9b2e-2c6d67a03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c016251-79e8-49ff-b566-e35bdbedab0e",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed6882-2c25-48fb-9545-cf4c3668c231",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "### Data Ingestion and Initial Processing\n",
    "\n",
    "The process starts with the ingestion of data files containing multiple reviews, which can vary in structure. This step is crucial for setting the stage for detailed analysis. The function `read_file` is used to open and read the contents of these files. Reviews are often separated by new lines and may begin with a distinctive line of asterisks, which indicates metadata or headers not part of the actual review content. These non-relevant lines are programmatically identified and omitted from processing to ensure only pertinent text is analyzed further.\n",
    "\n",
    "The reviews within these files are then split using '##' as a delimiter. This segmentation is critical as it separates tags that contain embedded metadata or sentiment scores from the main content. Each segment of the review, along with its associated tags, is stored in a structured format within a pandas DataFrame, which facilitates ease of manipulation and detailed analysis in subsequent steps.\n",
    "\n",
    "### Advanced Text Processing Techniques\n",
    "\n",
    "Once the initial data ingestion is complete, the reviews undergo various text processing steps encapsulated within the `pre_process_review` function. This function is adept at managing different nuances of text, such as concatenating titles where necessary, and ensuring the textual integrity of reviews that span multiple lines is maintained.\n",
    "\n",
    "### Preservation of Semantic Structures\n",
    "\n",
    "To maintain the semantic integrity of phrases within the reviews, the `preserve_compound_phrases` function is employed. This function utilizes spaCy, an advanced NLP library, to parse the text and identify compound nouns and adjectival modifiers. These elements are crucial for understanding the context and sentiment related to specific product features. The identified compounds are then reconstructed with underscores replacing spaces, ensuring they are treated as single tokens in subsequent analysis stages. This preservation prevents the loss of semantic unity and is vital for the accurate interpretation of product features.\n",
    "\n",
    "### Enhancement of Tokenization and Filtering\n",
    "\n",
    "Following the preservation of semantic structures, the `pre_processing_controller` function orchestrates several layers of tokenization and filtering. The text is initially tokenized, which separates each word or phrase for individual analysis. This tokenization process feeds into a dual filtering system:\n",
    "\n",
    "1. A 'Soft Filtered Review' captures tokens that either are part of the identified compound phrases or are standalone alphabetic words. This layer ensures that important phrases and words are retained without too much reduction of the text.\n",
    "2. A 'Filtered Review' applies a more stringent filter that includes the removal of common stop words, focusing only on the more meaningful terms relevant to sentiment analysis. This selective filtering is crucial for reducing noise and enhancing the focus on significant textual elements.\n",
    "\n",
    "These tokens are then reassembled into coherent strings, which form the basis for deeper linguistic analysis, such as lemmatization. The 'Soft Filtered Review' is transformed back into a string format while retaining the structural integrity of compound phrases. Lemmatization is performed on the 'Filtered Review' strings to reduce words to their base or dictionary form, which facilitates a more generalized but robust analysis of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c1933d-e77d-4110-afd5-f8aeb435af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present - This is a quirk to parse the data files\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = hanldle_titles(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and set content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def hanldle_titles(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8551b5d4-90ba-48b6-9987-18d38e018b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_compound_phrases(text):\n",
    "    # Process the text with an NLP model\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check for compounds or adjectives linked directly to nouns\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = f\"{token.text}_{token.head.text}\"\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        # Skip nouns that are already part of a compound to prevent duplicates\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        # Include all other tokens normally\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "def chunking_post_process(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Check if the current word is part of a compound phrase\n",
    "        \n",
    "        if '_' in words[i]:\n",
    "            # Append all parts of the compound phrase to the list\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Move to the next word after finishing the compound phrase\n",
    "        # Append non-compound words directly to the list\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    # Return the processed words as a single string\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a474f0fe-11c6-4582-916d-5e36195372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings and applies compound phrase preservation\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation and numbers \n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings and retains the compound phrases - soft means not to filter out stop words\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chunking_post_process)\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation, numbers and stop words \n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatise the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0e768-3ee0-4b03-83ac-c5461ea48ded",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fede2a8-b11f-44ac-aa8e-c527b0d0e377",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "\n",
    "1. **POS Noun Tagging:** Initially, the `POS_Noun_Tagging` function processes textual data from a DataFrame to extract nouns using part-of-speech (POS) tagging. This function tokenizes each text string, tags each token with its grammatical role, and filters out tokens that are identified as nouns (singular, plural, proper, etc.). The most common nouns are then identified and returned.\n",
    "\n",
    "2. **Dependency Parsing for Noun Extraction:** As an alternative to POS tagging, the `dependency_parsing_noun_extraction` function uses dependency parsing to identify nouns based on their syntactic relationships and roles in the sentence. This method might provide a more context-aware extraction of nouns. It similarly aggregates and counts the most frequent nouns found across the dataset.\n",
    "\n",
    "3. **Concrete Noun Filtering:** The `concrete_noun_filter` function refines the list of extracted nouns by checking their semantic categories using WordNet synsets. It retains nouns associated with tangible objects or entities, filtering out more abstract terms. This is particularly useful for focusing the feature set on concrete aspects likely discussed in product reviews.\n",
    "\n",
    "4. **Similarity Filtering:** The `similarity_filter` function further processes the list of nouns by comparing their semantic similarity using models like Word2Vec and GloVe. It averages similarities from both models and retains words that meet a specified similarity threshold with the first noun in the list, aiming to group semantically related terms together.\n",
    "\n",
    "5. **Feature Integration:** Finally, the `add_features_to_df` function integrates the extracted and filtered features back into the original DataFrame. It identifies and records which features appear in each text entry, adding this information as a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f8617b-2cd0-499a-a609-177fd19707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, noun_string, similarity_threshold):\n",
    "    \n",
    "    nouns = POS_Noun_Tagging(df, noun_string)\n",
    "    # nouns = dependency_parsing_noun_extraction(df, noun_string)\n",
    "    # nouns = concrete_noun_filter(nouns)\n",
    "    \n",
    "    features = similarity_filter(nouns, similarity_threshold)\n",
    "    df = add_features_to_df(df, features)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f298369f-55f2-44f6-940f-da089722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(df, noun_string):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df[noun_string].tolist()\n",
    "    features = []\n",
    "    \n",
    "    # Process each review to extract nouns\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)  # Tokenize the text\n",
    "        tagged = pos_tag(tokens)  # POS tagging\n",
    "        # Collect nouns from tags\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    # Count and retrieve the 15 most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "\n",
    "    return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2493ba4c-75f9-4d94-8e18-44e3c367deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing_noun_extraction(df, noun_string):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df[noun_string].tolist()\n",
    "    features = []\n",
    "\n",
    "    # Process each review to extract nouns using dependency parsing\n",
    "    for review in reviews:\n",
    "        doc = nlp(review)\n",
    "        # Extract nouns based on their dependency role and POS tag\n",
    "        features.extend([token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN']])\n",
    "\n",
    "    # Count and retrieve the most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    \n",
    "    return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad75ea7a-889f-4ef0-b72d-d4a70a42c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concrete_noun_filter(noun_tuples):\n",
    "    \n",
    "    concrete_indicators = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    filtered_features = set()  # Using a set to avoid duplicates\n",
    "    \n",
    "    for noun_tuple in noun_tuples:\n",
    "        noun = noun_tuple[0]\n",
    "        synsets = wn.synsets(noun, pos=wn.NOUN)\n",
    "        \n",
    "        found_concrete = False\n",
    "        for synset in synsets:\n",
    "            if found_concrete:\n",
    "                break\n",
    "            hypernyms = synset.hypernyms()\n",
    "            for hypernym in hypernyms:\n",
    "                if concrete_indicators.intersection(hypernym.lemma_names()):\n",
    "                    filtered_features.add(noun_tuple)\n",
    "                    found_concrete = True\n",
    "                    break  \n",
    "    \n",
    "    return list(filtered_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be50234-31a2-45fa-ae2b-d4a15be9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function that takes a list of filtered nouns, presumed to be features of the product at hand. The function returns a filtered noun list.\n",
    "The product is extracted as the first noun in the list, as the list is passed in noun mention relevance order.\n",
    "The nouns are filtered based on their similarity relevance to the eximated product word.\n",
    "An average of the Word2Vec and gloVe model is taken for NLP diversification.\n",
    "Further filtering is performed to reduce the probability that repeat words are removed eg: 'pic' and 'picture' - so the features are diluted,\n",
    "with similar meaning words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def similarity_filter(word_tuple_list, similarity_threshold):\n",
    "    \n",
    "    # Create a list of words from the tuples\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    noun_list = []\n",
    "    # Iterate over words to compute similarities\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Calculate similarity using GloVe\n",
    "            glove_similarity = glove_model.similarity(words[0], word)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            \n",
    "            # Append word to list if it meets the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                noun_list.append(word)\n",
    "        except KeyError:\n",
    "            # Skip the word if it's not found in the model's vocabulary\n",
    "            continue\n",
    "\n",
    "    items_to_remove = []\n",
    "    \n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(noun_list[0], topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_nouns = [item for item in noun_list if item not in items_to_remove]\n",
    "\n",
    "    return filtered_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ee67fd-4927-4677-938a-2b16f9e74b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_df(df, features):\n",
    "    \n",
    "    feature_list = features[1:]\n",
    "    featured_items_list = []\n",
    "    index_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Append the found features as a string \n",
    "            featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            item_list = [f\"{item.strip()}\" for item in featured_items]\n",
    "            featured_items_string = ', '.join(item_list)\n",
    "            featured_items_list.append(featured_items_string) \n",
    "            index_list.append(idx)\n",
    "        else:\n",
    "            featured_items_list.append('')  \n",
    "            index_list.append(idx)\n",
    "\n",
    "    df['Main_Index'] = index_list\n",
    "    df['Featured_Items'] = featured_items_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39335d95-f50e-4abe-afd9-46f6b3b7fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These functions are utilised within the opinion_miner_controller module.\n",
    "The primary function, create_feature_table, takes a DataFrame and a list of features as input.\n",
    "It processes the DataFrame, extracting features and sentiments, then updates a dictionary with the feature counts.\n",
    "Each review's features and sentiments are converted into tags, following a specific format for labeling data.\n",
    "These tags are added to the main DataFrame under the column 'My_Sentiment_Tags' for later evaluation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_feature_table(df, features):\n",
    "    # Process the DataFrame\n",
    "    df = process_data_df(df)\n",
    "    \n",
    "    # Extract the product title and features\n",
    "    product_title = features[0]\n",
    "    product_features = features[1:]\n",
    "    \n",
    "    # Create a dictionary to store feature counts\n",
    "    product_dict = {product_title: {feature: {'positive': 0, 'negative': 0} for feature in product_features}}\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Split the featured items and sentiments\n",
    "        feature_items = row['Featured_Items'].split(',')\n",
    "        sentiments = row['Sentiment'] if isinstance(row['Sentiment'], list) else [row['Sentiment']]\n",
    "        \n",
    "        # Update the feature table with counts\n",
    "        for feature, sentiment in zip(feature_items, sentiments):\n",
    "            update_feature_table(product_dict, product_title, feature.strip(), sentiment)\n",
    "    \n",
    "    return product_dict, df\n",
    "\n",
    "\n",
    "\n",
    "def update_feature_table(product_dict, title, feature, sentiment):\n",
    "    # Update the feature counts based on sentiment\n",
    "    if feature and feature in product_dict[title]:\n",
    "        if sentiment > 0:\n",
    "            product_dict[title][feature]['positive'] += 1\n",
    "        elif sentiment < 0:\n",
    "            product_dict[title][feature]['negative'] += 1\n",
    "\n",
    "\n",
    "\n",
    "def process_data_df(df):\n",
    "    my_tags = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Split features and get sentiments\n",
    "        features = row['Featured_Items'].split(',') if row['Featured_Items'] else []\n",
    "        sentiments = row['Sentiment']\n",
    "        \n",
    "        # Process features and sentiments into tags\n",
    "        tags = process_features_and_sentiments(features, sentiments)\n",
    "        my_tags.append(tags)\n",
    "    \n",
    "    # Add tags to the DataFrame\n",
    "    df['My_Sentiment_Tags'] = my_tags\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_features_and_sentiments(features, sentiments):\n",
    "    result_tags = []  # This will store the final list of sentiment tags for the features\n",
    "\n",
    "    # Ensure sentiments is always a list\n",
    "    if not isinstance(sentiments, list):\n",
    "        sentiments = [sentiments]  # Convert single integer to list\n",
    "\n",
    "    # Loop through each feature and its index\n",
    "    for i, feature in enumerate(features):\n",
    "        # Strip any whitespace around the feature name to ensure clean data\n",
    "        cleaned_feature = feature.strip()\n",
    "\n",
    "        # Continue only if the feature is not an empty string\n",
    "        if cleaned_feature:\n",
    "            # Access the corresponding sentiment value or default to 0 if out of range\n",
    "            sentiment_value = sentiments[i] if i < len(sentiments) else 0\n",
    "\n",
    "            # Format the feature and its sentiment into a string, appending it to the result list\n",
    "            tag_string = f\"{cleaned_feature}[{sentiment_value}]\"\n",
    "            result_tags.append(tag_string)\n",
    "\n",
    "    return result_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3dc59-7dc0-49fe-9dea-7cf815e53749",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153d7a-bb12-4cb4-a3ce-d8cd0780cbfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "The two sentiment analysis classifiers, `vader_textblob_classifier` and `senti_classifier`, employ distinct methodologies to gauge sentiment, each suitable for different types of text.\n",
    "\n",
    "The `vader_textblob_classifier` utilizes a hybrid approach by integrating two popular sentiment analysis tools: VADER (Valence Aware Dictionary and sEntiment Reasoner) and TextBlob. VADER is particularly adept at handling sentiments expressed in social media through its lexicon, which is attuned to informal language, including slang, emojis, and abbreviations. It quantifies sentiment intensity on a normalized, weighted composite score. TextBlob, on the other hand, uses a straightforward rule-based approach, deriving polarity scores based on adjective and adverb modifiers, which makes it useful for more straightforward and grammatically structured texts. In this classifier, sentiment scores are calculated by extracting context-specific snippets around key features in the text, which are then separately analyzed by both TextBlob for polarity and VADER for intensity. The final sentiment determination for each feature considers both scores, enhancing accuracy in environments where context heavily influences meaning.\n",
    "\n",
    "Conversely, the `senti_classifier` relies on SentiWordNet, an extension of the WordNet lexical database, which provides sentiment scores for words based on their semantic and syntactic context. It assesses the sentiment of phrases by first attempting to derive a composite sentiment score for the entire phrase. If detailed scores are unavailable, it breaks down the phrase into individual words and calculates sentiment scores for each, aggregating these to produce an average. This method, while potentially less nuanced in handling slang or implicit sentiment, is effective for analyzing texts with clear, conventional language usage, such as formal communication or reports.\n",
    "\n",
    "Both methods offer strengths depending on the textual content and required sentiment analysis precision. The `vader_textblob_classifier` is particularly robust in environments with nuanced and varied expressions of sentiment, while the `senti_classifier` provides reliable output for texts with explicit sentiment expressions within well-defined semantic structures. Choosing between them hinges on matching the tool to the specific linguistic characteristics and sentiment expression styles of the text corpus in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbb4e4d-5031-45c2-adfc-39b39631bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The sentiment controller function inputs the data dataframe, the classifier and the string type to in which to formulate the sentiment. \n",
    "Each feature that has been labelled will be passed individual for sentiment classification, for each review in the file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sentiment_controller(df, classifier, sent_string):\n",
    "\n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Function to get sentiment\n",
    "    def get_sentiment(review, classifier, features):\n",
    "        if classifier == 'senti':\n",
    "            pos_score, neg_score = senti_classifier(review)\n",
    "            return 1 if pos_score > neg_score else -1\n",
    "        elif classifier == 'vader_blob':\n",
    "            return vader_textblob_classifier(review, features)\n",
    "        return 0  # default case if no classifier matches\n",
    "\n",
    "    # Process each review in the DF\n",
    "    for idx, row in df.iterrows():\n",
    "        review = row[sent_string]\n",
    "        features = [feature.strip() for feature in row['Featured_Items'].split(',')]\n",
    "        \n",
    "        # Apply sentiment analysis for each feature or single feature case\n",
    "        if len(features) == 1:  \n",
    "            sentiment = get_sentiment(review, classifier, features)\n",
    "            sentiment_list.append(sentiment)\n",
    "        else: \n",
    "            sentiments = [get_sentiment(review, classifier, [feature]) for feature in features]\n",
    "            sentiment_list.append(sentiments)\n",
    "            \n",
    "    df['Sentiment'] = sentiment_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b87add5-a897-4fe5-99ac-89c3521c4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_classifier(phrase):\n",
    "    \n",
    "    # First try to get sentiment score for the whole phrase as a compound\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # If no score, break down the phrase and calculate average sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    \n",
    "    # Average the sentiment scores\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    # Retrieve sentiment scores from SentiWordNet\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fce2dfd-33cb-4d23-964a-50c531eaad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_textblob_classifier(sentence, features):\n",
    "    \n",
    "    # Dictionary to store sentiment results for each feature\n",
    "    sentiment_results = {}\n",
    "\n",
    "    features = [feature.strip() for feature in features]  \n",
    "    \n",
    "    # Analyse sentiment for each feature\n",
    "    for feature in features:\n",
    "        # Extract context around the feature if needed (optional improvement)\n",
    "        start_index = sentence.lower().find(feature.lower())\n",
    "        if start_index != -1:\n",
    "            # Extract a sub-sentence for context-based sentiment analysis\n",
    "            sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "            # Get sentiment using TextBlob\n",
    "            tb_sentiment = TextBlob(sub_sentence).sentiment.polarity\n",
    "            # Get sentiment using VADER\n",
    "            vader_sentiment = sentiment_intensity_analyser.polarity_scores(sub_sentence)['compound']\n",
    "  \n",
    "            Textblob = 1 if tb_sentiment > 0 else -1 if tb_sentiment < 0 else 0\n",
    "            Vader = 1 if vader_sentiment > 0.05 else -1 if vader_sentiment < -0.05 else 0\n",
    "            total_sentiment = Textblob + Vader\n",
    "            \n",
    "        else:\n",
    "           total_sentiment = 0\n",
    "            \n",
    "    return total_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829857e7-e868-4c16-ae01-307e0b89ae0a",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------\n",
    "### ------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2eef0-37b9-44f7-b01b-1ac9ebac146c",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "### Overview of the Process\n",
    "The `opinion_miner_controller` function orchestrates the workflow for sentiment analysis by coordinating file reading, preprocessing, feature extraction, sentiment analysis, feature table creation, and evaluation. Each function within this controller is designed to perform a specific task that contributes to the overall sentiment analysis process, ensuring that the system is modular, manageable, and scalable.\n",
    "\n",
    "### Key Functions and Their Roles\n",
    "1. **Evaluation Mapping (`evaluation_mapping`)**:\n",
    "   This function maps the extracted features to a unified schema, consolidating various synonyms or related terms into a single feature identifier. This normalization is critical for reducing redundancy and ensuring consistent sentiment evaluation across different datasets.\n",
    "\n",
    "2. **Creation of the Evaluation Table (`create_evaluation_table`)**:\n",
    "   It filters the DataFrame to include only the necessary columns ('Tags' and 'My Sentiment Tags') and rows that have valid sentiment tags. This selective inclusion helps focus the sentiment analysis on relevant data, enhancing processing efficiency and accuracy.\n",
    "\n",
    "3. **Feature Similarity Detection (`find_similar_features`)**:\n",
    "   Utilizing natural language processing tools like `nlp`, this function compares features based on their semantic similarity. By setting a threshold, it identifies and groups similar features, aiding in the robust mapping of sentiments to features even when different terminologies are used.\n",
    "\n",
    "4. **Tag Extraction and Processing (`extract_tags` and `process_tags`)**:\n",
    "   These functions extract and process sentiment scores from textual data. They apply the key mappings to align the sentiments from different sources, preparing them for comparison and analysis.\n",
    "\n",
    "5. **Confusion Matrix Computation (`compute_confusion_matrix`)**:\n",
    "   This function calculates the confusion matrix from the processed data, providing essential metrics (True Positives, False Positives, etc.) that are foundational for evaluating the performance of the sentiment analysis system.\n",
    "\n",
    "6. **Metrics Calculation (`calculate_metrics`)**:\n",
    "   It computes precision, recall, accuracy, and F1 score from the confusion matrix data. These metrics are critical for assessing the effectiveness of the sentiment analysis and guiding improvements in the system.\n",
    "\n",
    "### Integration and Visualisation\n",
    "After processing, the system integrates these components to produce a comprehensive view of sentiment analysis performance. The `opinion_miner_controller` seamlessly transitions between processing steps, from data ingestion to metrics calculation, ensuring a streamlined workflow.\n",
    "\n",
    "Furthermore, the system includes functions for visualizing disparities between different sentiment analysis methods, like `plot_comparison_metrics`. This function exaggerates differences to make them more discernible, which is particularly useful for presentations or detailed analytical reviews where nuances matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4cc00a-5a34-41b1-b5ce-8c963cc36f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing functions to:\n",
    "    - Filter for the models sentiment tags and the data's sentiment tags\n",
    "    - Extracts the data's sentiment tags as a dictionary \n",
    "    - Creates a mapping dictionary that maps the models features with the datas features, with similarity variability\n",
    "    - Create a dataframe for the models labeled reviews and the datas labeled reveiws for evaluation \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def create_evaluation_table(df):\n",
    "    \n",
    "    valid_rows = []\n",
    "\n",
    "    # Iterate through each row and check if 'Tags' is non-empty\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if the first element in 'Tags' is not an empty string\n",
    "        try:\n",
    "            if row['Tags'][0] != '':\n",
    "                # Append the row (as a DataFrame) to the list\n",
    "                valid_rows.append(row)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    eval_df = pd.DataFrame(valid_rows)\n",
    "    \n",
    "    return eval_df[['Tags', 'My_Sentiment_Tags']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_tags(sentiment_string):\n",
    "    \n",
    "    # Ensure the input is a string for regex operations\n",
    "    if not isinstance(sentiment_string, str):\n",
    "        if isinstance(sentiment_string, list):\n",
    "            # If it's a list, convert to a comma-separated string\n",
    "            sentiment_string = ', '.join(sentiment_string)\n",
    "        else:\n",
    "            # Otherwise, convert to string in any other case\n",
    "            sentiment_string = str(sentiment_string)\n",
    "    \n",
    "    # Use regex to find patterns of words followed by [number] and convert to a dictionary\n",
    "    output = {match[0]: int(match[1]) for match in re.findall(r'(\\w+)\\[([+-]?\\d+)\\]', sentiment_string)}\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluation_mapping(evaluation_dict, key_mapping):\n",
    "\n",
    "    # Create a dictionary with unique keys and initialize sentiment counts\n",
    "    unique_keys = set(key_mapping.values())\n",
    "    data_dict = {key.strip().lower(): {'positive': 0, 'negative': 0} for key in unique_keys if key}\n",
    "\n",
    "    # Populate the dictionary with sentiment values from the evaluation_dict using the key_mapping\n",
    "    for key, sentiments in evaluation_dict.items():\n",
    "        mapped_key = key_mapping.get(key)\n",
    "        if mapped_key:\n",
    "            mapped_key = mapped_key.strip().lower()\n",
    "            if mapped_key in data_dict:\n",
    "                # Increment sentiment counts based on mapped keys\n",
    "                data_dict[mapped_key]['positive'] += sentiments['positive']\n",
    "                data_dict[mapped_key]['negative'] += sentiments['negative']\n",
    "                \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_similar_features(my_features, annotated_features, threshold=0.7):\n",
    "\n",
    "    similar_features = {}\n",
    "    # Convert feature lists to spaCy NLP objects for vector comparison\n",
    "    annotated_features_nlp = {feature: nlp(feature) for feature in annotated_features}\n",
    "    my_features_nlp = {feature: nlp(feature) for feature in my_features}\n",
    "\n",
    "    # Compare each feature's NLP object for similarity above a set threshold\n",
    "    for my_feat, my_feat_nlp in my_features_nlp.items():\n",
    "        for anno_feat, anno_feat_nlp in annotated_features_nlp.items():\n",
    "            # Check if both terms have vectors and compare their similarity\n",
    "            if my_feat_nlp.has_vector and anno_feat_nlp.has_vector:\n",
    "                if my_feat_nlp.similarity(anno_feat_nlp) > threshold:\n",
    "                    similar_features[my_feat] = anno_feat\n",
    "    \n",
    "    return similar_features\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def process_tags(df, key_mapping):\n",
    "\n",
    "    results = []\n",
    "    reverse_mapping = {v: k for k, v in key_mapping.items()}\n",
    "\n",
    "    # Iterate through DataFrame to apply mappings and extract sentiments\n",
    "    for idx, row in df.iterrows():\n",
    "        tags = extract_tags(row['Tags'])\n",
    "        my_sentiment_tags = extract_tags(row['My_Sentiment_Tags'])\n",
    "\n",
    "        # Compare and store results of tag sentiments\n",
    "        for tag, sentiment in tags.items():\n",
    "            mapped_tag = reverse_mapping.get(tag, tag)  # Map tags to a common key, if possible\n",
    "            if mapped_tag in my_sentiment_tags:\n",
    "                # Store results if there's a matching sentiment tag\n",
    "                results.append({\n",
    "                    'Index': idx,\n",
    "                    'Feature': mapped_tag,\n",
    "                    'Tags Sentiment': sentiment,\n",
    "                    'My Sentiment Tags Sentiment': my_sentiment_tags[mapped_tag]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "730619e9-95a3-4393-981c-46e15eb2fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing functions to create the confusion matrix, calculate the metrics from the matrix and output a matrix comparison chart\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(result_df):\n",
    "\n",
    "    # Initialise counters for True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    # Iterate through each row in the DataFrame to compare sentiment tags\n",
    "    for index, row in result_df.iterrows():\n",
    "        \n",
    "        # Extract sentiment from 'Tags Sentiment' and 'My Sentiment Tags Sentiment'\n",
    "        tag_sent = row['Tags Sentiment']\n",
    "        my_sent = row['My Sentiment Tags Sentiment']\n",
    "\n",
    "        # Compare extracted sentiments and classify as TP, FP, TN, or FN\n",
    "        if tag_sent > 0 and my_sent > 0:\n",
    "            TP += 1\n",
    "        elif tag_sent <= 0 and my_sent > 0:\n",
    "            FP += 1\n",
    "        elif tag_sent <= 0 and my_sent <= 0:\n",
    "            TN += 1\n",
    "        elif tag_sent > 0 and my_sent <= 0:\n",
    "            FN += 1\n",
    "\n",
    "    # Sum up all counts and create a DataFrame for the confusion matrix\n",
    "    df = pd.DataFrame({'Total': (TP + FP + TN + FN),\n",
    "                       'TP': [TP],\n",
    "                       'FP': [FP],\n",
    "                       'TN': [TN],\n",
    "                       'FN': [FN]}, index=['Overall'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "\n",
    "    # Extract confusion matrix values\n",
    "    TP = conf_matrix.loc['Overall', 'TP']\n",
    "    FP = conf_matrix.loc['Overall', 'FP']\n",
    "    TN = conf_matrix.loc['Overall', 'TN']\n",
    "    FN = conf_matrix.loc['Overall', 'FN']\n",
    "    \n",
    "    # Calculate precision, recall, accuracy, and F1 score using the extracted values\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0  # Avoid division by zero\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN) if TP + FP + TN + FN != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    # Create a DataFrame to hold calculated metrics, rounded to three decimals\n",
    "    df = pd.DataFrame({'Precision': [round(precision, 3)], \n",
    "                       'Recall': [round(recall, 3)],\n",
    "                       'Accuracy': [round(accuracy, 3)],\n",
    "                       'F1 Score': [round(f1_score, 3)]}, index=['Overall'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_comparison_metrics(avg_metrics_1, avg_metrics_2):\n",
    "\n",
    "    # Define metric labels\n",
    "    labels = ['Precision', 'Recall', 'Accuracy', 'F1 Score']\n",
    "    \n",
    "    # Adjust data scales to exaggerate differences for clearer visual comparison\n",
    "    diff_scale = 10\n",
    "    adjusted_data1 = [1 + (val - min(avg_metrics_1 + avg_metrics_2)) * diff_scale for val in avg_metrics_1]\n",
    "    adjusted_data2 = [1 + (val - min(avg_metrics_1 + avg_metrics_2)) * diff_scale for val in avg_metrics_2]\n",
    "    \n",
    "    # Define bar width\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(len(labels))\n",
    "    \n",
    "    # Plot data for both metric sets\n",
    "    bars1 = ax.bar(index - bar_width/2, adjusted_data1, bar_width, label='Vader')\n",
    "    bars2 = ax.bar(index + bar_width/2, adjusted_data2, bar_width, label='Senti')\n",
    "    \n",
    "    # Set labels, title, and legend for the plot\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Adjusted Score')\n",
    "    ax.set_title('Comparison of Metrics with Exaggerated Differences')\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "344624fe-40a0-4a24-ae30-1726c5abdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main controller function of the Opinion Miner\n",
    "\n",
    "Arguments: \n",
    "    - File: The file name to be parsed\n",
    "    - sentiment_classifier: A string to define which sentiment classifer should be selected \n",
    "    - noun_string: Parameter to select which pre-processed review type to use for feature extraction\n",
    "    - sent_string: Parameter to select which pre-processed review type to use for sentiment analysis\n",
    "    - similarity_threshold: Parameter used during feature extraction to define how close the feautre list should be to the predicted name of the product\n",
    "\n",
    "Returns:\n",
    "    - conf_matrix_df: A confusion matrix comparing my models feature classifications with the provided data set\n",
    "    - metrics_df: A table displaying the precision, recall, accuracy and F1 Score\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def opinion_miner_controller(file, sentiment_classifier, noun_string, sent_string, similarity_threshold):\n",
    "\n",
    "    # Parse the raw text file into a pandas dataframe\n",
    "    df = read_file(file)\n",
    "\n",
    "    # Handle the reviews in various ways and store each variation in a master dataframe for further analysis \n",
    "    df = pre_processing_controller(df)\n",
    "\n",
    "    # Extracts a list of product features from the reviews, add data to the master dataframe\n",
    "    df, features = feature_extraction(df, noun_string, similarity_threshold)\n",
    "\n",
    "    # Calculates the sentiment of each feature in each review, passing in two sentiment parameters and returning an updated master dataframe\n",
    "    df = sentiment_controller(df, sentiment_classifier, sent_string)\n",
    "\n",
    "    # Extracts a feature table and updates the master dataframe with sentiment labeled \n",
    "    feature_table, df = create_feature_table(df, features)\n",
    "\n",
    "    # Filters the master dataframe for the models sentiment tags and the data's sentiment tags\n",
    "    evaluation_table = create_evaluation_table(df)\n",
    "\n",
    "    # Extracts the data's sentiment tags and returns a dictionary \n",
    "    annotated_tags_dict = extract_tags(evaluation_table)\n",
    "    \n",
    "    # Creates a dictionary that maps the models features with the datas features, allowing some variation in similarity\n",
    "    key_mapping = find_similar_features(annotated_tags_dict.keys(), feature_table[next(iter(feature_table))].keys())\n",
    "\n",
    "    # Outputs a dataframe with the models labeled reviews and the datas labeled reveiws for evaluation\n",
    "    result_df = process_tags(evaluation_table, key_mapping)    \n",
    "    \n",
    "    # Calculates a confusion matrix and metrics table \n",
    "    conf_matrix_df = compute_confusion_matrix(result_df)\n",
    "    metrics_df = calculate_metrics(conf_matrix_df)\n",
    "    \n",
    "    return feature_table, conf_matrix_df, metrics_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d22d0fe-9a94-4ec4-a00a-509309057ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function built to average the metrics across files to assist myself in paramater selection due to the large variability in the data files\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def average_metrics(files, file_count, sentiment_classifier, noun_string, sent_string, similarity_threshold):\n",
    "    \n",
    "    data = {}\n",
    "    file_idx = 0\n",
    "    \n",
    "    for file in files:\n",
    "        file_idx += 1\n",
    "        conf_matrix, metrics = opinion_miner_controller(file, sentiment_classifier, noun_string, sent_string, similarity_threshold)\n",
    "        data[file_idx] = {'conf_matrix': conf_matrix, 'metrics': metrics}\n",
    "        if file_idx == file_count:\n",
    "            break\n",
    "    \n",
    "    count = 0\n",
    "    precision, recall, accuracy, f1_score = 0, 0, 0, 0\n",
    "    \n",
    "    for entry in data:\n",
    "        count += 1\n",
    "        precision += data[entry]['metrics']['Precision'].iloc[0]\n",
    "        recall += data[entry]['metrics']['Recall'].iloc[0]\n",
    "        accuracy += data[entry]['metrics']['Accuracy'].iloc[0]\n",
    "        f1_score += data[entry]['metrics']['F1 Score'].iloc[0]\n",
    "    \n",
    "    avg_precision = round(precision / count, 3)\n",
    "    avg_recall = round(recall / count, 3)\n",
    "    avg_accuracy = round(accuracy / count, 3)\n",
    "    avg_f1_score = round(f1_score / count, 3)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_accuracy, avg_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3851157f-c723-4c9a-9712-9d640920a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The method I used to see which pre-processing review string combination to use when extracting features and classifying\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_optimium_string_variables(files):\n",
    "    \n",
    "    for noun_string in ['Soft_Filtered_Review_String', 'Lemmatised_Review_String']:\n",
    "        for sent_string in ['Soft_Filtered_Review_String', 'Lemmatised_Review_String', 'Filtered_Review_String']:\n",
    "    \n",
    "            try:\n",
    "                print('Noun String: ', noun_string, 'Sent String: ', sent_string)\n",
    "                opinion_miner_controller(files[0], 'senti', noun_string, sent_string)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# show_optimium_string_variables(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b6747df-4013-4057-8b8f-c25dbd3d4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The method I used to see which similarity filter to use in my noun feature processing \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sim_filter(files):\n",
    "    \n",
    "    for sim in np.arange(0, 0.5, 0.05).tolist():\n",
    "        opinion_miner_controller(files[0], 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', sim)\n",
    "\n",
    "# sim_filter(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b89d763-895c-49d2-9bb7-9bd6de46d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Function to show the metric comparison between my two sentiment models, averaged across the first 5 files \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sentiment_model_average_comparison(files, average_files):\n",
    "    \n",
    "    avg_vader_metrics = average_metrics(files, average_files, 'vader_blob', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25)\n",
    "    avg_senti_metrics = average_metrics(files, average_files, 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25)\n",
    "    plot_comparison_metrics(avg_vader_metrics, avg_senti_metrics)\n",
    "\n",
    "\n",
    "# sentiment_model_average_comparison(files, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd550a4-7585-4a7b-b3f2-0af67c1eeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output(feature_table, conf_matrix_df, metrics_df):\n",
    "    \n",
    "    df = pd.DataFrame(feature_table[next(iter(feature_table))]).T\n",
    "    df.index.name = next(iter(feature_table))\n",
    "    df.columns = ['Positive Sentiments', 'Negative Sentiments']\n",
    "    \n",
    "    df.plot(kind='bar', figsize=(8, 3), title=f'Sentiment Analysis of {next(iter(feature_table))} Features')\n",
    "    plt.xlabel(f'{next(iter(feature_table))} Feature')\n",
    "    plt.ylabel('Number of Sentiments')\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "    display(conf_matrix_df)\n",
    "    print('\\n')\n",
    "    display(metrics_df)\n",
    "    print('-------------------------------------------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "def run_examples(file_list):\n",
    "    for file in file_list:\n",
    "        feature_table, conf_matrix_df, metrics_df = opinion_miner_controller(file, 'vader_blob', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25)\n",
    "        show_output(feature_table, conf_matrix_df, metrics_df)\n",
    "\n",
    "\n",
    "file_list = [files[0], files[3], files[12]]\n",
    "\n",
    "run_examples(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc385a-3174-4797-9766-6c5a927a490d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
