{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61263af9-60d2-4c0d-bbd6-fa7c022e7af0",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "In the task assigned to me, I faced various challenges due to the limitations of the data provided. The issues primarily stemmed from inconsistencies in tagging conventions and poorly labeled data. This hindered the evaluation process as accurately assessing and interpreting the data, leading to some inaccuracies and inefficiencies in the results derived from the analysis.\n",
    "\n",
    "Key limitations of the data included:\n",
    "\n",
    "- **Inconsistent Naming Conventions**: The tags varied significantly, making it difficult to evaluate consistently. For example, tags like “Zoom” and “Zoom mode,” “dvd player” and “player,” and “Universal remote control” and “remote” indicate a lack of standardisation.\n",
    "\n",
    "- **Poor Labeling**: Some items were incorrectly labeled, such as labeling a “camera” to have a “door” feature. These inaccuracies questioned the reliability of the labels as a source of truth.\n",
    "\n",
    "- **Raw Text Format**: The data required manual parsing, which is more error-prone compared to structured formats that could be easily read by built-in parsers, like CSV.\n",
    "\n",
    "- **Complexity Added by Titles**: The inclusion of titles along with the body text in some reviews added an extra layer of complexity to the data parsing process.\n",
    "\n",
    "- **Varying File Lengths**: Files varied in length. I tended to prefer larger files for their more substantial data volume, which I believed would provide a more robust basis for prediction.\n",
    "\n",
    "To handle these issues, I developed a pipeline controlled by my `opinion_miner_controller` function. This pipeline is executed in the final two cells of my notebook, where the opinion miner runs and outputs various samples. The process includes several critical functions:\n",
    "\n",
    "1. `read_file`: My data parser manages data nuances, such as titles, annotations, and special string characters. It separates text data from tags and sentiments, organising it into a structured Pandas dataframe.\n",
    "\n",
    "2. `pre_processing_controller`: This function prepares the data for analysis by cleaning text, processing stop words, tokenising, lemmatising, and chunking nouns. Feature normalisation for machine learning is a part of pre-processing, but run outside of the main pipeline.\n",
    "\n",
    "3. `feature_extraction`: Extracts features from the parsed review based on the pre-processing string type, a similarity threshold between product and features, and a choice between two feature extraction models. gloVe and Word2Vec similarity models are used to assst in extracting nouns with a relation to the product. \n",
    "\n",
    "5. `sentiment_controller`: Uses parameters like the sentiment classifier and the pre-processing review string type to apply a Vader or Senti classification or a SentiWordNet classification.\n",
    "\n",
    "6. **Further Processing**: Includes creating feature table dictionaries and mapping dictionaries to align similar words (e.g., 'picture' and 'pic') to their correct tags for evaluation.\n",
    "\n",
    "7. **Output**: Outputs include a confusion matrix, a metrics table showing precision, accuracy, recall, and the F1 score, and a feature table from the miner.\n",
    "\n",
    "Additionally, outside the `opinion_miner_controller` function, I developed functions to optimise the miner’s performance:\n",
    "\n",
    "- `average_metrics`: Averages evaluation metrics across three sample files.\n",
    "\n",
    "- `sentiment_model_average_comparison`: Compares the performance of two sentiment models.\n",
    "\n",
    "- `noun_model_comparison`: Shows differences between two noun extraction methods.\n",
    "\n",
    "- `show_optimum_string_variables`: Identifies the best pre-processed strings for the miner.\n",
    "\n",
    "- `sim_filter`: Finds the optimal similarity parameter for feature extraction.\n",
    "\n",
    "I also undertook some further research to see if I could build on my Opinion Miner pipeline, this can be found under the **Further Work** section. Here I built A ML classifier, this pipeline consists of three functions: `parse_and_normalise_tags`, `build_ml_classifier` and `evaluate_ml_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97caa3c8-0c02-48d4-abe7-98b05163312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "sentiment_intensity_analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2b03ea-3191-44bf-9b2e-2c6d67a03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c016251-79e8-49ff-b566-e35bdbedab0e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18ed6882-2c25-48fb-9545-cf4c3668c231",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "The data parsing process initiates with the `read_file` function, designed to effectively handle and extract pertinent content from data files. This function begins by reading the file, identifying, and excluding metadata indicated by a line of asterisks. It then addresses complexities such as titles marked by `[t]`, which are handled by the `handle_titles` function by appending the title to the corresponding review to preserve context and maintain data integrity.\n",
    "\n",
    "Reviews are subsequently split using '##' as a delimiter to separate tags, which may contain embedded metadata and sentiment scores, from the main review content. The extracted tags and text data are then organised into a Pandas dataframe.\n",
    "\n",
    "This dataframe is processed by the `pre_processing_controller`, which includes several key functions:\n",
    "- **Tokenised_Review**: Tokenises reviews and preserves the integrity of compound phrases and adjectives directly linked to nouns using the `preserve_compound_phrases` function. This function constructs compound phrases by concatenating related words with an underscore, thus preserving semantic relationships within the text.\n",
    "  \n",
    "- **Soft_Filtered_Review**: Cleans up the Tokenised_Review by removing numerical characters, punctuation, and normalising capitalisation.\n",
    "  \n",
    "- **Soft_Filtered_Review_String**: Converts Soft_Filtered_Review to a string and processes it through `chunking_post_process` to further preserve compound phrases.\n",
    "  \n",
    "- **Filtered_Review**: Applies more aggressive filtering than Soft_Filtered_Review by removing stopwords, using the `nltk.corpus` library.\n",
    "  \n",
    "- **Lemmatised_Review_String**: Lemmatises reviews using the `Spacy` NLP library to reduce word dilution and enhance uniformity across the text data. Lemmatisation is a valuable pre-processing step that reduces words to their base forms, simplifying text complexity. Lemmatisation improves data consistency, by standardising words.\n",
    "\n",
    "#### References\n",
    "- https://spacy.io/models/en/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c1933d-e77d-4110-afd5-f8aeb435af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present - This is a quirk to parse the data files\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = hanldle_titles(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and set content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def hanldle_titles(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8551b5d4-90ba-48b6-9987-18d38e018b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_compound_phrases(text):\n",
    "    # Process the text with an NLP model\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check for compounds or adjectives linked directly to nouns\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = f\"{token.text}_{token.head.text}\"\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        # Skip nouns that are already part of a compound to prevent duplicates\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        # Include all other tokens normally\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "def chunking_post_process(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Check if the current word is part of a compound phrase\n",
    "        \n",
    "        if '_' in words[i]:\n",
    "            # Append all parts of the compound phrase to the list\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Move to the next word after finishing the compound phrase\n",
    "        # Append non-compound words directly to the list\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    # Return the processed words as a single string\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a474f0fe-11c6-4582-916d-5e36195372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings and applies compound phrase preservation\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation and numbers \n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings and retains the compound phrases - soft means not to filter out stop words\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chunking_post_process)\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation, numbers and stop words \n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatise the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0e768-3ee0-4b03-83ac-c5461ea48ded",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fede2a8-b11f-44ac-aa8e-c527b0d0e377",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "The `feature_extraction` function is the controller function. It is passed three parameters:\n",
    "\n",
    "- The `noun_comparison_flag` switches between the two feature extraction models during evaluation.\n",
    "- The `noun_string` selects which pre-processed review to use\n",
    "- The `similarity_threshold` defines the threshold to filter out dissimilar words\n",
    "\n",
    "`POS_Noun_Tagging` - To extract features from the reviews, I have incorporated Part-of-Speech (POS) tagging, a NLP technique where each word in a text is tagged with its grammatical role using the `pos_tag` function from the NLTK package. This tagging considers the word itself and also its context within the sentence. It factors the word's positioning relative to others and the overall sentence structure, which enhances the precision of the tagging process. The text then undergoes noun extraction, where words tagged with specific noun labels such as 'NN', 'NNS', 'NNP', 'NNPS' are retained. These nouns are then subjected to frequency analysis, identifying the 15 most common nouns for further analysis or processing. The 15 filter was manually chosen through analysing various compund noun outputs.\n",
    "\n",
    "`dependency_parsing_noun_extraction` - I also developed a dependency parser as a more advanced feature extraction model, which surpasses traditional POS tagging by constructing a detailed dependency tree. This tree visually maps the relationships between words in a sentence, using arrows to denote dependencies and illustrate which words depend on others. This approach provides a comprehensive map of word interactions and strongly enhances semantic understanding. According to the Stanford CS224n course on dependency parsing, this technique excels in accurately identifying grammatical roles and relationships, making it particularly effective in clarifying complex constructions like passive voice and nested phrases. This clarity is crucial for accurately interpreting sentences and extracting meaningful information, which aids in tasks like sentiment analysis and information extraction. Dependency parsing also plays a critical role in Semantic Role Labeling by assigning semantic roles to phrases based on their function in the main action of the sentence. Its linguistic consistency across different languages also strengthens its value.\n",
    "\n",
    "`similarity_filter` - This function is designed to refine a list of nouns, presumed to be features of a particular product, ensuring that only the most relevant and distinct nouns are retained. Initially, the product is identified as the first noun in the list, given that the list is sorted by noun relevance. The function then extracts words from a list of tuples, discarding any secondary values. It calculates the semantic similarity of each word to the product using both Word2Vec and GloVe models, and an average of these similarities determines the relevance of each noun. Words that meet a predefined similarity threshold are retained for further consideration. To enhance the uniqueness of the selected features, the function applies a secondary filtering process. This process removes synonyms or near synonymous words such as 'pic' and 'picture', by examining the similarity scores from GloVe's top similar words and excluding those that are highly similar. The models averaging adds a level of diversification. The 15 most common nouns are selected also.\n",
    "\n",
    "#### References\n",
    "- https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00561-y\n",
    "- https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7\n",
    "- https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf\n",
    "- https://wiki.pathmind.com/word2vec\n",
    "- https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/\n",
    "- https://towardsdatascience.com/a-comprehensive-python-implementation-of-glove-c94257c2813d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f8617b-2cd0-499a-a609-177fd19707a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, noun_string, similarity_threshold, noun_comparison_flag):\n",
    "\n",
    "    if noun_comparison_flag == 1:\n",
    "        nouns = POS_Noun_Tagging(df, noun_string)\n",
    "    else:\n",
    "        nouns = dependency_parsing_noun_extraction(df, noun_string)\n",
    "\n",
    "    features = similarity_filter(nouns, similarity_threshold)\n",
    "    df = add_features_to_df(df, features)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f298369f-55f2-44f6-940f-da089722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(df, noun_string):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df[noun_string].tolist()\n",
    "    features = []\n",
    "    \n",
    "    # Process each review to extract nouns\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)  # Tokenise the text\n",
    "        tagged = pos_tag(tokens)  # POS tagging\n",
    "        # Collect nouns from tags\n",
    "        features.extend([word.lower() for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    \n",
    "    # Count and retrieve the 15 most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "\n",
    "    return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2493ba4c-75f9-4d94-8e18-44e3c367deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing_noun_extraction(df, noun_string):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df[noun_string].tolist()\n",
    "    features = []\n",
    "\n",
    "    # Process each review to extract nouns using dependency parsing\n",
    "    for review in reviews:\n",
    "        doc = nlp(review)\n",
    "        # Extract nouns based on their dependency role and POS tag\n",
    "        features.extend([token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN']])\n",
    "\n",
    "    # Count and retrieve the most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(15)\n",
    "    \n",
    "    return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be50234-31a2-45fa-ae2b-d4a15be9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list, similarity_threshold):\n",
    "    \n",
    "    # Create a list of words from the tuples\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    noun_list = []\n",
    "    # Iterate over words to compute similarities\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Calculate similarity using GloVe\n",
    "            glove_similarity = glove_model.similarity(words[0], word)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            \n",
    "            # Append word to list if it meets the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                noun_list.append(word)\n",
    "        except KeyError:\n",
    "            # Skip the word if it's not found in the model's vocabulary\n",
    "            continue\n",
    "\n",
    "    items_to_remove = []\n",
    "    \n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(noun_list[0], topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_nouns = [item for item in noun_list if item not in items_to_remove]\n",
    "\n",
    "    return filtered_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39335d95-f50e-4abe-afd9-46f6b3b7fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These functions are utilised within the opinion_miner_controller module.\n",
    "The primary function, create_feature_table, takes a DataFrame and a list of features as input.\n",
    "It processes the DataFrame, extracting features and sentiments, then updates a dictionary with the feature counts.\n",
    "Each review's features and sentiments are converted into tags, following a specific format for labeling data.\n",
    "These tags are added to the main DataFrame under the column 'My_Sentiment_Tags' for later evaluation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_features_to_df(df, features):\n",
    "    \n",
    "    feature_list = features[1:]\n",
    "    featured_items_list = []\n",
    "    index_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Append the found features as a string \n",
    "            featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            item_list = [f\"{item.strip()}\" for item in featured_items]\n",
    "            featured_items_string = ', '.join(item_list)\n",
    "            featured_items_list.append(featured_items_string) \n",
    "            index_list.append(idx)\n",
    "        else:\n",
    "            featured_items_list.append('')  \n",
    "            index_list.append(idx)\n",
    "\n",
    "    df['Main_Index'] = index_list\n",
    "    df['Featured_Items'] = featured_items_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_feature_table(df, features):\n",
    "    # Process the DataFrame\n",
    "    df = process_data_df(df)\n",
    "    \n",
    "    # Extract the product title and features\n",
    "    product_title = features[0]\n",
    "    product_features = features[1:]\n",
    "    \n",
    "    # Create a dictionary to store feature counts\n",
    "    product_dict = {product_title: {feature: {'positive': 0, 'negative': 0} for feature in product_features}}\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Split the featured items and sentiments\n",
    "        feature_items = row['Featured_Items'].split(',')\n",
    "        sentiments = row['Sentiment'] if isinstance(row['Sentiment'], list) else [row['Sentiment']]\n",
    "        \n",
    "        # Update the feature table with counts\n",
    "        for feature, sentiment in zip(feature_items, sentiments):\n",
    "            update_feature_table(product_dict, product_title, feature.strip(), sentiment)\n",
    "    \n",
    "    return product_dict, df\n",
    "\n",
    "\n",
    "\n",
    "def update_feature_table(product_dict, title, feature, sentiment):\n",
    "    # Update the feature counts based on sentiment\n",
    "    if feature and feature in product_dict[title]:\n",
    "        if sentiment > 0:\n",
    "            product_dict[title][feature]['positive'] += 1\n",
    "        elif sentiment < 0:\n",
    "            product_dict[title][feature]['negative'] += 1\n",
    "\n",
    "\n",
    "\n",
    "def process_data_df(df):\n",
    "    my_tags = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Split features and get sentiments\n",
    "        features = row['Featured_Items'].split(',') if row['Featured_Items'] else []\n",
    "        sentiments = row['Sentiment']\n",
    "        \n",
    "        # Process features and sentiments into tags\n",
    "        tags = process_features_and_sentiments(features, sentiments)\n",
    "        my_tags.append(tags)\n",
    "    \n",
    "    # Add tags to the DataFrame\n",
    "    df['My_Sentiment_Tags'] = my_tags\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_features_and_sentiments(features, sentiments):\n",
    "    result_tags = []  # This will store the final list of sentiment tags for the features\n",
    "\n",
    "    # Ensure sentiments is always a list\n",
    "    if not isinstance(sentiments, list):\n",
    "        sentiments = [sentiments]  # Convert single integer to list\n",
    "\n",
    "    # Loop through each feature and its index\n",
    "    for i, feature in enumerate(features):\n",
    "        # Strip any whitespace around the feature name to ensure clean data\n",
    "        cleaned_feature = feature.strip()\n",
    "\n",
    "        # Continue only if the feature is not an empty string\n",
    "        if cleaned_feature:\n",
    "            # Access the corresponding sentiment value or default to 0 if out of range\n",
    "            sentiment_value = sentiments[i] if i < len(sentiments) else 0\n",
    "\n",
    "            # Format the feature and its sentiment into a string, appending it to the result list\n",
    "            tag_string = f\"{cleaned_feature}[{sentiment_value}]\"\n",
    "            result_tags.append(tag_string)\n",
    "\n",
    "    return result_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3dc59-7dc0-49fe-9dea-7cf815e53749",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e153d7a-bb12-4cb4-a3ce-d8cd0780cbfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "The `sentiment_controller` function inputs the data dataframe, the classifier and the string type to in which to formulate the sentiment. \n",
    "Each feature that has been labelled will be passed individual for sentiment classification, for each review in the file.\n",
    "\n",
    "I have selected two models for sentiment analysis, `SentiWordNet` and `VADER`, each with distinct strengths and applications.\n",
    "\n",
    "#### SentiWordNet\n",
    "SentiWordNet is an advanced lexical resource that augments the traditional WordNet database by integrating three types of sentiment scores: positivity, negativity, and objectivity, into each synset. These sentiment scores, ranging from 0 to 1, describe how objective, positive, and negative the terms within the synset are, determined through semi-supervised learning techniques and manual adjustments to ensure an accurate reflection of word sentiments. SentiWordNet enhances text representations by adding sentiment related properties of terms. The development of SentiWordNet involves the quantitative analysis of glosses related to synsets and vectorial term representations. This process uses a committee of eight classifiers with similar accuracy levels but differing behaviours to derive the sentiment scores, thereby providing detailed word sense representation and extensive coverage of over 115,000 WordNet synsets. However, SentiWordNet has limitations, including static sentiment scores that may not reflect changing contextual meanings, limited coverage of modern slang or newly coined terms, and potential inaccuracies in capturing the sentiments of polysemous words due to its reliance on generalised word usage.\n",
    "\n",
    "#### VADER\n",
    "To address some of the limitations seen in SentiWordNet, particularly in handling dynamic and informal text, VADER (Valence Aware Dictionary and sEntiment Reasoner) presents it self as a strong alternative. Recognised for its ability to efficiently process and analyse large volumes of text data, VADER is particularly valuable for evaluating customer feedback on social media, which may well translate to online produt reviews. Its strength lies in managing texts that feature unconventional forms, such as emojis, varied punctuation, and internet specific expressions. The core of VADER's functionality is its sentiment lexicon, developed through crowdsourced input via Amazon’s Mechanical Turk. This approach ensures the lexicon is not only comprehensive but also reflective of modern language and expressions typical in social media communications. Unlike SentiWordNet, VADER is stronger when understanding context, assessing how sentiments are influenced by how something is said through capitalisation, punctuation, or other stylistic nuances rather than solely by what is said. This sophisticated context understanding makes VADER especially effective in sentiment analysis, providing a deep insight into both explicit and implicit sentiments embedded within digital communications.\n",
    "\n",
    "#### References\n",
    "\n",
    "- https://ontotext.fbk.eu/sentiwn.html\n",
    "- https://srish6.medium.com/sentiment-analysis-using-the-sentiwordnet-lexicon-1a3d8d856a10\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/\n",
    "- https://towardsdatascience.com/an-short-introduction-to-vader-3f3860208d53\n",
    "- https://vadersentiment.readthedocs.io/en/latest/\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbb4e4d-5031-45c2-adfc-39b39631bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(df, classifier, sent_string):\n",
    "\n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Function to get sentiment\n",
    "    def get_sentiment(review, classifier, features):\n",
    "        if classifier == 'senti':\n",
    "            pos_score, neg_score = senti_classifier(review)\n",
    "            return 1 if pos_score > neg_score else -1\n",
    "        elif classifier == 'vader':\n",
    "            return vader_classifier(review, features)\n",
    "        return 0  # default case if no classifier matches\n",
    "\n",
    "    # Process each review in the DF\n",
    "    for idx, row in df.iterrows():\n",
    "        review = row[sent_string]\n",
    "        features = [feature.strip() for feature in row['Featured_Items'].split(',')]\n",
    "        \n",
    "        # Apply sentiment analysis for each feature or single feature case\n",
    "        if len(features) == 1:  \n",
    "            sentiment = get_sentiment(review, classifier, features)\n",
    "            sentiment_list.append(sentiment)\n",
    "        else: \n",
    "            sentiments = [get_sentiment(review, classifier, [feature]) for feature in features]\n",
    "            sentiment_list.append(sentiments)\n",
    "            \n",
    "    df['Sentiment'] = sentiment_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b87add5-a897-4fe5-99ac-89c3521c4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senti_classifier(phrase):\n",
    "    \n",
    "    # First try to get sentiment score for the whole phrase as a compound\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # If no score, break down the phrase and calculate average sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    \n",
    "    # Average the sentiment scores\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    # Retrieve sentiment scores from SentiWordNet\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fce2dfd-33cb-4d23-964a-50c531eaad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_classifier(sentence, features):\n",
    "\n",
    "    # Dictionary to store sentiment results for each feature\n",
    "    sentiment_results = {}\n",
    "\n",
    "    features = [feature.strip() for feature in features]  \n",
    "    \n",
    "    # Analyse sentiment for each feature\n",
    "    for feature in features:\n",
    "        # Extract context around the feature if needed (optional improvement)\n",
    "        start_index = sentence.lower().find(feature.lower())\n",
    "        if start_index != -1:\n",
    "            # Extract a sub-sentence for context-based sentiment analysis\n",
    "            sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "            # Get sentiment using VADER\n",
    "            vader_sentiment = sentiment_intensity_analyser.polarity_scores(sub_sentence)['compound']\n",
    "  \n",
    "            sentiment = 1 if vader_sentiment > 0.05 else -1 if vader_sentiment < -0.05 else 0\n",
    "            \n",
    "        else:\n",
    "           sentiment = 0\n",
    "            \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829857e7-e868-4c16-ae01-307e0b89ae0a",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b514aafe-9cdd-41f9-9b7c-d77dac81e398",
   "metadata": {},
   "source": [
    "# Further Work \n",
    "### A OneVsRestClassifier Logistic Regression Classifier\n",
    "\n",
    "\n",
    "* The `parse_and_normalise_tags` function parses annotated tags into a machine-readable format and normalises features to create a vector matrix.\n",
    "* The **MultiLabelBinarizer** from **sklearn.preprocessing** transforms the list of tag-sentiment combinations into a binary format. Each unique tag-sentiment combination across all entries becomes a feature column, with each row in the output matrix Y indicating the presence (1) or absence (0) of that tag-sentiment in a specific review.\n",
    "* The **MultilabelStratifiedShuffleSplit** from the **iterstrat.ml_stratifiers** package splits the data into training and test sets while maintaining the same proportion of each label in both sets, ensuring all labels are adequately represented.\n",
    "* A logistic regression model, wrapped in a **OneVsRestClassifier**, is employed to train a separate binary classifier for each label. The model predicts the probability of a review being associated with each tag-sentiment based on its TF-IDF features.\n",
    "* Class weights are calculated using the **compute_class_weight** function, which assigns weights inversely related to class frequencies. These weights are incorporated into the logistic regression model to adjust the model’s focus, ensuring that errors involving minority classes are more heavily penalised during training.\n",
    "* After prediction, the **mlb.inverse_transform** method converts the predicted binary labels back into the tag-sentiment format, making the output more interpretable by translating the binary predictions back into the original tags and sentiments.\n",
    "\n",
    "#### References\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "- https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "- https://blockgeni.com/using-one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "- https://machinelearningmastery.com/cost-sensitive-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914e74ea-3100-432e-8009-41a988a7e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d87c9f-f6a8-4c70-b471-e99a8c5ae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_normalise_tags(df):\n",
    "    \n",
    "    normalised_data = []\n",
    "\n",
    "    for idx, tags in enumerate(df['Tags']):\n",
    "        parsed_tags = {}\n",
    "        parsed_sents = []\n",
    "\n",
    "        for tag in tags:\n",
    "            tag = tag.strip()\n",
    "            inside = tag.find('[')\n",
    "            outside = tag.find(']')\n",
    "            feature = tag[0:inside].strip()\n",
    "            sent = tag[inside + 1:outside].strip()\n",
    "            \n",
    "            try:\n",
    "                sent = int(sent)\n",
    "                if feature in parsed_tags:\n",
    "                    # Average or list sentiment scores if more than one sentiment per feature\n",
    "                    if isinstance(parsed_tags[feature], list):\n",
    "                        parsed_tags[feature].append(sent)\n",
    "                    else:\n",
    "                        parsed_tags[feature] = [parsed_tags[feature], sent]\n",
    "                else:\n",
    "                    parsed_tags[feature] = sent\n",
    "            except ValueError:\n",
    "                continue  # Skip if the sentiment is not an integer\n",
    "\n",
    "        # Normalise sentiments if there are multiple entries\n",
    "        for key, value in parsed_tags.items():\n",
    "            if isinstance(value, list):\n",
    "                parsed_tags[key] = sum(value) / len(value)  # Average sentiments\n",
    "\n",
    "        normalised_data.append(parsed_tags)\n",
    "\n",
    "    # Add the normalised data to the DataFrame\n",
    "    df['Normalised'] = normalised_data\n",
    "    df = df[['Normalised', 'Lemmatised_Review_String']]\n",
    "    df = df.rename(columns={'Normalised': 'Tags_and_Sentiment', 'Lemmatised_Review_String': 'Reviews'})\n",
    "\n",
    "    # Flatten the normalised dictionary into a DataFrame\n",
    "    tags_sents_df = pd.DataFrame(df['Tags_and_Sentiment'].tolist()).fillna(0).astype(int)\n",
    "\n",
    "    # Join this back with the original DataFrame\n",
    "    df = df.join(tags_sents_df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ed3e2e-4050-4293-81c6-1ead5967e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ml_classifier(df):\n",
    "    \n",
    "    def extract_features(tags_sentiments_dict):\n",
    "        return [f\"{tag}_{sentiment}\" for tag, sentiment in tags_sentiments_dict.items()]\n",
    "    \n",
    "    df['Feature_Sentiments'] = df['Tags_and_Sentiment'].apply(extract_features)\n",
    "    \n",
    "    # Now apply MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y = mlb.fit_transform(df['Feature_Sentiments'])\n",
    "    label_counts = Y.sum(axis=0)\n",
    "    Y = Y[:, label_counts < len(Y)]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in msss.split(df['Reviews'], Y):\n",
    "        X_train = vectorizer.fit_transform(df.loc[train_index, 'Reviews'])\n",
    "        Y_train = Y[train_index]\n",
    "        X_test = vectorizer.transform(df.loc[test_index, 'Reviews'])\n",
    "        Y_test = Y[test_index]\n",
    "    \n",
    "        # Calculate class weights for balancing\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(Y_train), y=Y_train.ravel())\n",
    "        class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "        \n",
    "        model = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight=class_weight_dict))\n",
    "        model.fit(X_train, Y_train)\n",
    "    \n",
    "        # Accessing reviews during prediction (for illustration or further analysis)\n",
    "        predictions = []\n",
    "        for idx in test_index:\n",
    "            review = df.loc[idx, 'Reviews']\n",
    "            new_review_processed = vectorizer.transform([review])\n",
    "            predicted = model.predict(new_review_processed)\n",
    "            predicted_labels = mlb.inverse_transform(predicted)\n",
    "            predictions.append(predicted_labels)\n",
    "    \n",
    "    labels = list(df['Tags_and_Sentiment'].iloc[1372:])\n",
    "    \n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ce7e7f8-3f6a-4790-8bbf-c871ecde46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_classifier(predictions, labels):\n",
    "\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        \n",
    "        for pred in prediction[0]:\n",
    "            \n",
    "            split_pred = pred.split('_')\n",
    "            pred_feature = split_pred[0]\n",
    "            pred_sent = int(split_pred[1])\n",
    "\n",
    "            if pred_feature in label.keys():\n",
    "                if pred_sent > 0 and label.get(pred_feature) > 0:\n",
    "                    TP += 1\n",
    "                elif pred_sent > 0 and label.get(pred_feature) < 0:\n",
    "                    FP += 1\n",
    "                elif pred_sent < 0 and label.get(pred_feature) < 0:\n",
    "                    TN += 1\n",
    "                elif pred_sent < 0 and label.get(pred_feature) > 0:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "    # Sum up all counts and create a DataFrame for the confusion matrix\n",
    "    conf_df = pd.DataFrame({'Total': (TP + FP + TN + FN),\n",
    "                            'TP': [TP],\n",
    "                            'FP': [FP],\n",
    "                            'TN': [TN],\n",
    "                            'FN': [FN]}, index=['Overall'])\n",
    "\n",
    "    # Calculate precision, recall, accuracy, and F1 score using the extracted values\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0  # Avoid division by zero\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN) if TP + FP + TN + FN != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    \n",
    "    # Create a DataFrame to hold calculated metrics, rounded to three decimals\n",
    "    metric_df = pd.DataFrame({'Precision': [round(precision, 3)], \n",
    "                              'Recall': [round(recall, 3)],\n",
    "                              'Accuracy': [round(accuracy, 3)],\n",
    "                              'F1 Score': [round(f1_score, 3)]}, index=['Overall'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('Confusion matrix evaluating against matched labels in data file:')\n",
    "    print('\\n')\n",
    "    display(conf_df)\n",
    "    print('\\n')\n",
    "    print('Metrics table evaluating against matched labels in data file:')\n",
    "    print('\\n')\n",
    "    display(metric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72be0cf-0d62-441d-b58a-9a5c03cc4b4c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2af2eef0-37b9-44f7-b01b-1ac9ebac146c",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "During the pre-processing phase, I employed various methods to process the data. These included decapitalisation, removal of numerals, exclusion of stop words, punctuation elimination, lemmatisation, and noun chunking. As a result, I generated multiple versions of the review texts that I could utilise in my opinion mining process. To determine the most effective string combination, I tested each combination by scoring them based on the formula: (number of feature matches to tagged data multiplied by accuracy). The optimum review string was: Lemmatised_Review_String for feature extraction and the Soft_Filtered_Review_String for sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e4cc00a-5a34-41b1-b5ce-8c963cc36f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing functions to:\n",
    "    - Filter for the models sentiment tags and the data's sentiment tags\n",
    "    - Extracts the data's sentiment tags as a dictionary \n",
    "    - Creates a mapping dictionary that maps the models features with the datas features, with similarity variability\n",
    "    - Create a dataframe for the models labeled reviews and the datas labeled reveiws for evaluation \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def create_evaluation_table(df):\n",
    "    \n",
    "    valid_rows = []\n",
    "\n",
    "    # Iterate through each row and check if 'Tags' is non-empty\n",
    "    for idx, row in df.iterrows():\n",
    "        # Check if the first element in 'Tags' is not an empty string\n",
    "        try:\n",
    "            if row['Tags'][0] != '':\n",
    "                # Append the row (as a DataFrame) to the list\n",
    "                valid_rows.append(row)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    eval_df = pd.DataFrame(valid_rows)\n",
    "    \n",
    "    return eval_df[['Tags', 'My_Sentiment_Tags']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_tags(sentiment_string):\n",
    "    \n",
    "    # Ensure the input is a string for regex operations\n",
    "    if not isinstance(sentiment_string, str):\n",
    "        if isinstance(sentiment_string, list):\n",
    "            # If it's a list, convert to a comma-separated string\n",
    "            sentiment_string = ', '.join(sentiment_string)\n",
    "        else:\n",
    "            # Otherwise, convert to string in any other case\n",
    "            sentiment_string = str(sentiment_string)\n",
    "    \n",
    "    # Use regex to find patterns of words followed by [number] and convert to a dictionary\n",
    "    output = {match[0]: int(match[1]) for match in re.findall(r'(\\w+)\\[([+-]?\\d+)\\]', sentiment_string)}\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluation_mapping(evaluation_dict, key_mapping):\n",
    "\n",
    "    # Create a dictionary with unique keys and initialise sentiment counts\n",
    "    unique_keys = set(key_mapping.values())\n",
    "    data_dict = {key.strip().lower(): {'positive': 0, 'negative': 0} for key in unique_keys if key}\n",
    "\n",
    "    # Populate the dictionary with sentiment values from the evaluation_dict using the key_mapping\n",
    "    for key, sentiments in evaluation_dict.items():\n",
    "        mapped_key = key_mapping.get(key)\n",
    "        if mapped_key:\n",
    "            mapped_key = mapped_key.strip().lower()\n",
    "            if mapped_key in data_dict:\n",
    "                # Increment sentiment counts based on mapped keys\n",
    "                data_dict[mapped_key]['positive'] += sentiments['positive']\n",
    "                data_dict[mapped_key]['negative'] += sentiments['negative']\n",
    "                \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_similar_features(my_features, annotated_features, threshold=0.7):\n",
    "\n",
    "    similar_features = {}\n",
    "    # Convert feature lists to spaCy NLP objects for vector comparison\n",
    "    annotated_features_nlp = {feature: nlp(feature) for feature in annotated_features}\n",
    "    my_features_nlp = {feature: nlp(feature) for feature in my_features}\n",
    "\n",
    "    # Compare each feature's NLP object for similarity above a set threshold\n",
    "    for my_feat, my_feat_nlp in my_features_nlp.items():\n",
    "        for anno_feat, anno_feat_nlp in annotated_features_nlp.items():\n",
    "            # Check if both terms have vectors and compare their similarity\n",
    "            if my_feat_nlp.has_vector and anno_feat_nlp.has_vector:\n",
    "                if my_feat_nlp.similarity(anno_feat_nlp) > threshold:\n",
    "                    similar_features[my_feat] = anno_feat\n",
    "    \n",
    "    return similar_features\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def process_tags(df, key_mapping):\n",
    "\n",
    "    results = []\n",
    "    reverse_mapping = {v: k for k, v in key_mapping.items()}\n",
    "\n",
    "    # Iterate through DataFrame to apply mappings and extract sentiments\n",
    "    for idx, row in df.iterrows():\n",
    "        tags = extract_tags(row['Tags'])\n",
    "        my_sentiment_tags = extract_tags(row['My_Sentiment_Tags'])\n",
    "\n",
    "        # Compare and store results of tag sentiments\n",
    "        for tag, sentiment in tags.items():\n",
    "            mapped_tag = reverse_mapping.get(tag, tag)  # Map tags to a common key, if possible\n",
    "            if mapped_tag in my_sentiment_tags:\n",
    "                # Store results if there's a matching sentiment tag\n",
    "                results.append({\n",
    "                    'Index': idx,\n",
    "                    'Feature': mapped_tag,\n",
    "                    'Tags Sentiment': sentiment,\n",
    "                    'My Sentiment Tags Sentiment': my_sentiment_tags[mapped_tag]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "730619e9-95a3-4393-981c-46e15eb2fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing functions to create the confusion matrix, calculate the metrics from the matrix and output a matrix comparison chart\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(result_df):\n",
    "\n",
    "    # Initialise counters for True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN)\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    # Iterate through each row in the DataFrame to compare sentiment tags\n",
    "    for index, row in result_df.iterrows():\n",
    "        \n",
    "        # Extract sentiment from 'Tags Sentiment' and 'My Sentiment Tags Sentiment'\n",
    "        tag_sent = row['Tags Sentiment']\n",
    "        my_sent = row['My Sentiment Tags Sentiment']\n",
    "\n",
    "        # Compare extracted sentiments and classify as TP, FP, TN, or FN\n",
    "        if tag_sent > 0 and my_sent > 0:\n",
    "            TP += 1\n",
    "        elif tag_sent <= 0 and my_sent > 0:\n",
    "            FP += 1\n",
    "        elif tag_sent <= 0 and my_sent <= 0:\n",
    "            TN += 1\n",
    "        elif tag_sent > 0 and my_sent <= 0:\n",
    "            FN += 1\n",
    "\n",
    "    # Sum up all counts and create a DataFrame for the confusion matrix\n",
    "    df = pd.DataFrame({'Total': (TP + FP + TN + FN),\n",
    "                       'TP': [TP],\n",
    "                       'FP': [FP],\n",
    "                       'TN': [TN],\n",
    "                       'FN': [FN]}, index=['Overall'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "\n",
    "    # Extract confusion matrix values\n",
    "    TP = conf_matrix.loc['Overall', 'TP']\n",
    "    FP = conf_matrix.loc['Overall', 'FP']\n",
    "    TN = conf_matrix.loc['Overall', 'TN']\n",
    "    FN = conf_matrix.loc['Overall', 'FN']\n",
    "    \n",
    "    # Calculate precision, recall, accuracy, and F1 score using the extracted values\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0  # Avoid division by zero\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN) if TP + FP + TN + FN != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    # Create a DataFrame to hold calculated metrics, rounded to three decimals\n",
    "    df = pd.DataFrame({'Precision': [round(precision, 3)], \n",
    "                       'Recall': [round(recall, 3)],\n",
    "                       'Accuracy': [round(accuracy, 3)],\n",
    "                       'F1 Score': [round(f1_score, 3)]}, index=['Overall'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_comparison_metrics(avg_metrics_1, avg_metrics_2, option):\n",
    "\n",
    "    # Define metric labels\n",
    "    labels = ['Precision', 'Recall', 'Accuracy', 'F1 Score']\n",
    "    \n",
    "    # Adjust data scales to exaggerate differences for clearer visual comparison\n",
    "    diff_scale = 10\n",
    "    adjusted_data1 = [1 + (val - min(avg_metrics_1 + avg_metrics_2)) * diff_scale for val in avg_metrics_1]\n",
    "    adjusted_data2 = [1 + (val - min(avg_metrics_1 + avg_metrics_2)) * diff_scale for val in avg_metrics_2]\n",
    "    \n",
    "    # Define bar width\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    # Initialise plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    index = np.arange(len(labels))\n",
    "    \n",
    "    # Plot data for both metric sets\n",
    "    if option == 'sent':\n",
    "        bars1 = ax.bar(index - bar_width/2, adjusted_data1, bar_width, label='Vader')\n",
    "        bars2 = ax.bar(index + bar_width/2, adjusted_data2, bar_width, label='Senti')\n",
    "        name = 'Sentiment Models'\n",
    "    else:\n",
    "        bars1 = ax.bar(index - bar_width/2, adjusted_data1, bar_width, label='POS')\n",
    "        bars2 = ax.bar(index + bar_width/2, adjusted_data2, bar_width, label='Dependency')\n",
    "        name = 'Noun Extraction Models'\n",
    "    \n",
    "    # Set labels, title, and legend for the plot\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Adjusted Score')\n",
    "    ax.set_title(f'Comparison of {name} with Exaggerated Differences')\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "344624fe-40a0-4a24-ae30-1726c5abdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main controller function of the Opinion Miner\n",
    "\n",
    "Arguments: \n",
    "    - File: The file name to be parsed\n",
    "    - sentiment_classifier: A string to define which sentiment classifer should be selected \n",
    "    - noun_string: Parameter to select which pre-processed review type to use for feature extraction\n",
    "    - sent_string: Parameter to select which pre-processed review type to use for sentiment analysis\n",
    "    - similarity_threshold: Parameter used during feature extraction to define how close the feautre list should be to the predicted name of the product\n",
    "\n",
    "Returns:\n",
    "    - conf_matrix_df: A confusion matrix comparing my models feature classifications with the provided data set\n",
    "    - metrics_df: A table displaying the precision, recall, accuracy and F1 Score\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def opinion_miner_controller(file, sentiment_classifier, noun_string, sent_string, similarity_threshold, noun_comparison_flag):\n",
    "\n",
    "    # Parse the raw text file into a pandas dataframe\n",
    "    df = read_file(file)\n",
    "\n",
    "    # Handle the reviews in various ways and store each variation in a master dataframe for further analysis \n",
    "    df = pre_processing_controller(df)\n",
    "\n",
    "    # Extracts a list of product features from the reviews, add data to the master dataframe\n",
    "    df, features = feature_extraction(df, noun_string, similarity_threshold, noun_comparison_flag)\n",
    "\n",
    "    # Calculates the sentiment of each feature in each review, passing in two sentiment parameters and returning an updated master dataframe\n",
    "    df = sentiment_controller(df, sentiment_classifier, sent_string)\n",
    "\n",
    "    # Extracts a feature table and updates the master dataframe with sentiment labeled \n",
    "    feature_table, df = create_feature_table(df, features)\n",
    "\n",
    "    # Filters the master dataframe for the models sentiment tags and the data's sentiment tags\n",
    "    evaluation_table = create_evaluation_table(df)\n",
    "    \n",
    "    # Extracts the data's sentiment tags and returns a dictionary \n",
    "    annotated_tags_dict = extract_tags(evaluation_table)\n",
    "    \n",
    "    # Creates a dictionary that maps the models features with the datas features, allowing some variation in similarity\n",
    "    key_mapping = find_similar_features(annotated_tags_dict.keys(), feature_table[next(iter(feature_table))].keys())\n",
    "\n",
    "    # Outputs a dataframe with the models labeled reviews and the datas labeled reveiws for evaluation\n",
    "    result_df = process_tags(evaluation_table, key_mapping)    \n",
    "    \n",
    "    # Calculates a confusion matrix and metrics table \n",
    "    conf_matrix_df = compute_confusion_matrix(result_df)\n",
    "    metrics_df = calculate_metrics(conf_matrix_df)\n",
    "    \n",
    "    return feature_table, conf_matrix_df, metrics_df, df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d22d0fe-9a94-4ec4-a00a-509309057ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function built to average the metrics across files to assist myself in paramater selection due to the large variability in the data files\n",
    "The sentiment_model_average_comparison function is used to show the metric comparison between my two sentiment models, averaged across all the sample file list.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def average_metrics(file_list, sentiment_classifier, noun_string, sent_string, similarity_threshold, noun_comparison_flag):\n",
    "    \n",
    "    data = {}\n",
    "    file_idx = 0\n",
    "    \n",
    "    for file in file_list:\n",
    "        file_idx += 1\n",
    "        feature_table, conf_matrix, metrics, _ = opinion_miner_controller(file, sentiment_classifier, noun_string, sent_string, similarity_threshold, noun_comparison_flag)\n",
    "        data[file_idx] = {'conf_matrix': conf_matrix, 'metrics': metrics}\n",
    "    \n",
    "    count = 0\n",
    "    precision, recall, accuracy, f1_score = 0, 0, 0, 0\n",
    "    \n",
    "    for entry in data:\n",
    "        count += 1\n",
    "        precision += data[entry]['metrics']['Precision'].iloc[0]\n",
    "        recall += data[entry]['metrics']['Recall'].iloc[0]\n",
    "        accuracy += data[entry]['metrics']['Accuracy'].iloc[0]\n",
    "        f1_score += data[entry]['metrics']['F1 Score'].iloc[0]\n",
    "    \n",
    "    avg_precision = round(precision / count, 3)\n",
    "    avg_recall = round(recall / count, 3)\n",
    "    avg_accuracy = round(accuracy / count, 3)\n",
    "    avg_f1_score = round(f1_score / count, 3)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_accuracy, avg_f1_score\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_model_average_comparison(file_list):\n",
    "\n",
    "    print('\\n')\n",
    "    avg_vader_metrics = average_metrics(file_list, 'vader', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 1)\n",
    "    avg_senti_metrics = average_metrics(file_list, 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 1)\n",
    "    plot_comparison_metrics(avg_vader_metrics, avg_senti_metrics, 'sent')\n",
    "\n",
    "\n",
    "def noun_model_comparison(file_list):\n",
    "    \n",
    "    print('\\n')\n",
    "    pos_metrics = average_metrics(file_list, 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 1)\n",
    "    dependency_metrics = average_metrics(file, 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 0)\n",
    "    plot_comparison_metrics(pos_metrics, dependency_metrics, 'nouns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3851157f-c723-4c9a-9712-9d640920a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The method I used to see which pre-processing review string combination to use when extracting features and classifying\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_optimium_string_variables(files):\n",
    "\n",
    "    weighted_scores = []\n",
    "    string_combinations = []\n",
    "    \n",
    "    for noun_string in ['Soft_Filtered_Review_String', 'Lemmatised_Review_String']:\n",
    "        for sent_string in ['Soft_Filtered_Review_String', 'Lemmatised_Review_String', 'Filtered_Review_String']:\n",
    "            try:\n",
    "                _, conf_matrix_df, metrics_df, _ = opinion_miner_controller(files[0], 'senti', noun_string, sent_string, 0.25, 1)\n",
    "                string_combinations.append(f'{noun_string.split(\"_Review_String\")[0]} / {sent_string.split(\"_Review_String\")[0]}')\n",
    "                eval_count = conf_matrix_df['Total'].iloc[0]\n",
    "                accuracy = metrics_df['Accuracy'].iloc[0]\n",
    "                weighted_score = round(eval_count * accuracy, 1)\n",
    "                weighted_scores.append(weighted_score)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    data = {'string_combinations': string_combinations,\n",
    "        'weighted_scores': weighted_scores}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(df['string_combinations'], df['weighted_scores'], width=0.25, color='orange')\n",
    "    plt.bar(df['string_combinations'], df['weighted_scores'])\n",
    "    plt.xlabel('String Combinations')\n",
    "    plt.ylabel('Weighted Scores')\n",
    "    plt.title('Weighted Scores by Review String Combinations')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b6747df-4013-4057-8b8f-c25dbd3d4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The method I used to see which similarity filter to use in my noun feature processing \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sim_filter(files):\n",
    "    \n",
    "    for sim in np.arange(0, 0.5, 0.05).tolist():\n",
    "        _, conf_matrix_df, metrics_df, _ = opinion_miner_controller(files[0], 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', sim, 1)\n",
    "        display(conf_matrix_df)\n",
    "        display(metrics_df)\n",
    "        \n",
    "# sim_filter(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d44bfc9-ab6e-4a4c-9232-48e984f67a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(file_list):\n",
    "    example_index = 0\n",
    "    \n",
    "    for file in file_list:\n",
    "        example_index += 1 \n",
    "        print('\\n')\n",
    "        print(f'*****  Example {example_index}  *****')\n",
    "        print('\\n')\n",
    "        feature_table, conf_matrix_df, metrics_df, _ = opinion_miner_controller(file, 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 1)\n",
    "        \n",
    "        df = pd.DataFrame(feature_table[next(iter(feature_table))]).T\n",
    "        df.index.name = next(iter(feature_table))\n",
    "        df.columns = ['Positive Sentiments', 'Negative Sentiments']\n",
    "        \n",
    "        df.plot(kind='bar', figsize=(8, 3), title=f'Sentiment Analysis of {next(iter(feature_table))} Features')\n",
    "        plt.xlabel(f'{next(iter(feature_table))} Feature')\n",
    "        plt.ylabel('Number of Sentiments')\n",
    "        print('\\n')\n",
    "        print('My models feature table:')\n",
    "        print('\\n')\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n')\n",
    "        print('Confusion matrix evaluating against matched labels in data file:')\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "        print('\\n')\n",
    "        print('Metrics table evaluating against matched labels in data file:')\n",
    "        print('\\n')\n",
    "        display(metrics_df)\n",
    "        print('\\n')\n",
    "        print('-------------------------------------------------------------------------------------------')\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31be5d7e-5ea5-4da1-8b87-0e992e86c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opinion_miner(files):\n",
    "\n",
    "    # Three feature extraction sample outputs of my Opinion Miner \n",
    "    sample_file_list = [files[1], files[2], files[11]]\n",
    "    # show_examples(sample_file_list)\n",
    "\n",
    "    # Comparison of pre-processing review string combinations \n",
    "    # show_optimium_string_variables(files)\n",
    "    \n",
    "    # Metric comparison of my two sentiment models \n",
    "    # sentiment_model_average_comparison(sample_file_list)\n",
    "\n",
    "    # Metric comparison of my two noun models \n",
    "    # noun_model_comparison([files[1]])\n",
    "\n",
    "    # Output of ML Classifier \n",
    "    _, _, _, df = opinion_miner_controller(files[2], 'senti', 'Lemmatised_Review_String', 'Soft_Filtered_Review_String', 0.25, 1)\n",
    "    df = parse_and_normalise_tags(copy.deepcopy(df))\n",
    "    predictions, labels = build_ml_classifier(df)\n",
    "    evaluate_ml_classifier(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6310ab5-9554-4210-9ce8-f8e3e3b413a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leon/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sklearn/multiclass.py:84: UserWarning: Label not 317 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix evaluating against matched labels in data file:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total  TP  FP  TN  FN\n",
       "Overall      7   3   4   0   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Metrics table evaluating against matched labels in data file:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>0.429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Precision  Recall  Accuracy  F1 Score\n",
       "Overall      0.429     1.0     0.429       0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_opinion_miner(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8f237-4ccb-47f6-861f-febb9e5bd6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
