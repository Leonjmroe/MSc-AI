{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61263af9-60d2-4c0d-bbd6-fa7c022e7af0",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "#### Overview of the Data\n",
    "\n",
    "The dataset consists of product reviews from Amazon, segmented into features with annotated sentiment scores. Reviews include multiple aspects such as ease of use, picture quality, and additional functionalities, each tagged with a corresponding sentiment score. These annotations provide a rich basis for sentiment analysis but require precise parsing to effectively utilize the structured format in which they are presented.\n",
    "\n",
    "#### System-Level Outline\n",
    "\n",
    "##### Data Parsing and Pre-processing\n",
    "\n",
    "Using a custom `read_file` function, the system initially parses the structured reviews from text files. This function handles the dataset's specific format, which includes initial metadata and reviewer comments split by a unique delimiter ('##'). This is crucial for separating feature tags from review content, facilitating subsequent analysis.\n",
    "\n",
    "The `pre_process_review` function further refines this by extracting titles and adjusting reviews accordingly, ensuring that the context provided by review headers is not lost. Additionally, it preserves the integrity of the review flow, which is vital for understanding the nuances of each review.\n",
    "\n",
    "##### Enhancing NLP with Custom Processing\n",
    "\n",
    "To deepen the analysis, the `preserve_compound_phrases` function is employed. This function utilizes an NLP model to identify and preserve compound nouns and adjectives directly linked to nouns, which are often critical in understanding the specific features discussed. By preserving these compounds, the system maintains the granular detail necessary for precise feature extraction.\n",
    "\n",
    "Following this, the `chunking_post_process` method reassembles the text from tokenized forms back into a structured format conducive to further analysis, ensuring that compound phrases are treated as single entities within the dataset.\n",
    "\n",
    "##### Comprehensive Review Filtering\n",
    "\n",
    "The `pre_processing_controller` function orchestrates the entire preprocessing pipeline. It transforms raw review texts into a tokenized format, applies compound preservation, and executes two levels of filtering: soft filtering (preserving basic structure and some stopwords) and hard filtering (removing all non-alphabetic characters and stopwords). This dual approach allows for flexibility in analysis, from high-level sentiment trends to detailed feature-specific sentiments.\n",
    "\n",
    "##### Sentiment Analysis and Feature Extraction\n",
    "\n",
    "Once preprocessed, the data is ripe for sentiment analysis. Leveraging the structured format of feature tags and sentiment scores, the system can map sentiments directly to product features, allowing for an aggregated sentiment score for each feature. This quantification is pivotal in determining which features are most appreciated or criticized by users.\n",
    "\n",
    "##### Leveraging Data for Business Insights\n",
    "\n",
    "The final step involves synthesizing the analyzed data into actionable business insights. By understanding which features correlate strongly with positive or negative sentiments, companies can prioritize product improvements or highlight successful aspects in marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "97caa3c8-0c02-48d4-abe7-98b05163312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ce2b03ea-3191-44bf-9b2e-2c6d67a03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed6882-2c25-48fb-9545-cf4c3668c231",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "### Data Ingestion and Initial Processing\n",
    "\n",
    "The process starts with the ingestion of data files containing multiple reviews, which can vary in structure. This step is crucial for setting the stage for detailed analysis. The function `read_file` is used to open and read the contents of these files. Reviews are often separated by new lines and may begin with a distinctive line of asterisks, which indicates metadata or headers not part of the actual review content. These non-relevant lines are programmatically identified and omitted from processing to ensure only pertinent text is analyzed further.\n",
    "\n",
    "The reviews within these files are then split using '##' as a delimiter. This segmentation is critical as it separates tags that contain embedded metadata or sentiment scores from the main content. Each segment of the review, along with its associated tags, is stored in a structured format within a pandas DataFrame, which facilitates ease of manipulation and detailed analysis in subsequent steps.\n",
    "\n",
    "### Advanced Text Processing Techniques\n",
    "\n",
    "Once the initial data ingestion is complete, the reviews undergo various text processing steps encapsulated within the `pre_process_review` function. This function is adept at managing different nuances of text, such as concatenating titles where necessary, and ensuring the textual integrity of reviews that span multiple lines is maintained.\n",
    "\n",
    "### Preservation of Semantic Structures\n",
    "\n",
    "To maintain the semantic integrity of phrases within the reviews, the `preserve_compound_phrases` function is employed. This function utilizes spaCy, an advanced NLP library, to parse the text and identify compound nouns and adjectival modifiers. These elements are crucial for understanding the context and sentiment related to specific product features. The identified compounds are then reconstructed with underscores replacing spaces, ensuring they are treated as single tokens in subsequent analysis stages. This preservation prevents the loss of semantic unity and is vital for the accurate interpretation of product features.\n",
    "\n",
    "### Enhancement of Tokenization and Filtering\n",
    "\n",
    "Following the preservation of semantic structures, the `pre_processing_controller` function orchestrates several layers of tokenization and filtering. The text is initially tokenized, which separates each word or phrase for individual analysis. This tokenization process feeds into a dual filtering system:\n",
    "\n",
    "1. A 'Soft Filtered Review' captures tokens that either are part of the identified compound phrases or are standalone alphabetic words. This layer ensures that important phrases and words are retained without too much reduction of the text.\n",
    "2. A 'Filtered Review' applies a more stringent filter that includes the removal of common stop words, focusing only on the more meaningful terms relevant to sentiment analysis. This selective filtering is crucial for reducing noise and enhancing the focus on significant textual elements.\n",
    "\n",
    "These tokens are then reassembled into coherent strings, which form the basis for deeper linguistic analysis, such as lemmatization. The 'Soft Filtered Review' is transformed back into a string format while retaining the structural integrity of compound phrases. Lemmatization is performed on the 'Filtered Review' strings to reduce words to their base or dictionary form, which facilitates a more generalized but robust analysis of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "84c1933d-e77d-4110-afd5-f8aeb435af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present - This is a quirk to parse the data files\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and set content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8551b5d4-90ba-48b6-9987-18d38e018b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    # Process the text with an NLP model\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check for compounds or adjectives linked directly to nouns\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = f\"{token.text}_{token.head.text}\"\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        # Skip nouns that are already part of a compound to prevent duplicates\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        # Include all other tokens normally\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunking_post_process(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Check if the current word is part of a compound phrase\n",
    "        if '_' in words[i]:\n",
    "            # Append all parts of the compound phrase to the list\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Move to the next word after finishing the compound phrase\n",
    "        # Append non-compound words directly to the list\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    # Return the processed words as a single string\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a474f0fe-11c6-4582-916d-5e36195372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings and applies compound phrase preservation\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation and numbers \n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings and retains the compound phrases - soft means not to filter out stop words\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chunking_post_process)\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation, numbers and stop words \n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatise the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fede2a8-b11f-44ac-aa8e-c527b0d0e377",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "##### Part-of-Speech (POS) Noun Tagging\n",
    "\n",
    "The first step in the feature extraction process is the identification of nouns from customer reviews. This is accomplished by the `POS_Noun_Tagging` function, which employs tokenization and POS tagging to sift through the textual data. Nouns are indicative of features, as they often name the components or aspects of a product that customers discuss. By focusing on nouns, the function narrows down the vast amount of information in reviews to specific elements likely relevant to consumer sentiments.\n",
    "\n",
    "##### Concrete Noun Identification\n",
    "\n",
    "Further refining the noun extraction, the `is_concrete_noun` function determines whether a noun describes tangible aspects of a product, such as an \"object\" or \"device.\" This distinction is crucial because tangible feature mentions often directly relate to customer satisfaction and are actionable from a product improvement perspective. Using WordNet synsets and their hypernyms allows for an understanding of the word's concreteness in context, enhancing the relevance of the extracted features.\n",
    "\n",
    "##### Similarity-Based Filtering\n",
    "\n",
    "With the noun list refined to concrete nouns, the `similarity_filter` function applies vector space models like Word2Vec and GloVe to filter out words based on their semantic similarity. This step is vital to cluster similar features together, reducing redundancy and focusing on unique aspects mentioned across reviews. By setting a similarity threshold, the function ensures only significantly similar terms are grouped, maintaining a diverse yet focused feature set.\n",
    "\n",
    "##### Further Similarity Filtering\n",
    "\n",
    "The `further_similar_filter` function adds another layer of filtering by using GloVe's similarity scoring to eliminate closely related nouns, further condensing the feature list. This step is particularly useful in focusing on the most mentioned and hence potentially impactful features in customer reviews.\n",
    "\n",
    "##### Building the Feature Dictionary and DataFrames\n",
    "\n",
    "The `create_product_features_dict` function initializes a structured dictionary to map each product feature against its mentions and sentiment polarity. This dictionary is foundational for aggregating and analyzing sentiment data related to each feature. The `build_featured_df` function then constructs a DataFrame from the reviews, associating each with identified features and preparing the data for sentiment analysis.\n",
    "\n",
    "##### Sentiment Analysis and Count Aggregation\n",
    "\n",
    "The final analytical steps involve `feature_dict_count`, which tallies positive and negative mentions for each feature, providing a quantified sentiment outlook for each aspect of the product. This aggregation is pivotal for identifying strengths and weaknesses in the product as perceived by consumers.\n",
    "\n",
    "##### Efficient Data Management\n",
    "\n",
    "Lastly, the `df_filter` function restructures the DataFrame to optimize it for analysis, ensuring data is clean and well-organized for subsequent processing steps or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f298369f-55f2-44f6-940f-da089722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    nouns = POS_Noun_Tagging(df)\n",
    "    features = similarity_filter(nouns)\n",
    "    feature_df = build_featured_df(df, features)\n",
    "    return feature_df, features\n",
    "\n",
    "\n",
    "def POS_Noun_Tagging(df):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df['Lemmatised_Review_String'].tolist()\n",
    "    features = []\n",
    "    \n",
    "    # Process each review to extract nouns\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)  # Tokenize the text\n",
    "        tagged = pos_tag(tokens)  # POS tagging\n",
    "        # Collect nouns from tags\n",
    "        features.extend(word.lower() for word, tag in tagged if tag.startswith('NN'))\n",
    "    \n",
    "    # Count and retrieve the 15 most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(10)\n",
    "    \n",
    "    return common_features\n",
    "\n",
    "\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    # Define indicators for concrete nouns\n",
    "    concrete_indicators = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    # Retrieve synsets for the word as a noun\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    \n",
    "    # Check categories for each synset to determine if it's a concrete noun\n",
    "    for synset in synsets:\n",
    "        for hypernym in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_indicators.intersection(hypernym.lemma_names()):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6be50234-31a2-45fa-ae2b-d4a15be9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    # Set a threshold for filtering similar words\n",
    "    similarity_threshold = 0.25\n",
    "    \n",
    "    # Create a list of words from the tuples\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    noun_list = []\n",
    "    # Iterate over words to compute similarities\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Calculate similarity using GloVe\n",
    "            glove_similarity = glove_model.similarity(words[0], word)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            \n",
    "            # Append word to list if it meets the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                noun_list.append(word)\n",
    "        except KeyError:\n",
    "            # Skip the word if it's not found in the model's vocabulary\n",
    "            continue\n",
    "\n",
    "    items_to_remove = []\n",
    "    \n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(noun_list[0], topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_nouns = [item for item in noun_list if item not in items_to_remove]\n",
    "    \n",
    "    return filtered_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69ee67fd-4927-4677-938a-2b16f9e74b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_featured_df(df, features):\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_list = features[1:]\n",
    "    \n",
    "    featured_items_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Convert the review Series to a DataFrame with one row\n",
    "            review_df = review.to_frame().transpose()\n",
    "            \n",
    "            # New: Append the found features as a string (or you can keep it as list)\n",
    "            review_df['Featured_Items'] = [', '.join(featured_items)]  # As a single string of items\n",
    "    \n",
    "            review_df['Main_Index'] = idx\n",
    "            \n",
    "            # Append this review to the feature_df\n",
    "            feature_df = pd.concat([review_df, feature_df], axis=0)\n",
    "            \n",
    "            # Additionally, append the found features to the featured_items_list\n",
    "            featured_items_list.append(featured_items)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def create_feature_table(df, features):\n",
    "\n",
    "    df = df[['Main_Index', 'Featured_Items', 'Sentiment', 'Tags', 'Review', 'Tokenised_Review', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String', 'Filtered_Review', 'Filtered_Review_String', 'Lemmatised_Review_String', 'Lemmatised_Tokenised_Filtered_Review']]\n",
    "    df = df.set_index('Main_Index', drop=True) \n",
    "    df.index.name = 'Main Index'\n",
    "    df = df.iloc[::-1]\n",
    "\n",
    "     # Extract the product title\n",
    "    product_title = features[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = features[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sentiment_class_list = df[['Featured_Items', 'Sentiment']]\n",
    "    \n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "    \n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        elif sentiment == -1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "        else:\n",
    "            # print('More than two features, cannot classify')\n",
    "            pass\n",
    "\n",
    "    return product_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153d7a-bb12-4cb4-a3ce-d8cd0780cbfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "##### Sentiment Controller Function\n",
    "\n",
    "The core of the sentiment analysis system is the `sentiment_controller` function, which is responsible for aggregating and assigning sentiment scores to each review in the dataset. This function works by iterating through each review entry and identifying the associated product features. It is designed to handle reviews with a single feature distinctly, ensuring that the sentiment analysis is as precise and relevant to specific product attributes as possible.\n",
    "\n",
    "For reviews associated with exactly one product feature, the function utilizes the `get_phrase_sentiment` function to calculate a comprehensive sentiment score based on the textual content of the review. This distinction is crucial as it allows for a focused analysis, linking sentiments directly to individual features rather than generalizing across multiple attributes.\n",
    "\n",
    "##### Phrase and Word-Level Sentiment Calculation\n",
    "\n",
    "The `get_phrase_sentiment` function represents a layered approach to sentiment scoring. Initially, it attempts to understand the sentiment of the entire phrase as a compound unit, preserving the contextual integrity of the user’s opinion. If specific sentiment scores are available for the compound phrase, these are used directly. This approach is beneficial for capturing the sentiment of phrases where the combined meaning might differ significantly from the sum of individual word sentiments.\n",
    "\n",
    "If a direct compound sentiment score is unavailable, the function breaks down the phrase into individual words and computes the average sentiment score across all words. This breakdown is critical for capturing the nuances of language where compound scoring is not feasible, ensuring no sentiment information is lost.\n",
    "\n",
    "##### Individual Word Sentiment Scoring\n",
    "\n",
    "At the word level, the `get_word_sentiment` function retrieves sentiment scores using SentiWordNet, a well-regarded lexicon in sentiment analysis research. By analyzing the positive and negative sentiment scores associated with each word’s synset, the function provides a balanced view of the word’s emotional impact within the context of the review.\n",
    "\n",
    "This granular approach to sentiment analysis allows for a nuanced understanding of how specific words contribute to the overall sentiment of a phrase or sentence, enhancing the system's ability to detect and interpret the subtlest emotional undertones in consumer feedback.\n",
    "\n",
    "##### Integration into Review Analysis\n",
    "\n",
    "Once sentiments are calculated for each review, the `sentiment_controller` function appends these scores to the main DataFrame as a new column, facilitating further analysis and visualization of sentiment trends across different product features. This integration is pivotal in enabling end-to-end analysis, from raw review data to actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fcbb4e4d-5031-45c2-adfc-39b39631bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(df):\n",
    "    # Initialize a list to hold sentiment values for each review\n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, entry in df.iterrows():\n",
    "        review = entry['Soft_Filtered_Review_String']\n",
    "        features = [feature.strip() for feature in entry['Featured_Items'].split(',')]\n",
    "        \n",
    "        # Process sentiment per feature \n",
    "        if len(features) > 1:\n",
    "            sentiments = []\n",
    "            for feature in features:\n",
    "                pos_score, neg_score = get_phrase_sentiment(review)\n",
    "                sentiment = 1 if pos_score > neg_score else -1\n",
    "                sentiments.append(sentiment)\n",
    "            sentiment_list.append(sentiments)\n",
    "        else:\n",
    "            pos_score, neg_score = get_phrase_sentiment(review)\n",
    "            sentiment = 1 if pos_score > neg_score else -1\n",
    "            sentiment_list.append(sentiment)\n",
    "\n",
    "    # Add the sentiment list to the DataFrame as a new column\n",
    "    df['Sentiment'] = sentiment_list\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_phrase_sentiment(phrase):\n",
    "    # First try to get sentiment score for the whole phrase as a compound\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # If no score, break down the phrase and calculate average sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    \n",
    "    # Average the sentiment scores\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    # Retrieve sentiment scores from SentiWordNet\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3b8ab8c2-c93b-44ff-ad52-0ad986066159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = feature_df[feature_df.index == 214]\n",
    "\n",
    "# sample_features = sample['Featured_Items'].iloc[0].split(',')\n",
    "# sample_review = sample['Review'].iloc[0]\n",
    "\n",
    "# # Initialize VADER sentiment analyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def analyze_feature_sentiment(sentence, features):\n",
    "#     # Dictionary to store sentiment results for each feature\n",
    "#     sentiment_results = {}\n",
    "\n",
    "#     features = [feature.strip() for feature in features]  \n",
    "    \n",
    "#     # Analyze sentiment for each feature\n",
    "#     for feature in features:\n",
    "#         # Extract context around the feature if needed (optional improvement)\n",
    "#         start_index = sentence.lower().find(feature.lower())\n",
    "#         if start_index != -1:\n",
    "#             # Extract a sub-sentence for context-based sentiment analysis\n",
    "#             sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "#             # Get sentiment using TextBlob\n",
    "#             tb_sentiment = TextBlob(sub_sentence).sentiment.polarity\n",
    "#             # Get sentiment using VADER\n",
    "#             vader_sentiment = analyzer.polarity_scores(sub_sentence)['compound']\n",
    "\n",
    "#             # Store results\n",
    "#             sentiment_results[feature] = {\n",
    "#                 'TextBlob Sentiment': 'Positive' if tb_sentiment > 0 else 'Negative' if tb_sentiment < 0 else 'Neutral',\n",
    "#                 'VADER Sentiment': 'Positive' if vader_sentiment > 0.05 else 'Negative' if vader_sentiment < -0.05 else 'Neutral'\n",
    "#             }\n",
    "#         else:\n",
    "#             sentiment_results[feature] = {\n",
    "#                 'TextBlob Sentiment': 'Not Found',\n",
    "#                 'VADER Sentiment': 'Not Found'\n",
    "#             }\n",
    "\n",
    "#     return sentiment_results\n",
    "\n",
    "# results = analyze_feature_sentiment(sample_review, sample_features)\n",
    "# # display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7e787bd4-4691-4395-9a14-29655945a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "class LSTM_Classifier():\n",
    "\n",
    "    model = None\n",
    "    history = None\n",
    "    file_id = None\n",
    "    conf_matrix = None\n",
    "\n",
    "    def __init__(self, data, embedding_output_dim, lstm_unit, dropout_rate, reg_strength, epochs, batch_size):\n",
    "\n",
    "        self.data = data\n",
    "        self.embedding_output_dim = embedding_output_dim\n",
    "        self.lstm_unit = lstm_unit\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def tokenisation(self):\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=10000)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(self.data[0])  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(self.data[0])  # Transforms each text in training_texts to a sequence of integers\n",
    "        self.data[0] = pad_sequences(train_sequences, maxlen=100)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        self.data[1] = np.array(self.data[1])  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(self.data[2]) \n",
    "        self.data[2] = pad_sequences(test_sequences, maxlen=100) \n",
    "        self.data[3] = np.array(self.data[3])  \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - embedding_dim: Dimension of the embedding layer\n",
    "            - lstm_unit: Number of units in the LSTM layer\n",
    "            - dropout_rate: Dropout rate for regularisation\n",
    "            - reg_strength: Strength of L2 regularisation\n",
    "        \"\"\"\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data = pad_sequences_maxlen\n",
    "            Input(shape=(100,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=10000, output_dim=self.embedding_output_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(self.lstm_unit, dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation='sigmoid', kernel_regularizer=l2(self.reg_strength))\n",
    "        ])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        # Compile models with selected optimisation parameters\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting added early stopping with a patience of 3\n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=0.2, callbacks=EarlyStopping(patience=3))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "\n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_probabilities = self.model.predict(self.data[2])\n",
    "        test_predictions = (test_probabilities > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        self.conf_matrix = confusion_matrix(self.data[3], test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(self.conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(self.data[3], test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        self.visualise_accuracy()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def visualise_accuracy(self):\n",
    "\n",
    "        training_accuracy = self.history.history['accuracy']\n",
    "        validation_accuracy = self.history.history['val_accuracy']\n",
    "        training_loss = self.history.history['loss']\n",
    "        validation_loss = self.history.history['val_loss']\n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "        with plt.style.context('dark_background'):\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            # Plotting training and validation accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plotting training and validation loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, training_loss, label='Training Loss', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_loss, label='Validation Loss', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, file_id = None):\n",
    "        self.file_id = file_id\n",
    "        self.tokenisation()\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "        self.evaluate_model()\n",
    "\n",
    "\n",
    "\n",
    "# lstm_classifier = LSTM_Classifier(data, embedding_output_dim = 128, lstm_unit = 64, dropout_rate = 0.05, reg_strength = 0.0, epochs = 10, batch_size = 32)\n",
    "# lstm_classifier.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2eef0-37b9-44f7-b01b-1ac9ebac146c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e03af2-c3f8-4083-a39f-0f0745004b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_miner_controller(file):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    df, features = feature_extraction(df)\n",
    "    df = sentiment_controller(df)\n",
    "    feature_table = create_feature_table(df, features)\n",
    "    \n",
    "    return feature_table\n",
    "\n",
    "\n",
    "feature_dict = opinion_miner_controller(files[1])\n",
    "display(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64bf8-5415-4f4a-bb7a-b67f361545a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
