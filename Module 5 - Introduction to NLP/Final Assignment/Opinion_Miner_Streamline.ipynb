{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61263af9-60d2-4c0d-bbd6-fa7c022e7af0",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "#### Overview of the Data\n",
    "\n",
    "The dataset consists of product reviews from Amazon, segmented into features with annotated sentiment scores. Reviews include multiple aspects such as ease of use, picture quality, and additional functionalities, each tagged with a corresponding sentiment score. These annotations provide a rich basis for sentiment analysis but require precise parsing to effectively utilize the structured format in which they are presented.\n",
    "\n",
    "#### System-Level Outline\n",
    "\n",
    "##### Data Parsing and Pre-processing\n",
    "\n",
    "Using a custom `read_file` function, the system initially parses the structured reviews from text files. This function handles the dataset's specific format, which includes initial metadata and reviewer comments split by a unique delimiter ('##'). This is crucial for separating feature tags from review content, facilitating subsequent analysis.\n",
    "\n",
    "The `pre_process_review` function further refines this by extracting titles and adjusting reviews accordingly, ensuring that the context provided by review headers is not lost. Additionally, it preserves the integrity of the review flow, which is vital for understanding the nuances of each review.\n",
    "\n",
    "##### Enhancing NLP with Custom Processing\n",
    "\n",
    "To deepen the analysis, the `preserve_compound_phrases` function is employed. This function utilizes an NLP model to identify and preserve compound nouns and adjectives directly linked to nouns, which are often critical in understanding the specific features discussed. By preserving these compounds, the system maintains the granular detail necessary for precise feature extraction.\n",
    "\n",
    "Following this, the `chunking_post_process` method reassembles the text from tokenized forms back into a structured format conducive to further analysis, ensuring that compound phrases are treated as single entities within the dataset.\n",
    "\n",
    "##### Comprehensive Review Filtering\n",
    "\n",
    "The `pre_processing_controller` function orchestrates the entire preprocessing pipeline. It transforms raw review texts into a tokenized format, applies compound preservation, and executes two levels of filtering: soft filtering (preserving basic structure and some stopwords) and hard filtering (removing all non-alphabetic characters and stopwords). This dual approach allows for flexibility in analysis, from high-level sentiment trends to detailed feature-specific sentiments.\n",
    "\n",
    "##### Sentiment Analysis and Feature Extraction\n",
    "\n",
    "Once preprocessed, the data is ripe for sentiment analysis. Leveraging the structured format of feature tags and sentiment scores, the system can map sentiments directly to product features, allowing for an aggregated sentiment score for each feature. This quantification is pivotal in determining which features are most appreciated or criticized by users.\n",
    "\n",
    "##### Leveraging Data for Business Insights\n",
    "\n",
    "The final step involves synthesizing the analyzed data into actionable business insights. By understanding which features correlate strongly with positive or negative sentiments, companies can prioritize product improvements or highlight successful aspects in marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97caa3c8-0c02-48d4-abe7-98b05163312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") \n",
    "sentiment_intensity_analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2b03ea-3191-44bf-9b2e-2c6d67a03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed6882-2c25-48fb-9545-cf4c3668c231",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "### Data Ingestion and Initial Processing\n",
    "\n",
    "The process starts with the ingestion of data files containing multiple reviews, which can vary in structure. This step is crucial for setting the stage for detailed analysis. The function `read_file` is used to open and read the contents of these files. Reviews are often separated by new lines and may begin with a distinctive line of asterisks, which indicates metadata or headers not part of the actual review content. These non-relevant lines are programmatically identified and omitted from processing to ensure only pertinent text is analyzed further.\n",
    "\n",
    "The reviews within these files are then split using '##' as a delimiter. This segmentation is critical as it separates tags that contain embedded metadata or sentiment scores from the main content. Each segment of the review, along with its associated tags, is stored in a structured format within a pandas DataFrame, which facilitates ease of manipulation and detailed analysis in subsequent steps.\n",
    "\n",
    "### Advanced Text Processing Techniques\n",
    "\n",
    "Once the initial data ingestion is complete, the reviews undergo various text processing steps encapsulated within the `pre_process_review` function. This function is adept at managing different nuances of text, such as concatenating titles where necessary, and ensuring the textual integrity of reviews that span multiple lines is maintained.\n",
    "\n",
    "### Preservation of Semantic Structures\n",
    "\n",
    "To maintain the semantic integrity of phrases within the reviews, the `preserve_compound_phrases` function is employed. This function utilizes spaCy, an advanced NLP library, to parse the text and identify compound nouns and adjectival modifiers. These elements are crucial for understanding the context and sentiment related to specific product features. The identified compounds are then reconstructed with underscores replacing spaces, ensuring they are treated as single tokens in subsequent analysis stages. This preservation prevents the loss of semantic unity and is vital for the accurate interpretation of product features.\n",
    "\n",
    "### Enhancement of Tokenization and Filtering\n",
    "\n",
    "Following the preservation of semantic structures, the `pre_processing_controller` function orchestrates several layers of tokenization and filtering. The text is initially tokenized, which separates each word or phrase for individual analysis. This tokenization process feeds into a dual filtering system:\n",
    "\n",
    "1. A 'Soft Filtered Review' captures tokens that either are part of the identified compound phrases or are standalone alphabetic words. This layer ensures that important phrases and words are retained without too much reduction of the text.\n",
    "2. A 'Filtered Review' applies a more stringent filter that includes the removal of common stop words, focusing only on the more meaningful terms relevant to sentiment analysis. This selective filtering is crucial for reducing noise and enhancing the focus on significant textual elements.\n",
    "\n",
    "These tokens are then reassembled into coherent strings, which form the basis for deeper linguistic analysis, such as lemmatization. The 'Soft Filtered Review' is transformed back into a string format while retaining the structural integrity of compound phrases. Lemmatization is performed on the 'Filtered Review' strings to reduce words to their base or dictionary form, which facilitates a more generalized but robust analysis of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "84c1933d-e77d-4110-afd5-f8aeb435af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present - This is a quirk to parse the data files\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and set content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8551b5d4-90ba-48b6-9987-18d38e018b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    # Process the text with an NLP model\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check for compounds or adjectives linked directly to nouns\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = f\"{token.text}_{token.head.text}\"\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        # Skip nouns that are already part of a compound to prevent duplicates\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        # Include all other tokens normally\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunking_post_process(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Check if the current word is part of a compound phrase\n",
    "        if '_' in words[i]:\n",
    "            # Append all parts of the compound phrase to the list\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Move to the next word after finishing the compound phrase\n",
    "        # Append non-compound words directly to the list\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    # Return the processed words as a single string\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a474f0fe-11c6-4582-916d-5e36195372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings and applies compound phrase preservation\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation and numbers \n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings and retains the compound phrases - soft means not to filter out stop words\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chunking_post_process)\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation, numbers and stop words \n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatise the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fede2a8-b11f-44ac-aa8e-c527b0d0e377",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "##### Part-of-Speech (POS) Noun Tagging\n",
    "\n",
    "The first step in the feature extraction process is the identification of nouns from customer reviews. This is accomplished by the `POS_Noun_Tagging` function, which employs tokenization and POS tagging to sift through the textual data. Nouns are indicative of features, as they often name the components or aspects of a product that customers discuss. By focusing on nouns, the function narrows down the vast amount of information in reviews to specific elements likely relevant to consumer sentiments.\n",
    "\n",
    "##### Concrete Noun Identification\n",
    "\n",
    "Further refining the noun extraction, the `is_concrete_noun` function determines whether a noun describes tangible aspects of a product, such as an \"object\" or \"device.\" This distinction is crucial because tangible feature mentions often directly relate to customer satisfaction and are actionable from a product improvement perspective. Using WordNet synsets and their hypernyms allows for an understanding of the word's concreteness in context, enhancing the relevance of the extracted features.\n",
    "\n",
    "##### Similarity-Based Filtering\n",
    "\n",
    "With the noun list refined to concrete nouns, the `similarity_filter` function applies vector space models like Word2Vec and GloVe to filter out words based on their semantic similarity. This step is vital to cluster similar features together, reducing redundancy and focusing on unique aspects mentioned across reviews. By setting a similarity threshold, the function ensures only significantly similar terms are grouped, maintaining a diverse yet focused feature set.\n",
    "\n",
    "##### Further Similarity Filtering\n",
    "\n",
    "The `further_similar_filter` function adds another layer of filtering by using GloVe's similarity scoring to eliminate closely related nouns, further condensing the feature list. This step is particularly useful in focusing on the most mentioned and hence potentially impactful features in customer reviews.\n",
    "\n",
    "##### Building the Feature Dictionary and DataFrames\n",
    "\n",
    "The `create_product_features_dict` function initializes a structured dictionary to map each product feature against its mentions and sentiment polarity. This dictionary is foundational for aggregating and analyzing sentiment data related to each feature. The `build_featured_df` function then constructs a DataFrame from the reviews, associating each with identified features and preparing the data for sentiment analysis.\n",
    "\n",
    "##### Sentiment Analysis and Count Aggregation\n",
    "\n",
    "The final analytical steps involve `feature_dict_count`, which tallies positive and negative mentions for each feature, providing a quantified sentiment outlook for each aspect of the product. This aggregation is pivotal for identifying strengths and weaknesses in the product as perceived by consumers.\n",
    "\n",
    "##### Efficient Data Management\n",
    "\n",
    "Lastly, the `df_filter` function restructures the DataFrame to optimize it for analysis, ensuring data is clean and well-organized for subsequent processing steps or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "f298369f-55f2-44f6-940f-da089722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    nouns = POS_Noun_Tagging(df)\n",
    "    features = similarity_filter(nouns)\n",
    "    df = add_features_to_df(df, features)\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def POS_Noun_Tagging(df):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = df['Lemmatised_Review_String'].tolist()\n",
    "    features = []\n",
    "    \n",
    "    # Process each review to extract nouns\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)  # Tokenize the text\n",
    "        tagged = pos_tag(tokens)  # POS tagging\n",
    "        # Collect nouns from tags\n",
    "        features.extend(word.lower() for word, tag in tagged if tag.startswith('NN'))\n",
    "    \n",
    "    # Count and retrieve the 15 most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(20)\n",
    "    \n",
    "    return common_features\n",
    "\n",
    "\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    # Define indicators for concrete nouns\n",
    "    concrete_indicators = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    # Retrieve synsets for the word as a noun\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    \n",
    "    # Check categories for each synset to determine if it's a concrete noun\n",
    "    for synset in synsets:\n",
    "        for hypernym in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_indicators.intersection(hypernym.lemma_names()):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6be50234-31a2-45fa-ae2b-d4a15be9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    # Set a threshold for filtering similar words\n",
    "    similarity_threshold = 0.25\n",
    "    \n",
    "    # Create a list of words from the tuples\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    noun_list = []\n",
    "    # Iterate over words to compute similarities\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Calculate similarity using GloVe\n",
    "            glove_similarity = glove_model.similarity(words[0], word)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            \n",
    "            # Append word to list if it meets the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                noun_list.append(word)\n",
    "        except KeyError:\n",
    "            # Skip the word if it's not found in the model's vocabulary\n",
    "            continue\n",
    "\n",
    "    items_to_remove = []\n",
    "    \n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(noun_list[0], topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_nouns = [item for item in noun_list if item not in items_to_remove]\n",
    "    \n",
    "    return filtered_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "69ee67fd-4927-4677-938a-2b16f9e74b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_df(df, features):\n",
    "    \n",
    "    feature_list = features[1:]\n",
    "    featured_items_list = []\n",
    "    index_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Append the found features as a string \n",
    "            featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            item_list = [f\"{item.strip()}\" for item in featured_items]\n",
    "            featured_items_string = ', '.join(item_list)\n",
    "            featured_items_list.append(featured_items_string) \n",
    "            index_list.append(idx)\n",
    "        else:\n",
    "            featured_items_list.append('')  \n",
    "            index_list.append(idx)\n",
    "\n",
    "    df['Main_Index'] = index_list\n",
    "    df['Featured_Items'] = featured_items_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_data_df(df):\n",
    "    \n",
    "    my_tags = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        tags = row['Tags']\n",
    "        features = row['Featured_Items'].split(',') if row['Featured_Items'] else []\n",
    "        sentiments = row['Sentiment']\n",
    "\n",
    "        # Handle case when features list is empty\n",
    "        if not features or all(feature.strip() == '' for feature in features):\n",
    "            my_tags.append([])\n",
    "        elif len(features) == 1:\n",
    "            my_tags.append([f'{features[0].strip()}[{sentiments}]'])\n",
    "        else:\n",
    "            my_tags_list = []\n",
    "            for feature in features:\n",
    "                if feature.strip():  # Ensure that feature is not empty\n",
    "                    # Get the sentiment for the corresponding feature if possible\n",
    "                    index = features.index(feature)\n",
    "                    sentiment_value = sentiments[index] if len(sentiments) > index else 0\n",
    "                    tag_string = f'{feature.strip()}[{sentiment_value}]'\n",
    "                    my_tags_list.append(tag_string)\n",
    "            my_tags.append(my_tags_list)\n",
    "\n",
    "    df['My_Sentiment_Tags'] = my_tags\n",
    "    df = df[['Main_Index', 'My_Sentiment_Tags', 'Featured_Items', 'Sentiment', 'Tags', 'Review', 'Tokenised_Review', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String', 'Filtered_Review', 'Filtered_Review_String', 'Lemmatised_Review_String', 'Lemmatised_Tokenised_Filtered_Review']]\n",
    "    df = df.set_index('Main_Index', drop=True) \n",
    "    df.index.name = 'Main Index'\n",
    "    df = df.iloc[::-1]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_feature_table(df, features):\n",
    "\n",
    "    df = process_data_df(df)\n",
    "\n",
    "    # Extract the product title\n",
    "    product_title = features[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = features[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sentiment_class_list = df[['Featured_Items', 'Sentiment']]\n",
    "\n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "\n",
    "        # Check if sentiment is an integer or a list and handle accordingly\n",
    "        if isinstance(sentiment, int):\n",
    "            # Handle as an integer (assuming sentiment scores are integers)\n",
    "            product_dict = add_to_table(product_dict, feature, sentiment)\n",
    "        elif isinstance(sentiment, list):\n",
    "            # Handle as a list (assuming you need to process multiple sentiment scores)\n",
    "            for s in sentiment:\n",
    "                product_dict = add_to_table(product_dict, feature, s)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return product_dict, df\n",
    "\n",
    "\n",
    "\n",
    "def add_to_table(product_dict, feature, sentiment):\n",
    "\n",
    "    try:\n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        else:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return product_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153d7a-bb12-4cb4-a3ce-d8cd0780cbfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "##### Sentiment Controller Function\n",
    "\n",
    "The core of the sentiment analysis system is the `sentiment_controller` function, which is responsible for aggregating and assigning sentiment scores to each review in the dataset. This function works by iterating through each review entry and identifying the associated product features. It is designed to handle reviews with a single feature distinctly, ensuring that the sentiment analysis is as precise and relevant to specific product attributes as possible.\n",
    "\n",
    "For reviews associated with exactly one product feature, the function utilizes the `get_phrase_sentiment` function to calculate a comprehensive sentiment score based on the textual content of the review. This distinction is crucial as it allows for a focused analysis, linking sentiments directly to individual features rather than generalizing across multiple attributes.\n",
    "\n",
    "##### Phrase and Word-Level Sentiment Calculation\n",
    "\n",
    "The `get_phrase_sentiment` function represents a layered approach to sentiment scoring. Initially, it attempts to understand the sentiment of the entire phrase as a compound unit, preserving the contextual integrity of the user’s opinion. If specific sentiment scores are available for the compound phrase, these are used directly. This approach is beneficial for capturing the sentiment of phrases where the combined meaning might differ significantly from the sum of individual word sentiments.\n",
    "\n",
    "If a direct compound sentiment score is unavailable, the function breaks down the phrase into individual words and computes the average sentiment score across all words. This breakdown is critical for capturing the nuances of language where compound scoring is not feasible, ensuring no sentiment information is lost.\n",
    "\n",
    "##### Individual Word Sentiment Scoring\n",
    "\n",
    "At the word level, the `get_word_sentiment` function retrieves sentiment scores using SentiWordNet, a well-regarded lexicon in sentiment analysis research. By analyzing the positive and negative sentiment scores associated with each word’s synset, the function provides a balanced view of the word’s emotional impact within the context of the review.\n",
    "\n",
    "This granular approach to sentiment analysis allows for a nuanced understanding of how specific words contribute to the overall sentiment of a phrase or sentence, enhancing the system's ability to detect and interpret the subtlest emotional undertones in consumer feedback.\n",
    "\n",
    "##### Integration into Review Analysis\n",
    "\n",
    "Once sentiments are calculated for each review, the `sentiment_controller` function appends these scores to the main DataFrame as a new column, facilitating further analysis and visualization of sentiment trends across different product features. This integration is pivotal in enabling end-to-end analysis, from raw review data to actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "fcbb4e4d-5031-45c2-adfc-39b39631bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(df, classifer):\n",
    "    # Initialize a list to hold sentiment values for each review\n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        review = row['Soft_Filtered_Review_String']\n",
    "        features = [feature.strip() for feature in row['Featured_Items'].split(',')]\n",
    "        \n",
    "        # Process sentiment per feature \n",
    "        if len(features) > 1:\n",
    "            sentiments = []\n",
    "            for feature in features:\n",
    "                if classifer == 'senti':\n",
    "                    pos_score, neg_score = senti_classifier(review)\n",
    "                    sentiment = 1 if pos_score > neg_score else -1\n",
    "                elif classifer == 'vader_blob':\n",
    "                    sentiment = vader_textblob_classifier(review, features)\n",
    "                else:\n",
    "                    pass\n",
    "                sentiments.append(sentiment)\n",
    "            sentiment_list.append(sentiments)\n",
    "        else:\n",
    "            if classifer == 'senti':\n",
    "                pos_score, neg_score = senti_classifier(review)\n",
    "                sentiment = 1 if pos_score > neg_score else -1\n",
    "            elif classifer == 'vader_blob':\n",
    "                sentiment = vader_textblob_classifier(review, features)\n",
    "            else:\n",
    "                pass\n",
    "            sentiment_list.append(sentiment)\n",
    "            \n",
    "    # Add the sentiment list to the DataFrame as a new column\n",
    "    df['Sentiment'] = sentiment_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def senti_classifier(phrase):\n",
    "    # First try to get sentiment score for the whole phrase as a compound\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # If no score, break down the phrase and calculate average sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    \n",
    "    # Average the sentiment scores\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    # Retrieve sentiment scores from SentiWordNet\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vader_textblob_classifier(sentence, features):\n",
    "    # Dictionary to store sentiment results for each feature\n",
    "    sentiment_results = {}\n",
    "\n",
    "    features = [feature.strip() for feature in features]  \n",
    "    \n",
    "    # Analyze sentiment for each feature\n",
    "    for feature in features:\n",
    "        # Extract context around the feature if needed (optional improvement)\n",
    "        start_index = sentence.lower().find(feature.lower())\n",
    "        if start_index != -1:\n",
    "            # Extract a sub-sentence for context-based sentiment analysis\n",
    "            sub_sentence = sentence[max(start_index - 30, 0):min(start_index + 30 + len(feature), len(sentence))]\n",
    "\n",
    "            # Get sentiment using TextBlob\n",
    "            tb_sentiment = TextBlob(sub_sentence).sentiment.polarity\n",
    "            # Get sentiment using VADER\n",
    "            vader_sentiment = sentiment_intensity_analyser.polarity_scores(sub_sentence)['compound']\n",
    "\n",
    "  \n",
    "            Textblob = 1 if tb_sentiment > 0 else -1 if tb_sentiment < 0 else 0\n",
    "            Vader = 1 if vader_sentiment > 0.05 else -1 if vader_sentiment < -0.05 else 0\n",
    "            total_sentiment = Textblob + Vader\n",
    "            \n",
    "        else:\n",
    "           total_sentiment = 0\n",
    "            \n",
    "    return total_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2eef0-37b9-44f7-b01b-1ac9ebac146c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c2e03af2-c3f8-4083-a39f-0f0745004b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_miner_controller(file, sentiment_classifier):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    df, features = feature_extraction(df)\n",
    "    df = sentiment_controller(df, sentiment_classifier)\n",
    "    feature_table, df = create_feature_table(df, features)\n",
    "    \n",
    "    return feature_table, df\n",
    "\n",
    "\n",
    "feature_table_1, df = opinion_miner_controller(files[1], sentiment_classifier = 'senti')\n",
    "feature_table_2, df = opinion_miner_controller(files[1], sentiment_classifier = 'vader_blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "b1d93c8a-81e9-48f7-9ca6-b03eec0535a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera': {'use': {'positive': 21, 'negative': 11},\n",
       "  'picture': {'positive': 2, 'negative': 8},\n",
       "  'canon': {'positive': 17, 'negative': 16},\n",
       "  'time': {'positive': 10, 'negative': 14},\n",
       "  'shoot': {'positive': 4, 'negative': 2},\n",
       "  'feature': {'positive': 0, 'negative': 1},\n",
       "  'quality': {'positive': 13, 'negative': 2},\n",
       "  'image': {'positive': 3, 'negative': 2},\n",
       "  'point': {'positive': 1, 'negative': 1},\n",
       "  'look': {'positive': 1, 'negative': 1},\n",
       "  'photo': {'positive': 0, 'negative': 0},\n",
       "  'control': {'positive': 2, 'negative': 3},\n",
       "  'work': {'positive': 3, 'negative': 0},\n",
       "  'thing': {'positive': 3, 'negative': 4},\n",
       "  'battery': {'positive': 2, 'negative': 3},\n",
       "  'zoom': {'positive': 2, 'negative': 3}}}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_double_sentiment_comparison(dict1, dict2):\n",
    "    # Identify the product types dynamically\n",
    "    product_types = dict1.keys()\n",
    "\n",
    "    for product in product_types:\n",
    "        features = list(dict1[product].keys())\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        positives1 = [dict1[product][feature]['positive'] for feature in features]\n",
    "        negatives1 = [dict1[product][feature]['negative'] for feature in features]\n",
    "        positives2 = [dict2[product][feature]['positive'] for feature in features]\n",
    "        negatives2 = [dict2[product][feature]['negative'] for feature in features]\n",
    "\n",
    "        # Set up the bar chart for each product type\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        index = np.arange(len(features))\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Create bars\n",
    "        bars1_pos = ax.bar(index - bar_width/2, positives1, bar_width, label=f'Positive {product} Review 1', color='lightgreen')\n",
    "        bars1_neg = ax.bar(index - bar_width/2, negatives1, bar_width, label=f'Negative {product} Review 1', color='lightcoral', bottom=positives1)\n",
    "        \n",
    "        bars2_pos = ax.bar(index + bar_width/2, positives2, bar_width, label=f'Positive {product} Review 2', color='green')\n",
    "        bars2_neg = ax.bar(index + bar_width/2, negatives2, bar_width, label=f'Negative {product} Review 2', color='red', bottom=positives2)\n",
    "\n",
    "        # Adding labels, title and axes ticks\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Number of Mentions')\n",
    "        ax.set_title(f'Comparison of Sentiments for {product.capitalize()} Features')\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(features)\n",
    "        ax.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Call the function to plot the data for all product types\n",
    "# plot_double_sentiment_comparison(feature_table_1, feature_table_2)\n",
    "feature_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "b8619523-c2e5-4083-855d-7aec6d6c087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tag_count: 239  my_tag_count: 204\n"
     ]
    }
   ],
   "source": [
    "eval_df = df[['Tags', 'My_Sentiment_Tags', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String']]\n",
    "\n",
    "data_tag_count = 0\n",
    "my_tag_count = 0\n",
    "\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        if row['Tags'][0] != '':\n",
    "            data_tag_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if row['My_Sentiment_Tags'][0] != '':\n",
    "            my_tag_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "print('data_tag_count:', data_tag_count, ' my_tag_count:', my_tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "5a8faf86-e07c-44fe-a960-db35438af2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the attotated tags from the data so you can compare, listing positive and neagtive like my feature dict \n",
    "# Perform a word similarity comarison on the annotated dict and my feature dict\n",
    "# Filter for features that match between annotated dict and my feature dict \n",
    "# Apply simialrity comparison between my features and annotated features to widen comparison to annotated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4bc5cd27-ec0a-467f-8fa7-9f62a8e3bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract tags and convert them into a dictionary format\n",
    "def extract_tags(df):\n",
    "    tag_dict = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        for tag in row['Tags']:\n",
    "            # Assume tag format is \"feature[score]\"\n",
    "            match = re.match(r\"(\\w+)\\[([-+]?\\d+)\\]\", tag)\n",
    "            if match:\n",
    "                feature, score = match.groups()\n",
    "                score = int(score)\n",
    "                if feature not in tag_dict:\n",
    "                    tag_dict[feature] = {'positive': 0, 'negative': 0}\n",
    "                if score > 0:\n",
    "                    tag_dict[feature]['positive'] += 1\n",
    "                elif score < 0:\n",
    "                    tag_dict[feature]['negative'] += 1\n",
    "    return dict(tag_dict)\n",
    "\n",
    "# Using the DataFrame 'eval_df' assumed to be already defined\n",
    "annotated_tags_dict = extract_tags(eval_df)\n",
    "# annotated_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "86fab4b4-350c-4371-b406-f35858cd2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/pb8nhwfs3d357rgvk52mq_sc0000gn/T/ipykernel_55916/202858571.py:9: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if my_feat_nlp.similarity(anno_feat_nlp) > threshold:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'use': 'use',\n",
       " 'picture': 'picture',\n",
       " 'canon': 'canon',\n",
       " 'feature': 'feature',\n",
       " 'quality': 'quality',\n",
       " 'image': 'image',\n",
       " 'look': 'look',\n",
       " 'photo': 'photo',\n",
       " 'control': 'control',\n",
       " 'battery': 'battery',\n",
       " 'zoom': 'zoom'}"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find matching features based on similarity threshold\n",
    "def find_similar_features(my_features, annotated_features, threshold=0.8):\n",
    "    similar_features = {}\n",
    "    my_features_nlp = {feature: nlp(feature) for feature in my_features}\n",
    "    annotated_features_nlp = {feature: nlp(feature) for feature in annotated_features}\n",
    "\n",
    "    for my_feat, my_feat_nlp in my_features_nlp.items():\n",
    "        for anno_feat, anno_feat_nlp in annotated_features_nlp.items():\n",
    "            if my_feat_nlp.similarity(anno_feat_nlp) > threshold:\n",
    "                similar_features[my_feat] = anno_feat\n",
    "    return similar_features\n",
    "\n",
    "# Compare your feature dictionary keys with annotated tags\n",
    "similar_features = find_similar_features(feature_table_1['camera'].keys(), annotated_tags_dict.keys())\n",
    "\n",
    "similar_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "9d1fed8b-a479-4827-a4f3-a9dec161f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/pb8nhwfs3d357rgvk52mq_sc0000gn/T/ipykernel_55916/3057271662.py:16: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if other_feature not in visited and doc.similarity(other_doc) > threshold:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'weight': {'positive': 1, 'negative': 7},\n",
       " 'camera': {'positive': 83, 'negative': 19},\n",
       " 'feel': {'positive': 5, 'negative': 1},\n",
       " 'quality': {'positive': 8, 'negative': 3},\n",
       " 'optic': {'positive': 1, 'negative': 0},\n",
       " 'g3': {'positive': 5, 'negative': 1},\n",
       " 'use': {'positive': 8, 'negative': 0},\n",
       " 'download': {'positive': 6, 'negative': 4},\n",
       " 'highlight': {'positive': 9, 'negative': 3},\n",
       " 'battery': {'positive': 4, 'negative': 0},\n",
       " 'remote': {'positive': 1, 'negative': 0},\n",
       " 'lcd': {'positive': 3, 'negative': 0},\n",
       " 'shot': {'positive': 1, 'negative': 0},\n",
       " 'macro': {'positive': 1, 'negative': 0},\n",
       " 'strap': {'positive': 0, 'negative': 2},\n",
       " 'made': {'positive': 0, 'negative': 1},\n",
       " 'control': {'positive': 6, 'negative': 0},\n",
       " 'noise': {'positive': 1, 'negative': 1},\n",
       " 'compactflash': {'positive': 0, 'negative': 1},\n",
       " 'lag': {'positive': 0, 'negative': 1},\n",
       " 'finish': {'positive': 1, 'negative': 0},\n",
       " 'menu': {'positive': 3, 'negative': 0},\n",
       " 'delay': {'positive': 0, 'negative': 2},\n",
       " 'shape': {'positive': 3, 'negative': 0},\n",
       " 'canon': {'positive': 1, 'negative': 0},\n",
       " 'automode': {'positive': 1, 'negative': 0},\n",
       " 'service': {'positive': 1, 'negative': 0},\n",
       " 'depth': {'positive': 1, 'negative': 0},\n",
       " 'grain': {'positive': 0, 'negative': 1},\n",
       " 'price': {'positive': 0, 'negative': 1},\n",
       " 'dial': {'positive': 2, 'negative': 1},\n",
       " 'canera': {'positive': 1, 'negative': 0},\n",
       " '4mp': {'positive': 1, 'negative': 0},\n",
       " 'speed': {'positive': 1, 'negative': 0}}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_similar_features(feature_dict, threshold=0.5):\n",
    "\n",
    "    # Create spacy docs for each feature\n",
    "    feature_docs = {feature: nlp(feature) for feature in feature_dict.keys()}\n",
    "    \n",
    "    # Group features based on similarity\n",
    "    groups = []\n",
    "    visited = set()\n",
    "    \n",
    "    for feature, doc in feature_docs.items():\n",
    "        if feature in visited:\n",
    "            continue\n",
    "        current_group = [feature]\n",
    "        visited.add(feature)\n",
    "        for other_feature, other_doc in feature_docs.items():\n",
    "            if other_feature not in visited and doc.similarity(other_doc) > threshold:\n",
    "                current_group.append(other_feature)\n",
    "                visited.add(other_feature)\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    # Merge the groups into a new dictionary\n",
    "    merged_dict = {}\n",
    "    for group in groups:\n",
    "        group_name = group[0]  # Use the first feature name as the representative\n",
    "        merged_dict[group_name] = {'positive': 0, 'negative': 0}\n",
    "        for feature in group:\n",
    "            merged_dict[group_name]['positive'] += feature_dict[feature]['positive']\n",
    "            merged_dict[group_name]['negative'] += feature_dict[feature]['negative']\n",
    "    \n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "# Merge similar features\n",
    "merged_features = merge_similar_features(annotated_tags_dict)\n",
    "display(len(merged_features.keys()))\n",
    "display(len(annotated_tags_dict.keys()))\n",
    "merged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "b5b32ea6-15f7-4976-b37a-7206259fc11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')  # Load a larger model\n",
    "\n",
    "\n",
    "\n",
    "# Filter for matched features\n",
    "def filter_matched_features(my_dict, annotated_dict, similar_features):\n",
    "    filtered_matches = {}\n",
    "    for product in my_dict:\n",
    "        my_dict = list(my_dict[product].keys())\n",
    "        for my_feat, anno_feat in similar_features.items():\n",
    "            if my_feat in my_dict and anno_feat in annotated_dict:\n",
    "                filtered_matches[my_feat] = {\n",
    "                    'My Positive': my_dict[my_feat]['positive'],\n",
    "                    'My Negative': my_dict[my_feat]['negative'],\n",
    "                    'Annotated Positive': annotated_dict[anno_feat]['positive'],\n",
    "                    'Annotated Negative': annotated_dict[anno_feat]['negative']\n",
    "                }\n",
    "        return filtered_matches\n",
    "\n",
    "matched_features = filter_matched_features(feature_table_1['camera'], annotated_tags_dict, similar_features)\n",
    "\n",
    "matched_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "faf84fbf-791b-42c7-8400-93d393354c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use': 'use',\n",
       " 'picture': 'image',\n",
       " 'canon': 'canon',\n",
       " 'feature': 'feature',\n",
       " 'quality': 'quality',\n",
       " 'image': 'image',\n",
       " 'look': 'look',\n",
       " 'photo': 'picture',\n",
       " 'control': 'control',\n",
       " 'battery': 'battery',\n",
       " 'zoom': 'zoom'}"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4cdde-a192-4c24-8346-b811efd66753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
