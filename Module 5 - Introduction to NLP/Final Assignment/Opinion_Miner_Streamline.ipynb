{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61263af9-60d2-4c0d-bbd6-fa7c022e7af0",
   "metadata": {},
   "source": [
    "# Task and Data Analysis\n",
    "\n",
    "#### Overview of the Data\n",
    "\n",
    "The dataset consists of product reviews from Amazon, segmented into features with annotated sentiment scores. Reviews include multiple aspects such as ease of use, picture quality, and additional functionalities, each tagged with a corresponding sentiment score. These annotations provide a rich basis for sentiment analysis but require precise parsing to effectively utilize the structured format in which they are presented.\n",
    "\n",
    "#### System-Level Outline\n",
    "\n",
    "##### Data Parsing and Pre-processing\n",
    "\n",
    "Using a custom `read_file` function, the system initially parses the structured reviews from text files. This function handles the dataset's specific format, which includes initial metadata and reviewer comments split by a unique delimiter ('##'). This is crucial for separating feature tags from review content, facilitating subsequent analysis.\n",
    "\n",
    "The `pre_process_review` function further refines this by extracting titles and adjusting reviews accordingly, ensuring that the context provided by review headers is not lost. Additionally, it preserves the integrity of the review flow, which is vital for understanding the nuances of each review.\n",
    "\n",
    "##### Enhancing NLP with Custom Processing\n",
    "\n",
    "To deepen the analysis, the `preserve_compound_phrases` function is employed. This function utilizes an NLP model to identify and preserve compound nouns and adjectives directly linked to nouns, which are often critical in understanding the specific features discussed. By preserving these compounds, the system maintains the granular detail necessary for precise feature extraction.\n",
    "\n",
    "Following this, the `chunking_post_process` method reassembles the text from tokenized forms back into a structured format conducive to further analysis, ensuring that compound phrases are treated as single entities within the dataset.\n",
    "\n",
    "##### Comprehensive Review Filtering\n",
    "\n",
    "The `pre_processing_controller` function orchestrates the entire preprocessing pipeline. It transforms raw review texts into a tokenized format, applies compound preservation, and executes two levels of filtering: soft filtering (preserving basic structure and some stopwords) and hard filtering (removing all non-alphabetic characters and stopwords). This dual approach allows for flexibility in analysis, from high-level sentiment trends to detailed feature-specific sentiments.\n",
    "\n",
    "##### Sentiment Analysis and Feature Extraction\n",
    "\n",
    "Once preprocessed, the data is ripe for sentiment analysis. Leveraging the structured format of feature tags and sentiment scores, the system can map sentiments directly to product features, allowing for an aggregated sentiment score for each feature. This quantification is pivotal in determining which features are most appreciated or criticized by users.\n",
    "\n",
    "##### Leveraging Data for Business Insights\n",
    "\n",
    "The final step involves synthesizing the analyzed data into actionable business insights. By understanding which features correlate strongly with positive or negative sentiments, companies can prioritize product improvements or highlight successful aspects in marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97caa3c8-0c02-48d4-abe7-98b05163312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "import copy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Word2Vec_corpus = api.load('text8') \n",
    "Word2Vec_model = Word2Vec(Word2Vec_corpus) \n",
    "glove_model = api.load(\"glove-twitter-25\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce2b03ea-3191-44bf-9b2e-2c6d67a03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['Data/Customer_review_data/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "         'Data/Customer_review_data/Canon G3.txt',\n",
    "         'Data/Customer_review_data/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "         'Data/Customer_review_data/Nikon coolpix 4300.txt',\n",
    "         'Data/Customer_review_data/Nokia 6610.txt',\n",
    "         'Data/CustomerReviews-3_domains/Computer.txt',\n",
    "         'Data/CustomerReviews-3_domains/Router.txt',\n",
    "         'Data/CustomerReviews-3_domains/Speaker.txt',\n",
    "         'Data/Reviews-9-products/Canon PowerShot SD500.txt',\n",
    "         'Data/Reviews-9-products/Canon S100.txt',\n",
    "         'Data/Reviews-9-products/Diaper Champ.txt',\n",
    "         'Data/Reviews-9-products/Hitachi router.txt',\n",
    "         'Data/Reviews-9-products/ipod.txt',\n",
    "         'Data/Reviews-9-products/Linksys Router.txt',\n",
    "         'Data/Reviews-9-products/MicroMP3.txt',\n",
    "         'Data/Reviews-9-products/Nokia 6600.txt',\n",
    "         'Data/Reviews-9-products/norton.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed6882-2c25-48fb-9545-cf4c3668c231",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "#### Data Ingestion and Initial Processing\n",
    "\n",
    "The process begins with the reading of data files, where each file potentially contains multiple reviews with varying structures. A Python function, `read_file`, is employed to open and read the content of these files. Reviews are often separated by new lines and may begin with a distinctive marker of asterisks indicating metadata or headers that are not part of the actual review content. Such lines are programmatically identified and skipped, ensuring that only relevant text is processed further.\n",
    "\n",
    "Reviews within these files are then split using '##' as a delimiter to segregate tags from the main content, which allows for the extraction of embedded metadata or sentiment tags when present. Each piece of the review, along with its associated tags, is stored in a structured format within a pandas DataFrame, facilitating subsequent manipulations and analyses.\n",
    "\n",
    "#### Advanced Text Processing Techniques\n",
    "\n",
    "Once the initial ingestion is complete, the reviews undergo a series of sophisticated text processing steps encapsulated within the `pre_process_review` function. This function is designed to handle various nuances of the text, including title concatenation where necessary and preservation of the textual integrity for reviews that continue across multiple lines.\n",
    "\n",
    "##### Preservation of Semantic Structures\n",
    "\n",
    "To maintain the semantic integrity of phrases within the reviews, the `preserve_compound_phrases` function is applied. This function utilizes spaCy, an advanced NLP library, to parse the text and identify compound nouns and adjectival modifiers, which are crucial for understanding the context and sentiment related to specific product features. These phrases are then reconstructed with underscores replacing spaces to ensure they are treated as single tokens in subsequent analyses, preventing the loss of their semantic unity.\n",
    "\n",
    "##### Enhancement of Tokenization and Filtering\n",
    "\n",
    "Post semantic preservation, the `pre_processing_controller` function orchestrates several layers of tokenization and filtering. The text is first tokenized, ensuring that each word or phrase is individually analyzed. This tokenization feeds into a dual filtering process where:\n",
    "1. A 'Soft Filtered Review' captures tokens that either form part of the identified compound phrases or are standalone alphabetic words.\n",
    "2. A 'Filtered Review' applies a more stringent filter, additionally removing common stop words to focus on the more meaningful terms relevant to sentiment analysis.\n",
    "\n",
    "These tokens are then reassembled into coherent strings, forming the basis for deeper linguistic analysis, including lemmatization and stemming. Lemmatization is performed to reduce words to their base or dictionary form, whereas stemming further strips down the words to their root forms, often leading to a more generalized but powerful analysis of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84c1933d-e77d-4110-afd5-f8aeb435af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    tagged_reviews = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        # Split the text into lines and remove any leading/trailing whitespace\n",
    "        reviews = text.strip().split('\\n')\n",
    "\n",
    "        # Check if the file starts with a specific marker line of asterisks\n",
    "        if reviews[0] == '*' * 77:\n",
    "            # Skip the first 11 lines if the marker is present - This is a quirk to parse the data files\n",
    "            reviews = reviews[11:]\n",
    "\n",
    "        reviews = pre_process_review(reviews)\n",
    "        \n",
    "        for review in reviews:\n",
    "            # Split each review on '##' to separate tags from the content\n",
    "            parts = review.split('##')\n",
    "            \n",
    "            # If the split results in more than one part, process tags and content\n",
    "            if len(parts) > 1:\n",
    "                tags = parts[0].strip().split(',')\n",
    "                content = parts[1].strip() \n",
    "            else:\n",
    "                # If no '##' is found, set tags as empty and content to the whole line\n",
    "                tags = []\n",
    "                content = parts\n",
    "                \n",
    "            # Append a dictionary of tags and review content to the list\n",
    "            tagged_reviews.append({'Tags': tags, 'Review': content})\n",
    "\n",
    "        df = pd.DataFrame(tagged_reviews)\n",
    "        # Store the name of the file as an attribute of the DataFrame\n",
    "        df.attrs['title'] = file_path.split('/')[-1]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8551b5d4-90ba-48b6-9987-18d38e018b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing functions\n",
    "\n",
    "def pre_process_review(reviews):\n",
    "    processed_reviews = []\n",
    "    title_switch = False  # Indicates whether next review should append a title\n",
    "    title = ''\n",
    "\n",
    "    for review in reviews:\n",
    "        if review.startswith('[t]'):  # Checks for title marker\n",
    "            title = review[3:]  # Stores the title\n",
    "            title_switch = True\n",
    "        elif title_switch:  # Appends title to the review if flag is true\n",
    "            processed_reviews.append(review + title)\n",
    "            title_switch = False\n",
    "            title = ''\n",
    "        else:\n",
    "            processed_reviews.append(review)  # Adds review as is if no title is pending\n",
    "\n",
    "    return processed_reviews\n",
    "\n",
    "\n",
    "\n",
    "def preserve_compound_phrases(text):\n",
    "    # Process the text with an NLP model\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Check for compounds or adjectives linked directly to nouns\n",
    "        if token.dep_ in ('compound', 'amod') and token.head.pos_ == 'NOUN':\n",
    "            compound_phrase = f\"{token.text}_{token.head.text}\"\n",
    "            if compound_phrase not in processed_tokens:\n",
    "                processed_tokens.append(compound_phrase)\n",
    "        # Skip nouns that are already part of a compound to prevent duplicates\n",
    "        elif token.pos_ == 'NOUN' and any(child.dep_ == 'compound' for child in token.children):\n",
    "            continue\n",
    "        # Include all other tokens normally\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunking_post_process(text):\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        # Check if the current word is part of a compound phrase\n",
    "        if '_' in words[i]:\n",
    "            # Append all parts of the compound phrase to the list\n",
    "            while i < len(words) and '_' in words[i]:\n",
    "                processed_words.append(words[i])\n",
    "                i += 1\n",
    "            continue  # Move to the next word after finishing the compound phrase\n",
    "        # Append non-compound words directly to the list\n",
    "        processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    # Return the processed words as a single string\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a474f0fe-11c6-4582-916d-5e36195372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_controller(df):\n",
    "    \n",
    "    # Convert lists to strings and applies compound phrase preservation\n",
    "    df['Tokenised_Review'] = df['Review'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    df['Tokenised_Review'] = df['Tokenised_Review'].apply(lambda review: preserve_compound_phrases(review))\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation and numbers \n",
    "    df['Soft_Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower())])\n",
    "    \n",
    "    # Convert lists of tokens back to strings and retains the compound phrases - soft means not to filter out stop words\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    df['Soft_Filtered_Review_String'] = df['Soft_Filtered_Review_String'].apply(chunking_post_process)\n",
    "    \n",
    "    # Filters the reviews. Handling compound phrases, capitalisation, numbers and stop words \n",
    "    df['Filtered_Review'] = df['Tokenised_Review'].apply(lambda tokens: [token.lower() for token in tokens if (\"_\" in token) or (token.isalpha() and token.lower() not in stop_words)])\n",
    "    df['Filtered_Review_String'] = df['Filtered_Review'].apply(lambda tokens: ' '.join(tokens))\n",
    "    \n",
    "    # Lemmatise the filtered review strings\n",
    "    df['Lemmatised_Review_String'] = df['Filtered_Review_String'].apply(lambda review_string: \" \".join([token.lemma_ for token in nlp(review_string)]))\n",
    "    df['Lemmatised_Tokenised_Filtered_Review'] = df['Filtered_Review_String'].apply(lambda review: word_tokenize(review))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fede2a8-b11f-44ac-aa8e-c527b0d0e377",
   "metadata": {},
   "source": [
    "# Product Feature Extraction\n",
    "\n",
    "##### Part-of-Speech (POS) Noun Tagging\n",
    "\n",
    "The first step in the feature extraction process is the identification of nouns from customer reviews. This is accomplished by the `POS_Noun_Tagging` function, which employs tokenization and POS tagging to sift through the textual data. Nouns are indicative of features, as they often name the components or aspects of a product that customers discuss. By focusing on nouns, the function narrows down the vast amount of information in reviews to specific elements likely relevant to consumer sentiments.\n",
    "\n",
    "##### Concrete Noun Identification\n",
    "\n",
    "Further refining the noun extraction, the `is_concrete_noun` function determines whether a noun describes tangible aspects of a product, such as an \"object\" or \"device.\" This distinction is crucial because tangible feature mentions often directly relate to customer satisfaction and are actionable from a product improvement perspective. Using WordNet synsets and their hypernyms allows for an understanding of the word's concreteness in context, enhancing the relevance of the extracted features.\n",
    "\n",
    "##### Similarity-Based Filtering\n",
    "\n",
    "With the noun list refined to concrete nouns, the `similarity_filter` function applies vector space models like Word2Vec and GloVe to filter out words based on their semantic similarity. This step is vital to cluster similar features together, reducing redundancy and focusing on unique aspects mentioned across reviews. By setting a similarity threshold, the function ensures only significantly similar terms are grouped, maintaining a diverse yet focused feature set.\n",
    "\n",
    "##### Further Similarity Filtering\n",
    "\n",
    "The `further_similar_filter` function adds another layer of filtering by using GloVe's similarity scoring to eliminate closely related nouns, further condensing the feature list. This step is particularly useful in focusing on the most mentioned and hence potentially impactful features in customer reviews.\n",
    "\n",
    "##### Building the Feature Dictionary and DataFrames\n",
    "\n",
    "The `create_product_features_dict` function initializes a structured dictionary to map each product feature against its mentions and sentiment polarity. This dictionary is foundational for aggregating and analyzing sentiment data related to each feature. The `build_featured_df` function then constructs a DataFrame from the reviews, associating each with identified features and preparing the data for sentiment analysis.\n",
    "\n",
    "##### Sentiment Analysis and Count Aggregation\n",
    "\n",
    "The final analytical steps involve `feature_dict_count`, which tallies positive and negative mentions for each feature, providing a quantified sentiment outlook for each aspect of the product. This aggregation is pivotal for identifying strengths and weaknesses in the product as perceived by consumers.\n",
    "\n",
    "##### Efficient Data Management\n",
    "\n",
    "Lastly, the `df_filter` function restructures the DataFrame to optimize it for analysis, ensuring data is clean and well-organized for subsequent processing steps or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f298369f-55f2-44f6-940f-da089722320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_Noun_Tagging(string_list):\n",
    "    # Convert the string list to a regular list\n",
    "    reviews = string_list.tolist()\n",
    "    features = []\n",
    "    \n",
    "    # Process each review to extract nouns\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)  # Tokenize the text\n",
    "        tagged = pos_tag(tokens)  # POS tagging\n",
    "        # Collect nouns from tags\n",
    "        features.extend(word.lower() for word, tag in tagged if tag.startswith('NN'))\n",
    "    \n",
    "    # Count and retrieve the 15 most common nouns\n",
    "    feature_counts = Counter(features)\n",
    "    common_features = feature_counts.most_common(10)\n",
    "    \n",
    "    return common_features\n",
    "\n",
    "\n",
    "\n",
    "def is_concrete_noun(word):\n",
    "    # Define indicators for concrete nouns\n",
    "    concrete_indicators = {'object', 'artifact', 'instrumentality', 'container', 'device'}\n",
    "    # Retrieve synsets for the word as a noun\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    \n",
    "    # Check categories for each synset to determine if it's a concrete noun\n",
    "    for synset in synsets:\n",
    "        for hypernym in synset.closure(lambda s: s.hypernyms()):\n",
    "            if concrete_indicators.intersection(hypernym.lemma_names()):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6be50234-31a2-45fa-ae2b-d4a15be9a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_filter(word_tuple_list):\n",
    "    # Set a threshold for filtering similar words\n",
    "    similarity_threshold = 0.25\n",
    "    \n",
    "    # Create a list of words from the tuples\n",
    "    words = [word for word, _ in word_tuple_list]\n",
    "    \n",
    "    filtered_words = []\n",
    "    # Iterate over words to compute similarities\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Calculate similarity using Word2Vec\n",
    "            w2v_similarity = Word2Vec_model.wv.similarity(words[0], word)\n",
    "            # Calculate similarity using GloVe\n",
    "            glove_similarity = glove_model.similarity(words[0], word)\n",
    "            \n",
    "            # Calculate average similarity\n",
    "            avg_similarity = (w2v_similarity + glove_similarity) / 2\n",
    "            \n",
    "            # Append word to list if it meets the threshold\n",
    "            if avg_similarity >= similarity_threshold:\n",
    "                filtered_words.append(word)\n",
    "        except KeyError:\n",
    "            # Skip the word if it's not found in the model's vocabulary\n",
    "            continue\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def further_similar_filter(noun_list):\n",
    "    \n",
    "    items_to_remove = []\n",
    "    \n",
    "    for word in noun_list:\n",
    "        glove_similar_words = dict(glove_model.most_similar(noun_list[0], topn=10))\n",
    "        glove_similarity = glove_similar_words.get(word, 0) \n",
    "        if glove_similarity > 0:\n",
    "            items_to_remove.append(word)\n",
    "    \n",
    "    filtered_list = [item for item in noun_list if item not in items_to_remove]\n",
    "    \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69ee67fd-4927-4677-938a-2b16f9e74b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_product_features_dict(product_info):\n",
    "    # Extract the product title\n",
    "    product_title = product_info[0]\n",
    "\n",
    "    # Extract the features\n",
    "    product_features = product_info[1:]\n",
    "\n",
    "    # Create the dictionary with the desired structure\n",
    "    product_dict = {\n",
    "        product_title: {\n",
    "            feature: {\"positive\": 0, \"negative\": 0} for feature in product_features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def build_featured_df(df, feature_list_pos_noun):\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    feature_list = feature_list_pos_noun[1:]\n",
    "    \n",
    "    featured_items_list = []\n",
    "    \n",
    "    for idx, review in df.iterrows():\n",
    "        tokenised_review = review['Filtered_Review']\n",
    "        # Find the features present in the tokenised_review\n",
    "        featured_items = [item for item in feature_list if item in tokenised_review]\n",
    "            \n",
    "        if featured_items:\n",
    "            # Convert the review Series to a DataFrame with one row\n",
    "            review_df = review.to_frame().transpose()\n",
    "            \n",
    "            # New: Append the found features as a string (or you can keep it as list)\n",
    "            review_df['Featured_Items'] = [', '.join(featured_items)]  # As a single string of items\n",
    "    \n",
    "            review_df['Main_Index'] = idx\n",
    "            \n",
    "            # Append this review to the feature_df\n",
    "            feature_df = pd.concat([review_df, feature_df], axis=0)\n",
    "            \n",
    "            # Additionally, append the found features to the featured_items_list\n",
    "            featured_items_list.append(featured_items)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def feature_dict_count(feature_df, product_dict):\n",
    "    \n",
    "    sentiment_class_list = feature_df[['Featured_Items', 'Sentiment']]\n",
    "    \n",
    "    for idx, row in sentiment_class_list.iterrows():\n",
    "        feature = row['Featured_Items']\n",
    "        sentiment = row['Sentiment']\n",
    "    \n",
    "        if sentiment == 1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['positive'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['positive'] = val\n",
    "        elif sentiment == -1:\n",
    "            val = product_dict[list(product_dict.keys())[0]][feature]['negative'] + 1\n",
    "            product_dict[list(product_dict.keys())[0]][feature]['negative'] = val\n",
    "        else:\n",
    "            # print('More than two features, cannot classify')\n",
    "            pass\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "\n",
    "\n",
    "def df_filter(df):\n",
    "    df = df[['Main_Index', 'Featured_Items', 'Sentiment', 'Tags', 'Review', 'Tokenised_Review', 'Soft_Filtered_Review', 'Soft_Filtered_Review_String', 'Filtered_Review', 'Filtered_Review_String', 'Lemmatised_Review_String', 'Lemmatised_Tokenised_Filtered_Review']]\n",
    "    df = df.set_index('Main_Index', drop=True) \n",
    "    df.index.name = 'Main Index'\n",
    "    df = df.iloc[::-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e153d7a-bb12-4cb4-a3ce-d8cd0780cbfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "##### Sentiment Controller Function\n",
    "\n",
    "The core of the sentiment analysis system is the `sentiment_controller` function, which is responsible for aggregating and assigning sentiment scores to each review in the dataset. This function works by iterating through each review entry and identifying the associated product features. It is designed to handle reviews with a single feature distinctly, ensuring that the sentiment analysis is as precise and relevant to specific product attributes as possible.\n",
    "\n",
    "For reviews associated with exactly one product feature, the function utilizes the `get_phrase_sentiment` function to calculate a comprehensive sentiment score based on the textual content of the review. This distinction is crucial as it allows for a focused analysis, linking sentiments directly to individual features rather than generalizing across multiple attributes.\n",
    "\n",
    "##### Phrase and Word-Level Sentiment Calculation\n",
    "\n",
    "The `get_phrase_sentiment` function represents a layered approach to sentiment scoring. Initially, it attempts to understand the sentiment of the entire phrase as a compound unit, preserving the contextual integrity of the user’s opinion. If specific sentiment scores are available for the compound phrase, these are used directly. This approach is beneficial for capturing the sentiment of phrases where the combined meaning might differ significantly from the sum of individual word sentiments.\n",
    "\n",
    "If a direct compound sentiment score is unavailable, the function breaks down the phrase into individual words and computes the average sentiment score across all words. This breakdown is critical for capturing the nuances of language where compound scoring is not feasible, ensuring no sentiment information is lost.\n",
    "\n",
    "##### Individual Word Sentiment Scoring\n",
    "\n",
    "At the word level, the `get_word_sentiment` function retrieves sentiment scores using SentiWordNet, a well-regarded lexicon in sentiment analysis research. By analyzing the positive and negative sentiment scores associated with each word’s synset, the function provides a balanced view of the word’s emotional impact within the context of the review.\n",
    "\n",
    "This granular approach to sentiment analysis allows for a nuanced understanding of how specific words contribute to the overall sentiment of a phrase or sentence, enhancing the system's ability to detect and interpret the subtlest emotional undertones in consumer feedback.\n",
    "\n",
    "##### Integration into Review Analysis\n",
    "\n",
    "Once sentiments are calculated for each review, the `sentiment_controller` function appends these scores to the main DataFrame as a new column, facilitating further analysis and visualization of sentiment trends across different product features. This integration is pivotal in enabling end-to-end analysis, from raw review data to actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcbb4e4d-5031-45c2-adfc-39b39631bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_controller(feature_df):\n",
    "    # Initialize a list to hold sentiment values for each review\n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, entry in feature_df.iterrows():\n",
    "        review = entry['Soft_Filtered_Review_String']\n",
    "        features = [feature.strip() for feature in entry['Featured_Items'].split(',')]\n",
    "        \n",
    "        # Process sentiment only if there is exactly one feature\n",
    "        if len(features) == 1:\n",
    "            pos_score, neg_score = get_phrase_sentiment(review)\n",
    "            sentiment = 1 if pos_score > neg_score else -1\n",
    "            sentiment_list.append(sentiment)\n",
    "        else:\n",
    "            # Append neutral sentiment (0) if there are multiple or no features\n",
    "            sentiment_list.append(0)\n",
    "\n",
    "    # Add the sentiment list to the DataFrame as a new column\n",
    "    feature_df['Sentiment'] = sentiment_list\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "def get_phrase_sentiment(phrase):\n",
    "    # First try to get sentiment score for the whole phrase as a compound\n",
    "    pos_score, neg_score = get_word_sentiment(phrase.replace(\" \", \"_\"))\n",
    "    if pos_score or neg_score:\n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    # If no score, break down the phrase and calculate average sentiment scores\n",
    "    words = phrase.split()\n",
    "    total_pos, total_neg = 0, 0\n",
    "    for word in words:\n",
    "        pos, neg = get_word_sentiment(word)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "    \n",
    "    # Average the sentiment scores\n",
    "    avg_pos = total_pos / len(words) if words else 0\n",
    "    avg_neg = total_neg / len(words) if words else 0\n",
    "    \n",
    "    return avg_pos, avg_neg\n",
    "\n",
    "\n",
    "\n",
    "def get_word_sentiment(word):\n",
    "    # Retrieve sentiment scores from SentiWordNet\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if synsets:\n",
    "        return synsets[0].pos_score(), synsets[0].neg_score()\n",
    "    else:\n",
    "        return 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2eef0-37b9-44f7-b01b-1ac9ebac146c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2e03af2-c3f8-4083-a39f-0f0745004b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera': {'use': {'positive': 24, 'negative': 12},\n",
       "  'picture': {'positive': 2, 'negative': 10},\n",
       "  'canon': {'positive': 18, 'negative': 18},\n",
       "  'time': {'positive': 8, 'negative': 15},\n",
       "  'shoot': {'positive': 15, 'negative': 5},\n",
       "  'feature': {'positive': 0, 'negative': 1},\n",
       "  'quality': {'positive': 14, 'negative': 2}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def opinion_miner_controller(file):\n",
    "    \n",
    "    df = read_file(file)\n",
    "    df = pre_processing_controller(df)\n",
    "    \n",
    "    nouns = POS_Noun_Tagging(df['Lemmatised_Review_String'])\n",
    "    similar_nouns = similarity_filter(nouns)\n",
    "    features = further_similar_filter(similar_nouns)\n",
    "    feature_df = build_featured_df(df, features)\n",
    "    feature_df = sentiment_controller(feature_df)\n",
    "    feature_df = df_filter(feature_df)\n",
    "    product_dict = create_product_features_dict(features)\n",
    "    feature_dict = feature_dict_count(feature_df, copy.deepcopy(product_dict))\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "feature_dict = opinion_miner_controller(files[1])\n",
    "display(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64bf8-5415-4f4a-bb7a-b67f361545a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
