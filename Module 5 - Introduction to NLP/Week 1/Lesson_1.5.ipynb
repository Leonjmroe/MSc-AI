{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1, Lesson 5, Activity 10: Frequency Analysis\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Pre-process the provided texts using tokenization.\n",
    "- Run frequency analysis based on the techniques youâ€™ve learned about in Activity 9, visualise and analyse the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import data\n",
    "\n",
    "Import data from NLTK (see http://www.nltk.org/book_1ed/ch02.html), for example, using the Gutenberg dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "for fileid in gutenberg.fileids():\n",
    "    # print(fileid, gutenberg.raw(fileid)[:65])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use NLTK's FreqDist functionality\n",
    "\n",
    "Use the `FreqDist` functionality as shown in https://www.nltk.org/book/ch01.html and http://www.nltk.org/book_1ed/ch02.html. \n",
    "\n",
    "For the datasets available via NLTK you can either apply tokenization with `word_tokenize` or rely on the `.word` functionality, which provides you with tokenized output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 11454, '.': 6928, 'to': 5183, 'the': 4844, 'and': 4672, 'of': 4279, 'I': 3178, 'a': 3004, 'was': 2385, 'her': 2381, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = nltk.FreqDist(gutenberg.words(\"austen-emma.txt\"))\n",
    "fdist1\n",
    "# Print out most frequent 50 words with their counts. \n",
    "# Hint: you need to use most_common(number_of_words) method applied to fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell about the most frequent words in this text?\n",
    "\n",
    "Let's try visualising cumulative frequency of the most frequent $30$ words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you need to use plot(number_of_words, cumulative=True) method applied to fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this plot suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement FreqDist from scratch\n",
    "\n",
    "Collect words, calculate their frequency, and return a dictionary sorted in the reverse order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def collect_word_map(word_list):\n",
    "    word_map = {}\n",
    "    for a_word in word_list:\n",
    "        word_map[a_word] = # update the count for a_word in word_map by 1. \n",
    "                           # Hint: word_map.get(a_word) returns the current count,\n",
    "                           #       word_map.get(a_word, 0) allows you to cover cases where current word count is 0 \n",
    "    return word_map\n",
    "    \n",
    "# Let's sort the word frequency map by word counts, \n",
    "# starting from the largest count (reverse order), \n",
    "# and print up to 10 most frequent words\n",
    "word_map = collect_word_map(gutenberg.words(\"austen-emma.txt\"))\n",
    "sorted_map = (sorted(word_map.items(), key=operator.itemgetter(1)))[::-1]\n",
    "print(sorted_map[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the percentage of the content covered by specific (most frequent) words. E.g., what percentage of words used in text are commas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_percentage_map(word_map, up_to):\n",
    "    total_count = sum(word_map.values())\n",
    "    sorted_map = # sort the word frequency map by word counts, starting from the largest count (reverse order)\n",
    "    percentage_map = [(item[0], 100*float(item[1])/float(total_count)) for item in sorted_map[:up_to]]\n",
    "    return percentage_map\n",
    "\n",
    "print(collect_percentage_map(word_map, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualise the cumulative frequency counts as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def visualise_dist(word_map, up_to):\n",
    "    width = 10.0\n",
    "    percentage_map = # apply collect_percentage_map from above\n",
    "    sort_map = {}\n",
    "    rank = 0\n",
    "    cum_sum = 0\n",
    "    # Store cumulative percetage of coverage\n",
    "    for item in percentage_map:\n",
    "        rank += 1\n",
    "        cum_sum += item[1]\n",
    "        sort_map[rank] = cum_sum\n",
    "    # How much do the top n words account for?\n",
    "    print(\"Total cumulative coverage = %.2f\" % cum_sum + \"%\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"Cumulative coverage of the top \" + str(up_to) + \" words\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.xlabel(\"Top \" + str(up_to) + \" words\")\n",
    "    # Build the histogram for the percentages\n",
    "    plt.bar(range(len(sort_map)), sort_map.values())\n",
    "    # Label x axis with the ranks of the 1st to n-th most frequent word\n",
    "    # printing out each 5-th label on the axis\n",
    "    start, end = ax.get_xlim()\n",
    "    ax.xaxis.set_ticks(np.arange(start, end+1, 5))\n",
    "    plt.show()\n",
    "    \n",
    "# Explore statistics with a different number of top n words\n",
    "visualise_dist(word_map, 50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this cumulative distribution suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Apply to other texts\n",
    "\n",
    "This is an open-ended task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
