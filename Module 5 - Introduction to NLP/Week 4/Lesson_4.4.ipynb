{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4, Lesson 4, Activity 7: End-to-end topic classification\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(updated to newer library versions: Nadejda Roubtsova, February 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement a topic classification algorithm and apply it to the set of `20 Newsgroups` posts specified in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading\n",
    "\n",
    "First, let's import the libraries that we are going to use in this notebook. Then, let's define a method to load *training* and *test* subsets using a predefined list of categories. Note that following options are also available:\n",
    "- you can use `load_dataset('all', categories)` to load the whole dataset\n",
    "- you can use `load_dataset('train', None)` to load the set of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(a_set, cats):\n",
    "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return dataset\n",
    "\n",
    "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
    "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
    "\n",
    "# Apply load_dataset to the training subset\n",
    "newsgroups_train = load_dataset(a_set='train', cats=categories)\n",
    "\n",
    "# Apply load_dataset to the testing subset\n",
    "newsgroups_test = load_dataset(a_set='test', cats=categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is contained in the uploaded data subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'sci.space', 'talk.politics.mideast']\n",
      "Number of texts: 5913\n",
      "Number of target labels: 5913\n",
      "Equal sizes for data and targets\n",
      "First file name and location: /Users/leon/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.baseball/102665\n",
      "First data point text: I have posted the logos of the NL East teams to alt.binaries.pictures.misc \n",
      " Hopefully, I'll finish the series up next week with the NL West.\n",
      "\n",
      " Darren\n",
      "\n",
      "First 10 target labels: [4 3 9 7 4 3 0 5 7 8]\n",
      "\n",
      "***\n",
      "\n",
      "['comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'sci.space', 'talk.politics.mideast']\n",
      "Number of texts: 3937\n",
      "Number of target labels: 3937\n",
      "Equal sizes for data and targets\n",
      "First file name and location: /Users/leon/scikit_learn_data/20news_home/20news-bydate-test/misc.forsale/76785\n",
      "First data point text: As the title says. I would like to sell my Star LV2010 9 pin printer.\n",
      "Its a narrow colum dot matrix, supports both parallel and serial\n",
      "interfacing, prints at 200 characters per second, has a 16K buffer, \n",
      "and is very dependable...\n",
      "\n",
      "Drop some mail if your interested in it. $55 Plus shipping get the\n",
      "printer, and 6 extra srink-wraped ribbons, parallel connection\n",
      "cable, power cord, manual, and ONE sheet of paper (smile)...\n",
      "First 10 target labels: [1 7 2 5 3 5 7 3 0 2]\n"
     ]
    }
   ],
   "source": [
    "def check_data(dataset):\n",
    "    print(list(dataset.target_names))  # names of the categories\n",
    "    print(\"Number of texts:\", dataset.filenames.shape[0])  # the number of texts in the dataset\n",
    "    print(\"Number of target labels:\", dataset.target.shape[0])  # the number of target labels\n",
    "    \n",
    "    # Check that the sizes of both the data (number of texts) and labels (number of targets) is equal\n",
    "    if dataset.filenames.shape[0] == dataset.target.shape[0]:\n",
    "        print(\"Equal sizes for data and targets\")\n",
    "    \n",
    "    print(\"First file name and location:\", dataset.filenames[0])  # name and location of the file\n",
    "    print(\"First data point text:\", dataset.data[0])  # print the first data point text\n",
    "    \n",
    "    print(\"First 10 target labels:\", dataset.target[:10])  # print out the first 10 target labels\n",
    "\n",
    "# Now calling the check_data function for both the train and test datasets\n",
    "check_data(newsgroups_train)\n",
    "print(\"\\n***\\n\")\n",
    "check_data(newsgroups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ML pipeline with sklearn\n",
    "\n",
    "Now let's create word vectors based on the content of the posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "def text2vec(vectorizer, train_set, test_set):\n",
    "    # Fit the vectorizer to the training set and transform it\n",
    "    vectors_train = vectorizer.fit_transform(train_set.data)\n",
    "    # Transform the test set\n",
    "    vectors_test = vectorizer.transform(test_set.data)  # Complete this line to apply vectorizer to the test_set data\n",
    "    return vectors_train, vectors_test\n",
    "\n",
    "# Apply the function to the relevant data structures\n",
    "vectors_train, vectors_test = text2vec(vectorizer, newsgroups_train, newsgroups_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the data looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vectors_train: (5913, 52746)\n",
      "Shape of vectors_test: (3937, 52746)\n",
      "First vector of training data (in dense format): [[0. 0. 0. ... 0. 0. 0.]]\n",
      "Feature name at index 33404: nl\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of vectors_train:\", vectors_train.shape)\n",
    "\n",
    "# Print the shape of the vectors_test to compare it with the vectors_train\n",
    "print(\"Shape of vectors_test:\", vectors_test.shape)\n",
    "\n",
    "# Print the first vector of the training data in a dense format\n",
    "# Note: vectors_train is in a sparse format by default, you might want to convert it to dense format for better readability or inspection\n",
    "print(\"First vector of training data (in dense format):\", vectors_train[0].todense())\n",
    "\n",
    "# Assuming the request to print a specific feature name with index 33404 might not match the actual number of features\n",
    "# Let's safely access a feature name within the bounds\n",
    "feature_index = 33404 if 33404 < vectors_train.shape[1] else 0  # Fallback to 0 if 33404 is out of bounds\n",
    "print(f\"Feature name at index {feature_index}:\", vectorizer.get_feature_names_out()[feature_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply a machine learning classifier to the data\n",
    "\n",
    "Next, let's apply the Multinomial Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "predictions = clf.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the results\n",
    "\n",
    "Finally, let's evaluate the results, extract the most informative terms per topic, and print out and visualise the confusion matrix. What can you say about the final results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       comp.windows.x       0.92      0.90      0.91       395\n",
      "         misc.forsale       0.88      0.87      0.87       390\n",
      "            rec.autos       0.83      0.78      0.80       396\n",
      "      rec.motorcycles       0.85      0.80      0.83       398\n",
      "   rec.sport.baseball       0.92      0.84      0.88       397\n",
      "     rec.sport.hockey       0.71      0.94      0.81       399\n",
      "            sci.crypt       0.82      0.85      0.83       396\n",
      "              sci.med       0.92      0.82      0.87       396\n",
      "            sci.space       0.86      0.82      0.84       394\n",
      "talk.politics.mideast       0.86      0.90      0.88       376\n",
      "\n",
      "             accuracy                           0.85      3937\n",
      "            macro avg       0.86      0.85      0.85      3937\n",
      "         weighted avg       0.86      0.85      0.85      3937\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_report)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Show the top 10 most informative words for each category\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m show_top(\u001b[43mclassifier\u001b[49m, newsgroups_test\u001b[38;5;241m.\u001b[39mtarget_names, vectorizer, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def show_top(classifier, categories, vectorizer, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        top = np.argsort(classifier.feature_log_prob_[i])[-n:]\n",
    "        print(f'{category}: {\" \".join(feature_names[top])}')\n",
    "        \n",
    "\n",
    "full_report = metrics.classification_report(newsgroups_test.target, \n",
    "                                            predictions, target_names=newsgroups_test.target_names)\n",
    "print(full_report)\n",
    "# Show the top 10 most informative words for each category\n",
    "show_top(clf, newsgroups_test.target_names, vectorizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further evaluation using confusion matrices and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = clf.fit(vectors_train, newsgroups_train.target)\n",
    "ConfusionMatrixDisplay.from_estimator(classifier, vectors_test, newsgroups_test.target)\n",
    "plt.show()\n",
    "\n",
    "for i, category in enumerate(newsgroups_train.target_names):\n",
    "    print(i, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
