{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4, Lesson 4, Activity 7: End-to-end topic classification\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(updated to newer library versions: Nadejda Roubtsova, February 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement a topic classification algorithm and apply it to the set of `20 Newsgroups` posts specified in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading\n",
    "\n",
    "First, let's import the libraries that we are going to use in this notebook. Then, let's define a method to load *training* and *test* subsets using a predefined list of categories. Note that following options are also available:\n",
    "- you can use `load_dataset('all', categories)` to load the whole dataset\n",
    "- you can use `load_dataset('train', None)` to load the set of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset() missing 2 required positional arguments: 'a_set' and 'cats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomp.windows.x\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmisc.forsale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec.autos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec.motorcycles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec.sport.baseball\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m categories \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec.sport.hockey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msci.crypt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msci.med\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msci.space\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtalk.politics.mideast\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m newsgroups_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;66;43;03m# appy load_dataset to the training subset a_set='train' and the set of categories\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m newsgroups_test \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;66;03m# appy load_dataset to the training subset a_set='test' and the set of categories\u001b[39;00m\n\u001b[1;32m     16\u001b[0m                                )\n",
      "\u001b[0;31mTypeError\u001b[0m: load_dataset() missing 2 required positional arguments: 'a_set' and 'cats'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(a_set, cats):\n",
    "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return dataset\n",
    "\n",
    "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
    "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
    "\n",
    "newsgroups_train = load_dataset(# appy load_dataset to the training subset a_set='train' and the set of categories\n",
    "                                )\n",
    "newsgroups_test = load_dataset(# appy load_dataset to the training subset a_set='test' and the set of categories\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is contained in the uploaded data subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(dataset):\n",
    "    print(list(dataset.target_names)) # names of the categories\n",
    "    print(# the number of texts in the dataset can be accessed using dataset.filenames.shape\n",
    "          )\n",
    "    print(# the number of target labels is accessible in a similar way, using .target field instead of .filenames\n",
    "          # this number should be equal to the number of texts\n",
    "          )\n",
    "    if # check that the sizes of both the data (number of texts) and labels (number of targets) is equal\n",
    "        print(\"Equal sizes for data and targets\")\n",
    "    print(dataset.filenames[0]) # name and location of the file\n",
    "    print(dataset.data[0])\n",
    "    print(# print out the first 10 target labels\n",
    "          )\n",
    "    \n",
    "check_data(newsgroups_train)\n",
    "print(\"\\n***\\n\")\n",
    "check_data(newsgroups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ML pipeline with sklearn\n",
    "\n",
    "Now let's create word vectors based on the content of the posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "def text2vec(vectorizer, train_set, test_set):\n",
    "    vectors_train = vectorizer.fit_transform(train_set.data)\n",
    "    vectors_test = vectorizer.transform(# now apply vectorizer to the test_set data\n",
    "                                        # Note: you apply only the .transform method, \n",
    "                                        # not .fit_transform to the test data\n",
    "                                        )\n",
    "    return vectors_train, vectors_test\n",
    "\n",
    "vectors_train, vectors_test = text2vec(# apply to the relevant data structures\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the data looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_train.shape)\n",
    "print(# apply the same to the test data\n",
    "      # the number of test documents should be the same as before\n",
    "      # the number of features should be the same for the train and the test data\n",
    "      )\n",
    "print(vectors_train[0])\n",
    "print(vectorizer.get_feature_names_out()[33404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply a machine learning classifier to the data\n",
    "\n",
    "Next, let's apply the Multinomial Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "predictions = clf.predict(vectors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the results\n",
    "\n",
    "Finally, let's evaluate the results, extract the most informative terms per topic, and print out and visualise the confusion matrix. What can you say about the final results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def show_top(classifier, categories, vectorizer, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        top = np.argsort(classifier.feature_log_prob_[i])[-n:]\n",
    "        print(f'{category}: {\" \".join(feature_names[top])}')\n",
    "        \n",
    "\n",
    "full_report = metrics.classification_report(newsgroups_test.target, \n",
    "                                            predictions, target_names=newsgroups_test.target_names)\n",
    "print(full_report)\n",
    "show_top(# apply to the relvant data structures to return the top 10 most informative words\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further evaluation using confusion matrices and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = clf.fit(vectors_train, newsgroups_train.target)\n",
    "ConfusionMatrixDisplay.from_estimator(classifier, vectors_test, newsgroups_test.target)\n",
    "plt.show()\n",
    "\n",
    "for i, category in enumerate(newsgroups_train.target_names):\n",
    "    print(i, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
