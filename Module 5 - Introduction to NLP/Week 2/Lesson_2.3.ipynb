{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Lesson 3, Activity 7: Term-weighting techniques\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(revised: Nadejda Roubtsova, June 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement TF-IDF weighting using the material presented in this lesson.\n",
    "- Apply these techniques to the collection of documents provided.\n",
    "- Return the TF-IDF scores for the provided set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the data\n",
    "\n",
    "There are three components to this data:\n",
    "- documents with their ids and content – there are $1460$ of those to be precise;\n",
    "- questions / queries with their ids and content – there are $112$ of those;\n",
    "- mapping between the queries and relevant documents.\n",
    "\n",
    "First, let's read in documents from the `CISI.ALL` file and store the result in the `documents` data structure where document contents are stored under corresponding document ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    f = open(\"cisi/CISI.ALL\")\n",
    "    merged = \"\"\n",
    "    \n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged += \"\\n\" + a_line.strip()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip()\n",
    "    \n",
    "    documents = {}\n",
    "\n",
    "    content = \"\"\n",
    "    doc_id = \"\"\n",
    "\n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            doc_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".X\"):\n",
    "            documents[doc_id] = content\n",
    "            content = \"\"\n",
    "            doc_id = \"\"\n",
    "        else:\n",
    "            content += a_line.strip()[3:] + \" \"\n",
    "    f.close()\n",
    "    return documents\n",
    "\n",
    "documents = read_documents()\n",
    "print(f\"{len(documents)} documents in total\")\n",
    "print(\"Document with id 1:\")\n",
    "print(documents.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's read in queries from the `CISI.QRY` file and store the result in the `queries` data structure where query contents are stored under corresponding query ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    f = open(\"cisi/CISI.QRY\")\n",
    "    merged = \"\"\n",
    "    \n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged += \"\\n\" + a_line.strip()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip()\n",
    "    \n",
    "    queries = {}\n",
    "\n",
    "    content = \"\"\n",
    "    qry_id = \"\"\n",
    "\n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            if not content==\"\":\n",
    "                queries[qry_id] = content\n",
    "                content = \"\"\n",
    "                qry_id = \"\"\n",
    "            qry_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".W\") or a_line.startswith(\".T\"):\n",
    "            content += a_line.strip()[3:] + \" \"\n",
    "    queries[qry_id] = content\n",
    "    f.close()\n",
    "    return queries\n",
    "\n",
    "queries = read_queries()\n",
    "print(f\"{len(queries)} queries in total\")\n",
    "print(\"Query with id 1:\")\n",
    "print(queries.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's read in the mapping between the queries and the documents. We'll keep these in the `mappings` data structure where each query index (key) corresponds to the list of one or more document indices (value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mappings():\n",
    "    f = open(\"cisi/CISI.REL\")\n",
    "    \n",
    "    mappings = {}\n",
    "\n",
    "    for a_line in f.readlines():\n",
    "        voc = a_line.strip().split()\n",
    "        key = voc[0].strip()\n",
    "        current_value = voc[1].strip()\n",
    "        value = []\n",
    "        if key in mappings.keys():\n",
    "            value = mappings.get(key)\n",
    "        value.append(current_value)\n",
    "        mappings[key] = value\n",
    "\n",
    "    f.close()\n",
    "    return mappings\n",
    "\n",
    "mappings = read_mappings()\n",
    "print(f\"{len(mappings)} mappings in total\")\n",
    "print(mappings.keys())\n",
    "print(\"Mapping for query with id 1:\")\n",
    "print(mappings.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the data\n",
    "\n",
    "Practise application of the following steps:\n",
    "- tokenize the texts\n",
    "- put all to lowercase\n",
    "- remove stopwords\n",
    "- apply stemming\n",
    "\n",
    "Implement and apply these steps to a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def process(text): \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    st = LancasterStemmer()\n",
    "    word_list = [st.stem(word) for word in \n",
    "                 # a tokenized list of words, all converted to lowercase,\n",
    "                 # if the word is not in the stoplist and not a punctuation mark (from string.punctuation)\n",
    "                 ]\n",
    "    return word_list\n",
    "  \n",
    "word_list = process(documents.get(\"27\"))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Term weighting\n",
    "\n",
    "First calculate the term frequency in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(text): \n",
    "    terms = {}\n",
    "    st = LancasterStemmer()\n",
    "    stoplist = # as above\n",
    "    word_list = # as above\n",
    "    for word in word_list:\n",
    "        terms[word] = terms.get(word, 0) + 1\n",
    "    return terms\n",
    "\n",
    "doc_terms = {}\n",
    "qry_terms = {}\n",
    "for doc_id in documents.keys():\n",
    "    doc_terms[doc_id] = get_terms(# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "for qry_id in queries.keys():\n",
    "    qry_terms[qry_id] = get_terms(# apply to the content of the query with id qry_id\n",
    "                                  )\n",
    "\n",
    "\n",
    "print(f\"{len(doc_terms)} documents in total\") # Sanity check – this should be the same number as before\n",
    "d1_terms = doc_terms.get(\"1\")\n",
    "print(\"Terms and frequencies for document with id 1:\")\n",
    "print(d1_terms)\n",
    "print(f\"{len(d1_terms)} terms in this document\")\n",
    "print()\n",
    "print(f\"{len(qry_terms)} queries in total\") # Sanity check – this should be the same number as before\n",
    "q1_terms = qry_terms.get(\"1\")\n",
    "print(\"Terms and frequencies for query with id 1:\")\n",
    "print(q1_terms)\n",
    "print(f\"{len(q1_terms)} terms in this query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, collect shared vocabulary from all documents and queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vocabulary():\n",
    "    all_terms = []\n",
    "    for doc_id in doc_terms.keys():\n",
    "        for term in doc_terms.get(doc_id).keys():            \n",
    "            all_terms.append(term)\n",
    "    for qry_id in qry_terms.keys():\n",
    "        # apply the same procedure to the query terms\n",
    "    return sorted(set(all_terms))\n",
    "\n",
    "all_terms = collect_vocabulary()\n",
    "print(f\"{len(all_terms)} terms in the shared vocabulary\")\n",
    "print(\"First 10:\")\n",
    "print(all_terms[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each document and query as vectors containing word counts in the shared space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(input_terms, shared_vocabulary):\n",
    "    output = {}\n",
    "    for item_id in input_terms.keys(): # e.g., a document in doc_terms\n",
    "        terms = input_terms.get(item_id)\n",
    "        output_vector = []\n",
    "        for word in shared_vocabulary:\n",
    "            if word in terms.keys():\n",
    "                # add the raw count of the word from the shared vocabulary in doc to the doc vector\n",
    "                output_vector.append(int(terms.get(word)))\n",
    "            else:\n",
    "                # if the word from the shared vocabulary is not in doc, add 0 to the doc vector in this position\n",
    "                output_vector.append(0)\n",
    "        output[item_id] = output_vector\n",
    "    return output\n",
    "\n",
    "doc_vectors = vectorize(# apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "qry_vectors = vectorize(# apply vectorize to the qry_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "\n",
    "print(f\"{len(doc_vectors)} document vectors\") # This should be the same number as before\n",
    "d1460_vector = doc_vectors.get(\"1460\")\n",
    "print(f\"{len(d1460_vector)} terms in this document\") # This should be the same number as before\n",
    "print(f\"{len(qry_vectors)} query vectors\") # This should be the same number as before\n",
    "q112_vector = qry_vectors.get(\"112\")\n",
    "print(f\"{len(q112_vector)} terms in this query\") # This should be the same number as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idfs(shared_vocabulary, d_terms):\n",
    "    doc_idfs = {}\n",
    "    for term in shared_vocabulary:\n",
    "        doc_count = 0 # the number of documents containing this term\n",
    "        for doc_id in d_terms.keys():\n",
    "            terms = d_terms.get(doc_id)\n",
    "            if term in terms.keys():\n",
    "                doc_count += 1\n",
    "        doc_idfs[term] = math.log(float(len(d_terms.keys()))/float(1 + doc_count), 10)\n",
    "    return doc_idfs\n",
    "\n",
    "doc_idfs = calculate_idfs(# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "print(f\"{len(doc_idfs)} terms with idf scores\") # This should be the same number as before\n",
    "print(\"Idf score for the word system:\")\n",
    "print(doc_idfs.get(\"system\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_idf(input_terms, input_idfs, shared_vocabulary):\n",
    "    output = {}\n",
    "    for item_id in input_terms.keys():\n",
    "        terms = # collect terms from the document\n",
    "        output_vector = []\n",
    "        for term in shared_vocabulary:\n",
    "            if term in terms.keys():\n",
    "                output_vector.append(input_idfs.get(term)*float(terms.get(term)))\n",
    "            else:\n",
    "                output_vector.append(float(0))\n",
    "        output[item_id] = output_vector\n",
    "    return output\n",
    "\n",
    "doc_vectors = vectorize_idf(# apply to the relevant data structures\n",
    "                            )\n",
    "\n",
    "print(f\"{len(doc_vectors)} document vectors\") # This should be the same number as before\n",
    "print(\"Number of idf-scored words in a particular document:\")\n",
    "print(len(doc_vectors.get(\"1460\"))) # This should be the same number as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
