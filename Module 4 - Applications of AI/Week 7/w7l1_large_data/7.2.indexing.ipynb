{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Artificial Intelligence\n",
    "## Large Data Sets\n",
    "### Indexing\n",
    "In the previous demo you saw we could go through a large dataset in chunks. This time we'll show how we can leverage some of the material you learned last week about databases to work with various subsets of a large dataset efficiently.\n",
    "\n",
    "We'll also introduce a new library on top of SQLite called SQLAlchemy. SQLAlchemy's primary focus is providing a layer of abstraction that allows you to write classes which can be stored in and retrieved from databases directly, without having to write SQL. But it is also the main way to interface with a database from Pandas, especially if you want to use Pandas with a database other than SQLite.\n",
    "\n",
    "SQLAlchemy provides a really helpful interface between SQLite, which manages the database itself, and Pandas, which is obviously useful for doing data analysis within a single application, and actually reading CSV files in chunks. SQLAlchemy should be installed by default if you installed Python via Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "database = sqlalchemy.create_engine('sqlite:///database.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above creates the SQLite database via SQLAlchemy, or connects if it already exists.\n",
    "\n",
    "Now, to populate the database, we are going to read the CSV file in chunks in the same way that we did previously, inserting the data from those chunks to the database. This can be quite slow, but only needs to be done once on a large dataset. Note that we have to rename any columns from the CSV file that contain spaces, since this is not supported in the database.\n",
    "\n",
    "*Note: the code below may take a couple of minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 39.4 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "for df in pd.read_csv('data.csv', iterator=True, chunksize=100000):\n",
    "    df = df.rename(columns={c: c.replace(' ', '_') for c in df.columns})\n",
    "    df.to_sql('Sales', database, if_exists='append')\n",
    "    \n",
    "end_time = time.process_time()\n",
    "print(f\"Total time: {end_time-start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas automatically set up and executed the SQL for us to store all of this data in the SQLite database. \n",
    "\n",
    "If we're curious, we can query the metadata of this table using a PRAGMA statement, to see the names of the columns, their types, and so on. This is quite a niche thing to want to do, and in reality you would probably just continue using this data via Pandas. But, just to show you what it has set up for us, you can see the result below. Notice it has had a good guess at the types of each of the columns.\n",
    "\n",
    "(You can read more about this kind of meta-SQL [here](https://www.sqlitetutorial.net/sqlite-tutorial/sqlite-describe-table/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAGMA TABLE_INFO(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, item \u001b[38;5;129;01min\u001b[39;00m row\u001b[38;5;241m.\u001b[39mitems()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'execute'"
     ]
    }
   ],
   "source": [
    "result = database.execute(\"PRAGMA TABLE_INFO('Sales');\")\n",
    "for row in result:\n",
    "    print(\", \".join(f\"{key}: {item}\" for key, item in row.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually run SELECT queries against the underlying SQLite database too. We could pick out some data in the middle of the original dataset (with index over 1 million) to confirm that it seems to be loading correctly, as shown in the cell below.\n",
    "\n",
    "We can do this because Pandas automatically inserted its own row indices as a column called `index` into our table. Since this gives each row a unique value, it would be a good candidate for a primary key if we needed one (although it did not set this up). `index` is actually a bad name for a column because it already exists as a keyword in SQL, so to query it, we have to put square brackets around the name `[index]`. We also use LIMIT to just pull a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = database.execute(\"\"\"SELECT [index], Region \n",
    "                             FROM Sales \n",
    "                             WHERE [index] > 1000000 \n",
    "                             LIMIT 5;\"\"\")\n",
    "\n",
    "for row in result:\n",
    "    print(\", \".join(f\"{key}: {item}\" for key, item in row.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we want to do some data analysis on a subset of this data, then we will probably want the result back in a Pandas dataframe. We can pass the SQL query straight into Pandas to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.process_time()\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM Sales WHERE Country=\"Oman\";', database)\n",
    "\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(f\"Total time: {end_time-start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview all rows that have country Oman\n",
    "print(f\"Number of sales in Oman: {df.shape[0]}\")\n",
    "print(\"Preview of first three rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by using SQLite we are able to query this massive dataset in a reasonable amount of time. Database management systems do a lot of work by default to try to make queries fast, which may include some amount of indexing.\n",
    "\n",
    "But if we know we're going to be doing lots of queries that include the Country column, we can tell the database to specifically create an index on this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.execute(\"CREATE INDEX idx_country ON Sales (Country);\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try the query again that pulls all the data from Oman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.process_time()\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM Sales WHERE Country=\"Oman\";', database)\n",
    "\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(f\"Total time: {end_time-start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my test, the time went from 0.8 to 0.2 seconds, cutting the query time to 25% of its original. This could make a big difference in a data intensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
