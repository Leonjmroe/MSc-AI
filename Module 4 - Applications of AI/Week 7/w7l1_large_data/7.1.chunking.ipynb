{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Artificial Intelligence\n",
    "## Large Data Sets\n",
    "### Chunking\n",
    "In this notebook we'll work with a large CSV file. \n",
    "\n",
    "If you downloaded the large version of this demo, then the file has 5 million rows and is 624 MB. This doesn't seem that large compared to the RAM of modern machines, but even if we manage to load it, we shouldn't expect any analysis to run smoothly. \n",
    "\n",
    "If you downloaded the smaller version for bandwidth reasons, then the dataset is just 100,000 rows, and you might be able to read it all, but the same principles will work just for demonstration.\n",
    "\n",
    "This notebook is going to rely on two of the features in the CSV reader from Pandas. It's worth considering how you would have incorporated these into the CSV reader you wrote in an earlier week.\n",
    "\n",
    "First, let's suppose the file is so big you have no way to open it in any application. We can't write our analysis code if we don't even know what columns the dataset contains. Thankfully, we can get a glimpse of what's contained in the data set using the 'number of rows' parameter `nrows` of the Pandas CSV reader. Here we have set that to five so we can see the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "      <th>Item Type</th>\n",
       "      <th>Sales Channel</th>\n",
       "      <th>Order Priority</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Units Sold</th>\n",
       "      <th>Unit Price</th>\n",
       "      <th>Unit Cost</th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Total Cost</th>\n",
       "      <th>Total Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia and Oceania</td>\n",
       "      <td>Palau</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Online</td>\n",
       "      <td>H</td>\n",
       "      <td>3/6/2016</td>\n",
       "      <td>517073523</td>\n",
       "      <td>3/26/2016</td>\n",
       "      <td>2401</td>\n",
       "      <td>651.21</td>\n",
       "      <td>524.96</td>\n",
       "      <td>1563555.21</td>\n",
       "      <td>1260428.96</td>\n",
       "      <td>303126.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Online</td>\n",
       "      <td>L</td>\n",
       "      <td>4/18/2010</td>\n",
       "      <td>380507028</td>\n",
       "      <td>5/26/2010</td>\n",
       "      <td>9340</td>\n",
       "      <td>47.45</td>\n",
       "      <td>31.79</td>\n",
       "      <td>443183.00</td>\n",
       "      <td>296918.60</td>\n",
       "      <td>146264.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Cereal</td>\n",
       "      <td>Online</td>\n",
       "      <td>M</td>\n",
       "      <td>1/8/2015</td>\n",
       "      <td>504055583</td>\n",
       "      <td>1/31/2015</td>\n",
       "      <td>103</td>\n",
       "      <td>205.70</td>\n",
       "      <td>117.11</td>\n",
       "      <td>21187.10</td>\n",
       "      <td>12062.33</td>\n",
       "      <td>9124.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>Snacks</td>\n",
       "      <td>Online</td>\n",
       "      <td>C</td>\n",
       "      <td>1/19/2014</td>\n",
       "      <td>954955518</td>\n",
       "      <td>2/27/2014</td>\n",
       "      <td>1414</td>\n",
       "      <td>152.58</td>\n",
       "      <td>97.44</td>\n",
       "      <td>215748.12</td>\n",
       "      <td>137780.16</td>\n",
       "      <td>77967.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middle East and North Africa</td>\n",
       "      <td>Oman</td>\n",
       "      <td>Cereal</td>\n",
       "      <td>Offline</td>\n",
       "      <td>H</td>\n",
       "      <td>4/26/2019</td>\n",
       "      <td>970755660</td>\n",
       "      <td>6/2/2019</td>\n",
       "      <td>7027</td>\n",
       "      <td>205.70</td>\n",
       "      <td>117.11</td>\n",
       "      <td>1445453.90</td>\n",
       "      <td>822931.97</td>\n",
       "      <td>622521.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Region  Country        Item Type Sales Channel  \\\n",
       "0         Australia and Oceania    Palau  Office Supplies        Online   \n",
       "1                        Europe   Poland        Beverages        Online   \n",
       "2                 North America   Canada           Cereal        Online   \n",
       "3                        Europe  Belarus           Snacks        Online   \n",
       "4  Middle East and North Africa     Oman           Cereal       Offline   \n",
       "\n",
       "  Order Priority Order Date   Order ID  Ship Date  Units Sold  Unit Price  \\\n",
       "0              H   3/6/2016  517073523  3/26/2016        2401      651.21   \n",
       "1              L  4/18/2010  380507028  5/26/2010        9340       47.45   \n",
       "2              M   1/8/2015  504055583  1/31/2015         103      205.70   \n",
       "3              C  1/19/2014  954955518  2/27/2014        1414      152.58   \n",
       "4              H  4/26/2019  970755660   6/2/2019        7027      205.70   \n",
       "\n",
       "   Unit Cost  Total Revenue  Total Cost  Total Profit  \n",
       "0     524.96     1563555.21  1260428.96     303126.25  \n",
       "1      31.79      443183.00   296918.60     146264.40  \n",
       "2     117.11       21187.10    12062.33       9124.77  \n",
       "3      97.44      215748.12   137780.16      77967.96  \n",
       "4     117.11     1445453.90   822931.97     622521.93  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv', nrows=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that our file contains some sales data with information about the region and country in which a sale took place, the type of items involved, order ID, shipment dates, unit prices, and so on. \n",
    "\n",
    "Now that we know what the data looks like, we can build something that works over the entire dataset. To get through the data with reasonable memory usage, we'll use a strategy called chunking, where we read the CSV file in blocks or chunks and process the data for that chunk independently. This allows us to work with data files that may contain millions or even billions of data entries. \n",
    "\n",
    "First let's work out what we are trying to do to each chunk, and ensure this code works. Suppose we want a list of every country from the dataset, along with the number of rows in which that country appears. We can try this out on a smaller chunk of data just to ensure it works first of all.\n",
    "\n",
    "We can use the `.value_counts()` method on the country column to count the unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Portugal      3\n",
       "Oman          3\n",
       "Montenegro    3\n",
       "Poland        3\n",
       "Qatar         3\n",
       "             ..\n",
       "Austria       1\n",
       "Namibia       1\n",
       "Estonia       1\n",
       "Grenada       1\n",
       "Nauru         1\n",
       "Name: Country, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', nrows=100)\n",
    "country_counts = df[\"Country\"].value_counts()\n",
    "country_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a Pandas `Series` object, which is like a 1D DataFrame. Each row is labelled with the name of the country. \n",
    "\n",
    "If we add two `Series` objects together, it will sum the values that are in the rows with the same label. However, we will get errors in any countries which only occur in one of the chunks but not the other – they will be set to NaN.\n",
    "\n",
    "In the following cell we demonstrate this on two chunks of 100 values from the CSV file. We use `skiprows` to select the *second* chunk of 100 rows – we must keep row zero, since it contains the header, so we skip rows from `range(1, 100)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algeria                 NaN\n",
       "Andorra                 NaN\n",
       "Antigua and Barbuda     2.0\n",
       "Austria                 NaN\n",
       "Azerbaijan              NaN\n",
       "                       ... \n",
       "Uzbekistan              NaN\n",
       "Vatican City            NaN\n",
       "Yemen                   4.0\n",
       "Zambia                  2.0\n",
       "Zimbabwe                2.0\n",
       "Name: Country, Length: 118, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', nrows=100, skiprows=range(1,100))\n",
    "country_counts = country_counts + df[\"Country\"].value_counts()\n",
    "country_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the `.add(...)` method rather than `+`, we can specify the `fillvalue` argument, which will give a default value to any missing countries (which we'll set to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algeria                 1\n",
       "Andorra                 1\n",
       "Antigua and Barbuda     2\n",
       "Austria                 1\n",
       "Azerbaijan              1\n",
       "                       ..\n",
       "Uzbekistan              2\n",
       "Vatican City            2\n",
       "Yemen                   4\n",
       "Zambia                  2\n",
       "Zimbabwe                2\n",
       "Name: Country, Length: 118, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get chunk 1 and count countries\n",
    "df = pd.read_csv('data.csv', nrows=100)\n",
    "country_counts = df[\"Country\"].value_counts()\n",
    "\n",
    "# get chunk 2 and count countries\n",
    "df = pd.read_csv('data.csv', nrows=100, skiprows=range(1, 100))\n",
    "chunk_counts = df[\"Country\"].value_counts()\n",
    "\n",
    "# add chunk 2 to chunk 1 with missing values set to zero\n",
    "country_counts = country_counts.add(chunk_counts, fill_value=0)\n",
    "\n",
    "country_counts.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the theory in place for how to do what we want in chunks, we can apply this to the entire CSV file, then tune the chunk size to fit into memory.\n",
    "\n",
    "The number of options available in the Pandas CSV reader is overwhelming, but you can see all of the details in [the documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). To go through the CSV file in chunks, we'll enable the `iterator` option, and provide a value for the the `chunksize` option as well. This will mean that the `read_csv` function itself will not return a DataFrame, but an iterator, that will itself return DataFrame chunks when iterated. As usual, the easiest way to go through the contents of an iterator is in a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberia                           27226\n",
      "Panama                            27150\n",
      "Belize                            27144\n",
      "Federated States of Micronesia    27139\n",
      "Cote d'Ivoire                     27126\n",
      "                                  ...  \n",
      "Lithuania                         26920\n",
      "Vatican City                      26903\n",
      "Malaysia                          26875\n",
      "Netherlands                       26874\n",
      "Belarus                           26867\n",
      "Length: 185, dtype: int64\n",
      "\n",
      "Total time: 8.1 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "country_counts = pd.Series(dtype=int)\n",
    "iterator = pd.read_csv('data.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "for df in iterator:\n",
    "    chunk_counts = df[\"Country\"].value_counts()\n",
    "    country_counts = country_counts.add(chunk_counts, fill_value=0)\n",
    "    \n",
    "print(country_counts.astype(int).sort_values(ascending=False))\n",
    "\n",
    "print()\n",
    "end_time = time.process_time()\n",
    "print(f\"Total time: {end_time-start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above still takes a few seconds to run – it's processing 5 million rows after all – but at least it can run without running out of memory. A bigger choice of chunk size might improve the performance at the cost of using more memory at once.\n",
    "\n",
    "The important thing is that provided you have written the analysis correctly, the results will always be the same. So, this is an effective way of working with large data files that you may have stored locally on your machine – it enables some analysis that would otherwise be infeasible. \n",
    "\n",
    "One key point to watch out for is that you need to design your processing carefully so that any operations are valid when the data is chunked in this way. For example, if a calculation needed information from multiple rows across multiple chunks, then we would need to consider some alternative way of handling the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
