{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Artificial Intelligence\n",
    "## Big Data – MapReduce\n",
    "### Setup\n",
    "This example will show you how you can use Apache Spark with PySpark on a local machine. This won't allow you to benefit from the massive parallelism that makes this technique so powerful, but we can still demonstrate how some code would be written, so you could then apply this knowledge to a real cluster deployment across many machines.\n",
    "\n",
    "Unlike the other examples, running this code will require you to install some new software, and since this will be platform dependant, this might require a bit of experimentation. If you'd rather not install any new software, you can still read the notebook and see the results below each cell.\n",
    "\n",
    "### Installation\n",
    "First you will need to install Apache Spark and PySpark, which may require you to install Java and/or Scala as well if you do not already. Try following the instructions on one of the sites listed below. \n",
    "\n",
    "*Note that we have not tested all of these resources, and are not responsible for the content of third-party websites. Please ensure you are comfortable with technical installations before making any modifications to your own machine. If you are unsure, ask for advice or simply read the notebook without installing anything.*\n",
    "\n",
    "* Windows 10 – [https://phoenixnap.com/kb/install-spark-on-windows-10](https://phoenixnap.com/kb/install-spark-on-windows-10)\n",
    "* macOS – [https://medium.com/swlh/pyspark-on-macos-installation-and-use-31f84ca61400](https://medium.com/swlh/pyspark-on-macos-installation-and-use-31f84ca61400)\n",
    "* Linux – [https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "You do not necessarily need to set the environment variables or run Spark from the command line as the tutorials might get you to do – just get the software installed first of all. We are going to use another Python library called `findspark` to help us use Spark without any additional configuration (though if this does not work, return to the tutorials!).\n",
    "\n",
    "To install `findspark`, use your normal Python package manager, e.g. `pip install findspark` or `conda install pyspark`.\n",
    "\n",
    "Once that is done, the code below should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Example\n",
    "First, we must create a `SparkContext`. This is the entry point to any Spark functionality. When we run a Spark application a 'driver program' starts which has the main function, and your SparkContext gets initialised here. The driver program then sends operations to be run inside the executors on worker nodes. \n",
    "\n",
    "As we are leaving everything with its default settings, the local machine will also act as the worker node, but this is where you could configure Spark to run on a real cluster of multiple worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/18 19:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our SparkContext is running, we can load some data that exists on the distributed file system. Again for our example we'll use a local text file from our machine. But we could just as easily pass in the location of a massive dataset on the distributed file system, where the actual data is split into chunks across the cluster.\n",
    "\n",
    "We'll use the PySpark `.textFile(...)` method of the SparkContext object to load our file – a text document containing the entirety of War and Peace by Leo Tolstoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "text_file = sc.textFile(\"war-and-peace.txt\")\n",
    "print(type(text_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns an object of type `RDD` which stands for Resilient Distributed Dataset. This is the fundamental data structure of Spark. An RDD is an immutable, distributed collection of objects. The `text_file` object in the cell above doesn't store the entire text file in memory, it is just a *reference* to the data which is distributed on different nodes of the cluster – or in our case, just stored on our machine. Operations are performed lazily, so nothing actually happens with this data until we try to access it.\n",
    "\n",
    "An RDD can contain any type of Python objects, including user defined classes. \n",
    "\n",
    "We can look at the contents of our RDD using the `.take(n)` function, which will return a list of the first `n` elements – again this is evaluating lazily, it will only read as much from the file on disk as it actually needs to, depending on how many lines the user has requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of War and Peace, by Leo Tolstoy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(text_file.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark makes use of RDD to achieve fast and efficient MapReduce operations. Because RDDs are immutable, when we apply transformations and actions to them this results in a new RDD object with the result of the transformation rather than an edit to the original RDD. In fact, due to the lazy evaluation, it is really just remembering the operations that it *needs* to perform once you actually request some results. \n",
    "\n",
    "We are going to count the number of times each word occurs in the file, as we showed in the unit example. So the basic idea is to perform a map which splits the text into words, another map which associates each word with the value `1`, then a reduce which combines these values per key.\n",
    "\n",
    "Remember that a `map` operation itself performs any operation over the data, so we must pass a *function* in as a parameter. Python makes this pretty easy thanks to its `lambda` functions, but you could also pass in the name of an existing or custom function.\n",
    "\n",
    "The operations are chained into one long sequence in the cell below, with some additional work to remove empty lines and convert all the words to lowercase. \n",
    "\n",
    "We start with `flatMap` because we are splitting strings into lists of strings, so we can have more than one output from a single input. `flatMap` collects all of the results into one flat structure. `.map` operations must have just one output for each input.\n",
    "\n",
    "`filter` is another common functional programming higher-order function, which is being used here to split out any empty strings from the data, which might occur due to empty lines in the original file, or because of the way the words were split.\n",
    "\n",
    "Then we use another `map` to associate each word as a key with a value of `1`. Notice that key-value pairs are simply tuples in Python.\n",
    "\n",
    "Finally we reduce the data, which will combine key-value pairs that have the same key, and here we simply add the values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = text_file.flatMap(lambda text: text.lower().split()) \\\n",
    "         .filter(lambda word: word != '') \\\n",
    "         .map(lambda word: (word, 1)) \\\n",
    "         .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above takes no time at all because, again, nothing is evaluated yet. The result of this operation, the `counts` variable, is another RDD. The data itself could be distributed across our cluster of nodes.\n",
    "\n",
    "If we want to check the results we can move the data into the driver node by calling a function called `collect()`. We only want to use the `collect` function after we have run our analysis resulting in a smaller dataset, because the whole point is that we might not be able to store the original dataset in our local memory.\n",
    "\n",
    "In fact, even though War and Peace would fit in memory in its entirety, we still don't want to print out every unique word in this notebook –  let's just look at the counts of the most frequent 10 words. \n",
    "\n",
    "In the cell below, we use `takeOrdered(...)` which allows us to take the top `n` items according to some ordering (sorting) from the RDD. The key is another lambda function: in this case defining how to order each item. We want to order by the *value* rather than the *key*, so we use element `1` rather than `0`, and we also want them in descending order, so we order by `-x[1]` rather than `x[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 34262\n",
      "and: 21403\n",
      "to: 16502\n",
      "of: 14903\n",
      "a: 10414\n",
      "he: 9297\n",
      "in: 8607\n",
      "his: 7932\n",
      "that: 7417\n",
      "was: 7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output = counts.takeOrdered(10, key = lambda x: -x[1])\n",
    "for word, count in output:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have the results of our analysis. The beauty of this approach is that this code which works on a single novel would scale up to a massive dataset containing billions of sentences – providing that we have a cluster large enough to store it, and with enough notes that we can process small chunks of the data in parallel before reducing and returning the results back to our driver node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
