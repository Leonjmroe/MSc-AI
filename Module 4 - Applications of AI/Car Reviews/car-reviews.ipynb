{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438ff605-17d9-4012-afb0-35bc1054acf5",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67c31d7-4029-4f31-bd7f-e52cdf5e9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# My custom built Naive Bayes Classifier \n",
    "from naive_bayes import NaiveBayesClassifier\n",
    "\n",
    "# Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc84ff8-868d-4c65-8090-15a3d906ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling \n",
    "\n",
    "data = pd.read_csv('car-reviews.csv')\n",
    "\n",
    "positive_data = data[691:]\n",
    "negative_data = data[:691]\n",
    "\n",
    "train_positive = positive_data[:553]\n",
    "test_positive = positive_data[553:]\n",
    "\n",
    "train_negative = negative_data[:553]\n",
    "test_negative = negative_data[553:]\n",
    "\n",
    "raw_training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "raw_testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "\n",
    "raw_training_data['Sentiment'] = np.where(raw_training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "raw_testing_data['Sentiment'] = np.where(raw_testing_data['Sentiment'] == 'Pos', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37510d9c-ca2c-4cf3-9a8f-0e190eae5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentiment_words = None \n",
    "        self.vector_sample = []\n",
    "        self.stemmed_sample = []\n",
    "        self.non_stemmed_sample = []\n",
    "\n",
    "    \n",
    "    def pre_processing(self, sentiment, review):\n",
    "        words = review.split()\n",
    "        stemmed_words = []\n",
    "        stemmed_sample = {'Sentiment': sentiment,\n",
    "                  'Word Table': {} }\n",
    "        non_stemmed_sample = {'Sentiment': sentiment,\n",
    "                              'Word Table': {} }\n",
    "    \n",
    "        # Loop through all words in sample \n",
    "        for word in words:\n",
    "\n",
    "            # Remove cappital sensitivity \n",
    "            word = word.lower()\n",
    "\n",
    "            # Removes all punctuation \n",
    "            word = ''.join(char for char in word if char not in string.punctuation)\n",
    "\n",
    "            # Remove numbers and filter out stop words \n",
    "            if word not in stop_words:\n",
    "                try:\n",
    "                    int(word)\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    non_stemmed_sample['Word Table'][word] = non_stemmed_sample['Word Table'][word] + 1 \n",
    "                except:\n",
    "                    non_stemmed_sample['Word Table'][word] = 0\n",
    "        \n",
    "                # Applies the Snoball Stemmer from the NLTK package to the list of words in the sample \n",
    "                stemmed_word = snowball_stemmer.stem(word)\n",
    "                stemmed_words.append(stemmed_word)\n",
    "    \n",
    "        # Create word count table of filtered stemmed words \n",
    "        for stemmed_word in stemmed_words:\n",
    "            try:\n",
    "                stemmed_sample['Word Table'][stemmed_word] = stemmed_sample['Word Table'][stemmed_word] + 1 \n",
    "            except:\n",
    "                stemmed_sample['Word Table'][stemmed_word] = 0\n",
    "\n",
    "        self.stemmed_sample.append(stemmed_sample)\n",
    "        self.non_stemmed_sample.append(non_stemmed_sample)\n",
    "\n",
    "        return stemmed_sample\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def word_processing(self, training_data, testing):\n",
    "        processed_samples = []\n",
    "        all_words = None\n",
    "    \n",
    "        # Run each sample though pre-processing to filter and stem the words. Table returned\n",
    "        for index, row in training_data.iterrows():\n",
    "            sentiment = row['Sentiment']\n",
    "            review = row['Review']\n",
    "            sample = self.pre_processing(sentiment, review)\n",
    "            processed_samples.append(sample)  \n",
    "\n",
    "            # Create a sample to demonstrate the code logic in categorising and counting words per sample \n",
    "            if index in [0, (len(training_data) // 2), (len(training_data) - 1)] and not testing:\n",
    "                self.vector_sample.append(training_data['Review'].iloc[index])\n",
    "\n",
    "        # Itterate through each processed sample and add new words the all_words list \n",
    "        for idx, sample in enumerate(processed_samples):\n",
    "            words = list(sample['Word Table'].keys())\n",
    "            \n",
    "            if idx == 0:\n",
    "                all_words = words\n",
    "            else:\n",
    "                for word in words:\n",
    "                    if word not in all_words:\n",
    "                        all_words.append(word)\n",
    "\n",
    "        return all_words, processed_samples\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def sentiment_filter(self, all_words):\n",
    "        \n",
    "        # Initialise the sentiment intensity analyser\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentiment_words = []\n",
    "        \n",
    "        for word in all_words:\n",
    "            # Find the sentiment intensity score of the word\n",
    "            score = sia.polarity_scores(word)['compound']\n",
    "            \n",
    "            # Filter words based on sentiment being over or under 0. 0 being a neutral word\n",
    "            if score != 0:\n",
    "                sentiment_words.append(word)\n",
    "\n",
    "        return sentiment_words\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def process_binary_data(self, processed_samples):\n",
    "        all_data = []\n",
    "        sample_data = []\n",
    "        sample_count = 0\n",
    "\n",
    "        # Convert word data to binary lists. First element is the label, the rest are the features \n",
    "        for sample in processed_samples:\n",
    "            sentiment = sample['Sentiment']\n",
    "            sample_data.append(sentiment)\n",
    "\n",
    "            '''\n",
    "            This logic below transforms the processed sample data into a binary format, which is vital for machine learning models to learn from.\n",
    "\n",
    "            The sentiment of each sample is encoded at the start of its binary list (1 for 'Pos', 0 for all others), this is the label for supervised learning. \n",
    "            Then for each sample, a binary feature vector is generated. Each element in this vector signals whether a word from the list of sentiment-relevant words \n",
    "            (self.sentiment_words) is present (1) or absent (0) in the sample.\n",
    "\n",
    "            The reason for selecting this approach is due to the consistency in feature vector length. This ensures uniformity in the length of feature vectors across \n",
    "            samples, this is vital for efficient model training and evaluation. A consistent vector size eliminates the need for adjustments based on individual sample\n",
    "            content. This approach also simplifies error handling, by assigning absent words a '0' in the feature vector, the method avoids potential errors due to missing\n",
    "            keys. This approach also utilises Python's handling of non-existent dictionary keys, which naturally avoids raising errors for such cases. As a result, this \n",
    "            eliminates the necessity for explicit error handling constructs like try-except blocks, reducing code complexity and improving readability. \n",
    "            This design also enhances the robustness and maintainability of the code by relying on Python's inherent features.\n",
    "            '''\n",
    "            \n",
    "            for word in self.sentiment_words:\n",
    "                if word in list(sample['Word Table'].keys()):\n",
    "                    sample_data.append(1)\n",
    "                else:\n",
    "                    sample_data.append(0)\n",
    "                    \n",
    "            all_data.append(sample_data) \n",
    "            sample_data = []\n",
    "    \n",
    "        return np.array(all_data)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def process(self, raw_data, testing=False):\n",
    "        # Controller function that passes the raw text data to pre-processing and filters the word table for non-neutral words and outputs a binary data table for learning\n",
    "        \n",
    "        all_words, processed_samples = self.word_processing(raw_data, testing)\n",
    "\n",
    "        if testing == False:\n",
    "            self.sentiment_words = self.sentiment_filter(all_words)\n",
    "            \n",
    "        binary_data = self.process_binary_data(processed_samples)\n",
    "        \n",
    "        return binary_data\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_vector_samples(self):\n",
    "        # 3 selected examples of my code outputting a word table during pre-processing \n",
    "        \n",
    "        print(f'**********************  Vector Samples  **********************')\n",
    "        print('\\n')\n",
    "        for sample in self.vector_sample:\n",
    "            json_str = json.dumps(sample, indent=2)\n",
    "            flattened_str = json_str.replace('\\n', ' ').replace('  ', '')\n",
    "            print(flattened_str)\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    def get_stemmed_samples(self):\n",
    "        # 5 selected examples of my code stemming words during pre-processing \n",
    "        \n",
    "        example_count = 0\n",
    "        stemmed_sample_data = [{'Index': 0,\n",
    "                               'Stemmed Word': 'work'},\n",
    "                               {'Index': 25,\n",
    "                               'Stemmed Word': 'cost'},\n",
    "                               {'Index': 42,\n",
    "                               'Stemmed Word': 'comfort'},\n",
    "                               {'Index': 97,\n",
    "                               'Stemmed Word': 'replace'},\n",
    "                               {'Index': 105,\n",
    "                               'Stemmed Word': 'handl'}]\n",
    "\n",
    "        print(f'**********************  Stemmed Samples  **********************')\n",
    "        print('\\n')\n",
    "        \n",
    "        for stemmed_data in stemmed_sample_data:\n",
    "        \n",
    "            example_count += 1\n",
    "            print(f'Stemmed Sample {example_count}')\n",
    "            print('\\n')\n",
    "            index = stemmed_data['Index']\n",
    "            word = stemmed_data['Stemmed Word']\n",
    "        \n",
    "            print('Training Data Index: ', index)\n",
    "            print('Stemmed Word: ', word)\n",
    "            print('\\n')\n",
    "        \n",
    "            for sample in self.non_stemmed_sample[index]['Word Table']:\n",
    "                if word in sample:        \n",
    "                    count = self.non_stemmed_sample[index]['Word Table'][sample]\n",
    "                    print('Pre-Stemmed Word: ', sample)\n",
    "                    \n",
    "            print('\\n')\n",
    "            print('Pre-Stemmed Word Table:')\n",
    "            print('\\n')\n",
    "            print(self.non_stemmed_sample[index]['Word Table'])\n",
    "            print('\\n')\n",
    "            print('Stemmed Word Table:')\n",
    "            print('\\n')\n",
    "            print(self.stemmed_sample[index]['Word Table'])\n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a5a5ee-7be9-4402-affc-28cf5d95b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>115</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>47</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 115                  23\n",
       "Actual Positive                  47                  91"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy: 0.746\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_data(processor):\n",
    "    \n",
    "    training_data = processor.process(raw_training_data, testing=False)\n",
    "    testing_data = processor.process(raw_testing_data, testing=True)\n",
    "    \n",
    "    return training_data, testing_data\n",
    "\n",
    "\n",
    "\n",
    "def custom_bayes(training_data, testing_data):\n",
    "    \n",
    "    naive_bayes = NaiveBayesClassifier(training_data, testing_data)\n",
    "    predictions = naive_bayes.run()\n",
    "\n",
    "    conf_matrix = confusion_matrix(testing_data[:, 0], predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "    display(conf_matrix_df)\n",
    "    print('\\n')\n",
    "    \n",
    "    accuracy = np.sum(predictions == testing_data[:, 0]) / len(testing_data[:, 0])\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def show_samples(processor):\n",
    "    \n",
    "    processor.get_vector_samples()\n",
    "    processor.get_stemmed_samples()\n",
    "\n",
    "\n",
    "\n",
    "processor = Processor()\n",
    "training_data, testing_data = process_data(processor)\n",
    "custom_bayes(training_data, testing_data)\n",
    "show_samples(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b2b2b-3ae8-4039-b7a6-845bcff3cf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f4e9ea6-4a4a-4890-a007-ca3350f15e3d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f020296-2cb8-4336-b805-5d7ae20ccbf6",
   "metadata": {},
   "source": [
    "### The Approach\n",
    "\n",
    "\n",
    "When shifting focus from the Bayesian model in Task 1 to the LSTM Recurrent Neural Network (RNN) model for Task 2, my objective was to harness the sophisticated capabilities of Long Short-Term Memory (LSTM) networks, a specific category of RNNs, to markedly improve our Natural Language Processing (NLP) methods. The core goal centered around addressing the limitations of the Bayesian approach, particularly its dependency on manually curated features, including word stemming and sentiment analysis. I anticipate that a deep learning model capable of independently learning and identifying features from unprocessed text data will prove to be more robust.\n",
    "\n",
    "The Bayesian model, while effective in certain scenarios, fundamentally treats input features independently, which poses a challenge in NLP tasks where the sequential and contextual nature of language is vital. In contrast, LSTM networks are cleverly designed to address these challenges, boasting an architecture that enables them to remember information over extended sequences. This is achieved through their unique composition of memory cells and a sophisticated system of gates. Including input, forget, and output gates. These regulate the flow of information. Such an arrangement allows LSTMs not only to maintain crucial contextual information across long pasages of text but also to dynamically adapt to new inputs, effectively capturing the intricacies and nuances of natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f72e2-8faf-4e9c-81b6-bb1dcde2eda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3d5bd9e-33fb-4dec-8181-dd4f9b46a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccceb103-113c-4366-ba81-8712e69f7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('car-reviews.csv')\n",
    "    \n",
    "    # Splitting the data into positive and negative reviews based on a predetermined index\n",
    "    positive_data = data[691:]  \n",
    "    negative_data = data[:691]  \n",
    "    \n",
    "    # Further split positive and negative datasets into training and testing subsets\n",
    "    train_positive = positive_data[:553]\n",
    "    test_positive = positive_data[553:] \n",
    "    train_negative = negative_data[:553]\n",
    "    test_negative = negative_data[553:]  \n",
    "    \n",
    "    # Concatenate the positive and negative training data, and do the same for testing data\n",
    "    training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert the 'Sentiment' column into a binary format: 1 for positive, 0 for negative\n",
    "    training_data['Sentiment'] = np.where(training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "    testing_data['Sentiment'] = np.where(testing_data['Sentiment'] == 'Pos', 1, 0)\n",
    "\n",
    "    # Extract reviews (as text) and their corresponding labels for training and testing\n",
    "    training_texts = list(training_data['Review'])\n",
    "    training_labels = list(training_data['Sentiment'])\n",
    "    test_texts = list(testing_data['Review'])\n",
    "    test_labels = list(testing_data['Sentiment'])\n",
    "\n",
    "    # Return the processed text and labels for training and testing\n",
    "    return training_texts, training_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d548080-6756-429e-8924-170b4240ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "    def tokenisation():\n",
    "        \n",
    "        training_texts, training_labels, test_texts, test_labels = prepare_data()\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=10000)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(training_texts)  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(training_texts)  # Transforms each text in training_texts to a sequence of integers\n",
    "        train_data = pad_sequences(train_sequences, maxlen=100)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        train_labels = np.array(training_labels)  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(test_texts) \n",
    "        test_data = pad_sequences(test_sequences, maxlen=100) \n",
    "        test_labels = np.array(test_labels)  \n",
    "    \n",
    "        return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def LTSM_model(train_data, train_labels, test_data, test_labels, embedding_dim, lstm_unit, dropout_rate, reg_strength):\n",
    "    \n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - embedding_dim: Dimension of the embedding layer\n",
    "            - lstm_unit: Number of units in the LSTM layer\n",
    "            - dropout_rate: Dropout rate for regularization\n",
    "            - reg_strength: Strength of L2 regularization\n",
    "        \"\"\"\n",
    "    \n",
    "        model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data (100 set = sequence length)\n",
    "            Input(shape=(100,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=10000, output_dim=embedding_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(lstm_unit, dropout=dropout_rate, recurrent_dropout=dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))\n",
    "        ])\n",
    "    \n",
    "        if load = True:\n",
    "            model = load_model(model)\n",
    "        else:\n",
    "            model = compile_model(model)\n",
    "    \n",
    "        evaluate_model(model)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate_model():\n",
    "    \n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_predictions_proba = model.predict(test_data)\n",
    "        test_predictions = (test_predictions_proba > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(test_labels, test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        graphing(history)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def complile_model():\n",
    "    \n",
    "        # Compile the model with Adam optimizer and binary crossentropy loss, tracking accuracy\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        # Save the model params and weight with the lowest validation loss and add early stopping if validation loss does not decrease for 2 epochs \n",
    "        checkpoint_filepath = os.getcwd() + '/LSTM_Model_Checkpoint.keras'\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
    "        callbacks = [EarlyStopping(patience=2), model_checkpoint_callback]\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting\n",
    "        history = model.fit(train_data, train_labels, batch_size=32, epochs=20, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model():\n",
    "    \n",
    "        model.load_weights('LSTM_Model_Checkpoint.keras')\n",
    "    \n",
    "        # Example: Visualize the weights of the first Dense layer\n",
    "        dense_layer_weights = model.layers[2].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        plt.hist(dense_layer_weights.flatten())\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Weight Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def graphing(self, history):\n",
    "    \n",
    "        # Training history data\n",
    "        training_accuracy = history.history['accuracy']\n",
    "        validation_accuracy = history.history['val_accuracy']\n",
    "        training_loss = history.history['loss']\n",
    "        validation_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Plotting accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o')\n",
    "        plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plotting loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "        plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        train_data, train_labels, test_data, test_labels = tokenisation()\n",
    "        LTSM_model(train_data, train_labels, test_data, test_labels, embedding_dim = 62, lstm_unit = 48, dropout_rate = 0.2, reg_strength = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f4eef-6bf1-4a63-88dc-f959fed3e84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53a25575-7c87-4e1a-9af2-5fe01fb0fc8f",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b7195-7bea-4a58-9ef2-efec78b2096b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10385a-7b49-4f31-9c4b-8274fa218dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
