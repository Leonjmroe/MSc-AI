{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "b67c31d7-4029-4f31-bd7f-e52cdf5e9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/leon/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# My custom built Naive Bayes Classifier \n",
    "from naive_bayes import NaiveBayesClassifier\n",
    "\n",
    "# Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "7bc84ff8-868d-4c65-8090-15a3d906ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling \n",
    "\n",
    "data = pd.read_csv('car-reviews.csv')\n",
    "\n",
    "positive_data = data[691:]\n",
    "negative_data = data[:691]\n",
    "\n",
    "train_positive = positive_data[:553]\n",
    "test_positive = positive_data[553:]\n",
    "\n",
    "train_negative = negative_data[:553]\n",
    "test_negative = negative_data[553:]\n",
    "\n",
    "raw_training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "raw_testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "\n",
    "raw_training_data['Sentiment'] = np.where(raw_training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "raw_testing_data['Sentiment'] = np.where(raw_testing_data['Sentiment'] == 'Pos', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "37510d9c-ca2c-4cf3-9a8f-0e190eae5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentiment_words = None \n",
    "        self.vector_sample = []\n",
    "        self.stemmed_sample = []\n",
    "        self.non_stemmed_sample = []\n",
    "\n",
    "    \n",
    "    def pre_processing(self, sentiment, review):\n",
    "        words = review.split()\n",
    "        stemmed_words = []\n",
    "        stemmed_sample = {'Sentiment': sentiment,\n",
    "                  'Word Table': {} }\n",
    "        non_stemmed_sample = {'Sentiment': sentiment,\n",
    "                              'Word Table': {} }\n",
    "    \n",
    "        # Loop through all words in sample \n",
    "        for word in words:\n",
    "\n",
    "            # Remove cappital sensitivity \n",
    "            word = word.lower()\n",
    "\n",
    "            # Removes all punctuation \n",
    "            word = ''.join(char for char in word if char not in string.punctuation)\n",
    "\n",
    "            # Remove numbers and filter out stop words \n",
    "            if word not in stop_words:\n",
    "                try:\n",
    "                    int(word)\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    non_stemmed_sample['Word Table'][word] = non_stemmed_sample['Word Table'][word] + 1 \n",
    "                except:\n",
    "                    non_stemmed_sample['Word Table'][word] = 0\n",
    "        \n",
    "                # Applies the Snoball Stemmer from the NLTK package to the list of words in the sample \n",
    "                stemmed_word = snowball_stemmer.stem(word)\n",
    "                stemmed_words.append(stemmed_word)\n",
    "    \n",
    "        # Create word count table of filtered stemmed words \n",
    "        for stemmed_word in stemmed_words:\n",
    "            try:\n",
    "                stemmed_sample['Word Table'][stemmed_word] = stemmed_sample['Word Table'][stemmed_word] + 1 \n",
    "            except:\n",
    "                stemmed_sample['Word Table'][stemmed_word] = 0\n",
    "\n",
    "        self.stemmed_sample.append(stemmed_sample)\n",
    "        self.non_stemmed_sample.append(non_stemmed_sample)\n",
    "\n",
    "        return stemmed_sample\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def word_processing(self, training_data, testing):\n",
    "        processed_samples = []\n",
    "        all_words = None\n",
    "    \n",
    "        # Run each sample though pre-processing to filter and stem the words. Table returned\n",
    "        for index, row in training_data.iterrows():\n",
    "            sentiment = row['Sentiment']\n",
    "            review = row['Review']\n",
    "            sample = self.pre_processing(sentiment, review)\n",
    "            processed_samples.append(sample)  \n",
    "\n",
    "            # Create a sample to demonstrate the code logic in categorising and counting words per sample \n",
    "            if index in [0, (training_data.count()[0] / 2), (training_data.count()[0] - 1)] and testing == False:\n",
    "                self.vector_sample.append(sample)\n",
    "\n",
    "        # Itterate through each processed sample and add new words the all_words list \n",
    "        for idx, sample in enumerate(processed_samples):\n",
    "            words = list(sample['Word Table'].keys())\n",
    "            \n",
    "            if idx == 0:\n",
    "                all_words = words\n",
    "            else:\n",
    "                for word in words:\n",
    "                    if word not in all_words:\n",
    "                        all_words.append(word)\n",
    "\n",
    "        return all_words, processed_samples\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def sentiment_filter(self, all_words):\n",
    "        \n",
    "        # Initialise the sentiment intensity analyser\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        sentiment_words = []\n",
    "        \n",
    "        for word in all_words:\n",
    "            # Find the sentiment intensity score of the word\n",
    "            score = sia.polarity_scores(word)['compound']\n",
    "            \n",
    "            # Filter words based on sentiment being over or under 0. 0 being a neutral word\n",
    "            if score != 0:\n",
    "                sentiment_words.append(word)\n",
    "\n",
    "        return sentiment_words\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def process_binary_data(self, processed_samples):\n",
    "        all_data = []\n",
    "        sample_data = []\n",
    "        sample_count = 0\n",
    "\n",
    "        # Convert word data to binary lists. First element is the label, the rest are the features \n",
    "        for sample in processed_samples:\n",
    "            sentiment = sample['Sentiment']\n",
    "            sample_data.append(sentiment)\n",
    "\n",
    "            '''\n",
    "            This logic below transforms the processed sample data into a binary format, which is vital for machine learning models to learn from.\n",
    "\n",
    "            The sentiment of each sample is encoded at the start of its binary list (1 for 'Pos', 0 for all others), this is the label for supervised learning. \n",
    "            Then for each sample, a binary feature vector is generated. Each element in this vector signals whether a word from the list of sentiment-relevant words \n",
    "            (self.sentiment_words) is present (1) or absent (0) in the sample.\n",
    "\n",
    "            The reason for selecting this approach is due to the consistency in feature vector length. This ensures uniformity in the length of feature vectors across \n",
    "            samples, this is vital for efficient model training and evaluation. A consistent vector size eliminates the need for adjustments based on individual sample\n",
    "            content. This approach also simplifies error handling, by assigning absent words a '0' in the feature vector, the method avoids potential errors due to missing\n",
    "            keys. This approach also utilises Python's handling of non-existent dictionary keys, which naturally avoids raising errors for such cases. As a result, this \n",
    "            eliminates the necessity for explicit error handling constructs like try-except blocks, reducing code complexity and improving readability. \n",
    "            This design also enhances the robustness and maintainability of the code by relying on Python's inherent features.\n",
    "            '''\n",
    "            \n",
    "            for word in self.sentiment_words:\n",
    "                if word in list(sample['Word Table'].keys()):\n",
    "                    sample_data.append(1)\n",
    "                else:\n",
    "                    sample_data.append(0)\n",
    "                    \n",
    "            all_data.append(sample_data) \n",
    "            sample_data = []\n",
    "    \n",
    "        return np.array(all_data)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def process(self, raw_data, testing=False):\n",
    "        # Controller function that passes the raw text data to pre-processing and filters the word table for non-neutral words and outputs a binary data table for learning\n",
    "        \n",
    "        all_words, processed_samples = self.word_processing(raw_data, testing)\n",
    "\n",
    "        if testing == False:\n",
    "            self.sentiment_words = self.sentiment_filter(all_words)\n",
    "            \n",
    "        binary_data = self.process_binary_data(processed_samples)\n",
    "        \n",
    "        return binary_data\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_vector_samples(self):\n",
    "        # 3 selected examples of my code outputting a word table during pre-processing \n",
    "        \n",
    "        print(f'**********************  Vector Samples  **********************')\n",
    "        print('\\n')\n",
    "        for sample in self.vector_sample:\n",
    "            json_str = json.dumps(sample, indent=2)\n",
    "            flattened_str = json_str.replace('\\n', ' ').replace('  ', '')\n",
    "            print(flattened_str)\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    def get_stemmed_samples(self):\n",
    "        # 5 selected examples of my code stemming words during pre-processing \n",
    "        \n",
    "        example_count = 0\n",
    "        stemmed_sample_data = [{'Index': 0,\n",
    "                               'Stemmed Word': 'work'},\n",
    "                               {'Index': 25,\n",
    "                               'Stemmed Word': 'cost'},\n",
    "                               {'Index': 42,\n",
    "                               'Stemmed Word': 'comfort'},\n",
    "                               {'Index': 97,\n",
    "                               'Stemmed Word': 'replace'},\n",
    "                               {'Index': 105,\n",
    "                               'Stemmed Word': 'handl'}]\n",
    "\n",
    "        print(f'**********************  Stemmed Samples  **********************')\n",
    "        print('\\n')\n",
    "        \n",
    "        for stemmed_data in stemmed_sample_data:\n",
    "        \n",
    "            example_count += 1\n",
    "            print(f'Stemmed Sample {example_count}')\n",
    "            print('\\n')\n",
    "            index = stemmed_data['Index']\n",
    "            word = stemmed_data['Stemmed Word']\n",
    "        \n",
    "            print('Training Data Index: ', index)\n",
    "            print('Stemmed Word: ', word)\n",
    "            print('\\n')\n",
    "        \n",
    "            for sample in self.non_stemmed_sample[index]['Word Table']:\n",
    "                if word in sample:        \n",
    "                    count = self.non_stemmed_sample[index]['Word Table'][sample]\n",
    "                    print('Pre-Stemmed Word: ', sample)\n",
    "                    \n",
    "            print('\\n')\n",
    "            print('Pre-Stemmed Word Table:')\n",
    "            print('\\n')\n",
    "            print(self.non_stemmed_sample[index]['Word Table'])\n",
    "            print('\\n')\n",
    "            print('Stemmed Word Table:')\n",
    "            print('\\n')\n",
    "            print(self.stemmed_sample[index]['Word Table'])\n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "02a5a5ee-7be9-4402-affc-28cf5d95b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>115</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>47</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                 115                  23\n",
       "Actual Positive                  47                  91"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy: 0.746\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \n",
    "    processor = Processer()\n",
    "    training_data = processor.process(raw_training_data, testing=False)\n",
    "    testing_data = processor.process(raw_testing_data, testing=True)\n",
    "    \n",
    "    return training_data, testing_data\n",
    "\n",
    "\n",
    "\n",
    "def custom_bayes(training_data, testing_data):\n",
    "    \n",
    "    naive_bayes = NaiveBayesClassifier(training_data, testing_data)\n",
    "    predictions = naive_bayes.run()\n",
    "\n",
    "    conf_matrix = confusion_matrix(testing_data[:, 0], predictions)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "    display(conf_matrix_df)\n",
    "    print('\\n')\n",
    "    \n",
    "    accuracy = np.sum(predictions == testing_data[:, 0]) / len(testing_data[:, 0])\n",
    "    print(f\"Accuracy: {round(accuracy, 3)}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def show_samples():\n",
    "    \n",
    "    processor.get_vector_samples()\n",
    "    processor.get_stemmed_samples()\n",
    "\n",
    "\n",
    "\n",
    "training_data, testing_data = process_data()\n",
    "custom_bayes(training_data, testing_data)\n",
    "# show_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b2b2b-3ae8-4039-b7a6-845bcff3cf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
