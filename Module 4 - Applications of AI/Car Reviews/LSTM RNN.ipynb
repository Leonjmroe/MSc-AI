{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5424172a-daa1-4f61-a09e-b0254855ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f18501-5680-499f-b429-d6a1455ce8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('car-reviews.csv')\n",
    "    \n",
    "    # Splitting the data into positive and negative reviews based on a predetermined index\n",
    "    positive_data = data[691:]  \n",
    "    negative_data = data[:691]  \n",
    "    \n",
    "    # Further split positive and negative datasets into training and testing subsets\n",
    "    train_positive = positive_data[:553]\n",
    "    test_positive = positive_data[553:] \n",
    "    train_negative = negative_data[:553]\n",
    "    test_negative = negative_data[553:]  \n",
    "    \n",
    "    # Concatenate the positive and negative training data, and do the same for testing data\n",
    "    training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert the 'Sentiment' column into a binary format: 1 for positive, 0 for negative\n",
    "    training_data['Sentiment'] = np.where(training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "    testing_data['Sentiment'] = np.where(testing_data['Sentiment'] == 'Pos', 1, 0)\n",
    "\n",
    "    # Extract reviews (as text) and their corresponding labels for training and testing\n",
    "    training_texts = list(training_data['Review'])\n",
    "    training_labels = list(training_data['Sentiment'])\n",
    "    test_texts = list(testing_data['Review'])\n",
    "    test_labels = list(testing_data['Sentiment'])\n",
    "\n",
    "    # Return the processed text and labels for training and testing\n",
    "    return [training_texts, training_labels, test_texts, test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98dd9bb-298a-49ee-acb8-eb677f0720a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier():\n",
    "\n",
    "    self.model = None\n",
    "\n",
    "    def __init__(self, data, load_model, embedding_dim, lstm_unit, dropout_rate, reg_strength, epochs, batch_size, dense_output_activation_function, model_optimisation, model_loss_function):\n",
    "\n",
    "        self.data = data\n",
    "        self.load_model = load_model\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.validation_split = validation_split\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_unit = lstm_unit\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dense_output_activation_function = dense_output_activation_function\n",
    "        self.model_optimisation = model_optimisation\n",
    "        self.model_loss_function = model_loss_function \n",
    "\n",
    "    \n",
    "    \n",
    "    def tokenisation(self):\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=10000)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(self.data[0])  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(self.tr_data)  # Transforms each text in training_texts to a sequence of integers\n",
    "        self.data[0] = pad_sequences(train_sequences, maxlen=100)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        self.data[1] = np.array(self.data[1])  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(self.data[2]) \n",
    "        self.data[2] = pad_sequences(test_sequences, maxlen=100) \n",
    "        self.data[3] = np.array(self.data[3])  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def LTSM_model(self):\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data (100 set = sequence length)\n",
    "            Input(shape=(100,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=10000, output_dim=self.ebedding_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(self.lstm_unit, dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation=self.activation_function, kernel_regularizer=l2(self.reg_strength))\n",
    "        ])\n",
    "    \n",
    "        if self.load_model == True:\n",
    "            model = load_model()\n",
    "        else:\n",
    "            model = compile_model()\n",
    "\n",
    "        self.evaluate_model(self.model)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "    \n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_predictions_proba = self.model.predict(self.data[2])\n",
    "        test_predictions = (test_predictions_proba > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        conf_matrix = confusion_matrix(self.data[3], test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(self.data[3], test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        self.graphing(history)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def complile_model(self):\n",
    "    \n",
    "        # Compile the model with Adam optimizer and binary crossentropy loss, tracking accuracy\n",
    "        self.model.compile(optimizer=self.model_optimisation, loss=self.model_loss_function, metrics=['accuracy'])\n",
    "    \n",
    "        # Save the model params and weight with the lowest validation loss and add early stopping if validation loss does not decrease for 2 epochs \n",
    "        checkpoint_filepath = os.getcwd() + '/LSTM_Model_Checkpoint.keras'\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
    "        callbacks = [EarlyStopping(patience=self.early_stopping_patience), model_checkpoint_callback]\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting\n",
    "        history = model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=self.validation_split, callbacks=callbacks)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "    \n",
    "        model.load_weights('LSTM_Model_Checkpoint.keras')\n",
    "    \n",
    "        # Example: Visualize the weights of the first Dense layer\n",
    "        dense_layer_weights = model.layers[2].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        plt.hist(dense_layer_weights.flatten())\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Weight Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def graphing(self, history):\n",
    "    \n",
    "        # Training history data\n",
    "        training_accuracy = history.history['accuracy']\n",
    "        validation_accuracy = history.history['val_accuracy']\n",
    "        training_loss = history.history['loss']\n",
    "        validation_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Plotting accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o')\n",
    "        plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plotting loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "        plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.tokenisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282c1c36-d54d-4e2f-84e5-cf8f83785c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM_Classifier' object has no attribute 'tr_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m prepare_data()\n\u001b[1;32m      3\u001b[0m lstm_classifier \u001b[38;5;241m=\u001b[39m LSTM_Classifier(data, \n\u001b[1;32m      4\u001b[0m                                   load_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                                   embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m62\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                   model_loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m                                  )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mlstm_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 155\u001b[0m, in \u001b[0;36mLSTM_Classifier.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenisation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mLSTM_Classifier.tokenisation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Updates internal vocabulary based on the list of texts\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the list of texts to a sequence of integers\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m train_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_data\u001b[49m)  \u001b[38;5;66;03m# Transforms each text in training_texts to a sequence of integers\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m pad_sequences(train_sequences, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Ensures all sequences in a list have the same length by padding/truncating\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m1\u001b[39m])  \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM_Classifier' object has no attribute 'tr_data'"
     ]
    }
   ],
   "source": [
    "data = prepare_data()\n",
    "\n",
    "lstm_classifier = LSTM_Classifier(data, \n",
    "                                  load_model = False,\n",
    "                                  early_stopping_patience = 2,\n",
    "                                  validation_split = 0.2,\n",
    "                                  embedding_dim = 62,\n",
    "                                  lstm_unit = 48,\n",
    "                                  dropout_rate = 0.2,\n",
    "                                  reg_strength = 0.3,\n",
    "                                  epochs = 20,\n",
    "                                  batch_size = 32,\n",
    "                                  dense_output_activation_function = 'sigmoid',\n",
    "                                  model_optimisation = 'adam',\n",
    "                                  model_loss_function = 'binary_crossentropy',\n",
    "                                 )\n",
    "\n",
    "lstm_classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5421fe-3795-44ae-bbdb-083f78264012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
