{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5424172a-daa1-4f61-a09e-b0254855ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32f18501-5680-499f-b429-d6a1455ce8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('car-reviews.csv')\n",
    "    \n",
    "    # Splitting the data into positive and negative reviews based on a predetermined index\n",
    "    positive_data = data[691:]  \n",
    "    negative_data = data[:691]  \n",
    "    \n",
    "    # Further split positive and negative datasets into training and testing subsets\n",
    "    train_positive = positive_data[:553]\n",
    "    test_positive = positive_data[553:] \n",
    "    train_negative = negative_data[:553]\n",
    "    test_negative = negative_data[553:]  \n",
    "    \n",
    "    # Concatenate the positive and negative training data, and do the same for testing data\n",
    "    training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert the 'Sentiment' column into a binary format: 1 for positive, 0 for negative\n",
    "    training_data['Sentiment'] = np.where(training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "    testing_data['Sentiment'] = np.where(testing_data['Sentiment'] == 'Pos', 1, 0)\n",
    "\n",
    "    # Extract reviews (as text) and their corresponding labels for training and testing\n",
    "    training_texts = list(training_data['Review'])\n",
    "    training_labels = list(training_data['Sentiment'])\n",
    "    test_texts = list(testing_data['Review'])\n",
    "    test_labels = list(testing_data['Sentiment'])\n",
    "\n",
    "    # Return the processed text and labels for training and testing\n",
    "    return list((training_texts, training_labels, test_texts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f98dd9bb-298a-49ee-acb8-eb677f0720a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier():\n",
    "\n",
    "    model = None\n",
    "    history = None\n",
    "    file_id = None\n",
    "\n",
    "    def __init__(self, data, early_stopping_patience, validation_split, tokeniser_word_count, pad_sequences_maxlen, embedding_input_dim, \n",
    "                 embedding_output_dim, lstm_unit, dropout_rate, reg_strength, epochs, batch_size, dense_output_activation_function, learning_rate, model_loss_function):\n",
    "\n",
    "        self.data = data\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.tokeniser_word_count = tokeniser_word_count\n",
    "        self.pad_sequences_maxlen = pad_sequences_maxlen\n",
    "\n",
    "        self.embedding_input_dim = embedding_input_dim\n",
    "        self.embedding_output_dim = embedding_output_dim\n",
    "        self.lstm_unit = lstm_unit\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dense_output_activation_function = dense_output_activation_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_loss_function = model_loss_function \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def tokenisation(self):\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=self.tokeniser_word_count)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(self.data[0])  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(self.data[0])  # Transforms each text in training_texts to a sequence of integers\n",
    "        self.data[0] = pad_sequences(train_sequences, maxlen=self.pad_sequences_maxlen)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        self.data[1] = np.array(self.data[1])  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(self.data[2]) \n",
    "        self.data[2] = pad_sequences(test_sequences, maxlen=self.pad_sequences_maxlen) \n",
    "        self.data[3] = np.array(self.data[3])  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def run_model(self):\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data = pad_sequences_maxlen\n",
    "            Input(shape=(self.pad_sequences_maxlen,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=self.embedding_input_dim, output_dim=self.embedding_output_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(self.lstm_unit, dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation=self.dense_output_activation_function, kernel_regularizer=l2(self.reg_strength))\n",
    "        ])\n",
    "\n",
    "        \n",
    "        if self.file_id is not None:\n",
    "            model = self.load_model()\n",
    "        else:\n",
    "            model = self.train_model()\n",
    "\n",
    "        self.evaluate_model()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "    \n",
    "        unique_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save the model params and weight with the lowest validation loss and add early stopping if validation loss does not decrease for 2 epochs \n",
    "        checkpoint_filepath = os.getcwd() + f'/LSTM_Models/Weights/LSTM_Model_{unique_id}.keras'\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
    "        callbacks = [EarlyStopping(patience=self.early_stopping_patience), model_checkpoint_callback]\n",
    "\n",
    "        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=self.model_loss_function, metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting\n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=self.validation_split, callbacks=callbacks)\n",
    "\n",
    "        # Save the history file\n",
    "        with open(os.getcwd() + f'/LSTM_Models/histories/LSTM_History_{unique_id}.pkl', 'wb') as history_file:\n",
    "            pickle.dump(self.history.history, history_file)\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "    \n",
    "        self.model.load_weights(os.getcwd() + f'/LSTM_Models/Weights/LSTM_Model_{self.file_id}.keras')\n",
    "        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=self.model_loss_function, metrics=['accuracy'])\n",
    "\n",
    "        with open(os.getcwd() + f'/LSTM_Models/Histories/LSTM_History_{self.file_id}.pkl', 'rb') as history_file:\n",
    "            self.history = pickle.load(history_file)\n",
    "\n",
    "        # New history with new weights - overwrites above, which loads history snapshot \n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, \\\n",
    "                                      validation_split=self.validation_split, callbacks=[EarlyStopping(patience=self.early_stopping_patience)])\n",
    "\n",
    "        self.file_id = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def evaluate_model(self):\n",
    "\n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_probabilities = self.model.predict(self.data[2])\n",
    "        test_predictions = (test_probabilities > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        conf_matrix = confusion_matrix(self.data[3], test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(self.data[3], test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        self.visualise_accuracy()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def visualise_model_params(self):\n",
    "        \n",
    "        # Example: Visualize the weights of the first Dense layer\n",
    "        dense_layer_weights = model.layers[2].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        plt.hist(dense_layer_weights.flatten())\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Weight Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def visualise_accuracy(self):\n",
    "        \n",
    "        if self.file_id is not None:\n",
    "            training_accuracy = self.history['accuracy']\n",
    "            validation_accuracy = self.history['val_accuracy']\n",
    "            training_loss = self.history['loss']\n",
    "            validation_loss = self.history['val_loss']\n",
    "        else:\n",
    "            training_accuracy = self.history.history['accuracy']\n",
    "            validation_accuracy = self.history.history['val_accuracy']\n",
    "            training_loss = self.history.history['loss']\n",
    "            validation_loss = self.history.history['val_loss']\n",
    "        \n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "        \n",
    "        with plt.style.context('dark_background'):\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            # Plotting training and validation accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plotting training and validation loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, training_loss, label='Training Loss', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_loss, label='Validation Loss', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, file_id = None):\n",
    "        self.file_id = file_id\n",
    "        self.tokenisation()\n",
    "        self.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c1c36-d54d-4e2f-84e5-cf8f83785c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 2/28\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.5547 - loss: 0.6942"
     ]
    }
   ],
   "source": [
    "data = prepare_data()\n",
    "\n",
    "lstm_classifier = LSTM_Classifier(data, \n",
    "                                  early_stopping_patience = 3,\n",
    "                                  validation_split = 0.2,\n",
    "                                  tokeniser_word_count = 10000,\n",
    "                                  pad_sequences_maxlen = 100,\n",
    "                                  embedding_input_dim = 10000,\n",
    "                                  embedding_output_dim = 64,\n",
    "                                  lstm_unit = 128,\n",
    "                                  dropout_rate = 0.128,\n",
    "                                  reg_strength = 0.001,\n",
    "                                  epochs = 20,\n",
    "                                  batch_size = 32,\n",
    "                                  learning_rate = 0.0018,\n",
    "                                  dense_output_activation_function = 'sigmoid',\n",
    "                                  model_loss_function = 'binary_crossentropy',\n",
    "                                 )\n",
    "\n",
    "lstm_classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d151b-17da-44e0-86b7-3a4d2db463a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45c7c4af-30bc-4888-9b01-b55a42bdf45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Optimization:   0%|                              | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Optuna Optimization:  10%|██▏                   | 1/10 [00:06<01:02,  6.99s/it]\u001b[A\n",
      "Optuna Optimization:  20%|████▍                 | 2/10 [00:14<00:56,  7.05s/it]\u001b[A\n",
      "Optuna Optimization:  30%|██████▌               | 3/10 [00:21<00:48,  7.00s/it]\u001b[A\n",
      "Optuna Optimization:  40%|████████▊             | 4/10 [00:27<00:41,  6.85s/it]\u001b[A\n",
      "Optuna Optimization:  50%|███████████           | 5/10 [00:37<00:39,  7.97s/it]\u001b[A\n",
      "Optuna Optimization:  60%|█████████████▏        | 6/10 [00:47<00:33,  8.49s/it]\u001b[A\n",
      "Optuna Optimization:  70%|███████████████▍      | 7/10 [00:53<00:23,  7.89s/it]\u001b[A\n",
      "Optuna Optimization:  80%|█████████████████▌    | 8/10 [01:03<00:16,  8.46s/it]\u001b[A\n",
      "Optuna Optimization:  90%|███████████████████▊  | 9/10 [01:10<00:07,  7.95s/it]\u001b[A\n",
      "Optuna Optimization: 100%|█████████████████████| 10/10 [01:19<00:00,  7.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 10\n",
      "Best trial: {'learning_rate': 0.0018571478288567467}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Adjust Optuna verbosity\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    # Hyperparameters to be tuned using suggest_float for continuous and suggest_categorical for discrete\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=64),\n",
    "        Bidirectional(LSTM(128, dropout=0.12)),\n",
    "        Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    history = model.fit(data[0], data[1], batch_size=32, epochs=20, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    _, accuracy = model.evaluate(data[2], data[3], verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    trials = 10\n",
    "    with tqdm(total=trials, desc=\"Optuna Optimization\") as bar:\n",
    "        def optuna_progress_bar(study, trial):\n",
    "            bar.update(1)\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=trials, callbacks=[optuna_progress_bar])\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfff36c-2443-4a75-a45e-2b6a4d8ed989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d1b5c-4bf4-4fd3-9f9a-33ff03ae9e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
