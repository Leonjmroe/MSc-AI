{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5424172a-daa1-4f61-a09e-b0254855ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f18501-5680-499f-b429-d6a1455ce8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('car-reviews.csv')\n",
    "    \n",
    "    # Splitting the data into positive and negative reviews based on a predetermined index\n",
    "    positive_data = data[691:]  \n",
    "    negative_data = data[:691]  \n",
    "    \n",
    "    # Further split positive and negative datasets into training and testing subsets\n",
    "    train_positive = positive_data[:553]\n",
    "    test_positive = positive_data[553:] \n",
    "    train_negative = negative_data[:553]\n",
    "    test_negative = negative_data[553:]  \n",
    "    \n",
    "    # Concatenate the positive and negative training data, and do the same for testing data\n",
    "    training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert the 'Sentiment' column into a binary format: 1 for positive, 0 for negative\n",
    "    training_data['Sentiment'] = np.where(training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "    testing_data['Sentiment'] = np.where(testing_data['Sentiment'] == 'Pos', 1, 0)\n",
    "\n",
    "    # Extract reviews (as text) and their corresponding labels for training and testing\n",
    "    training_texts = list(training_data['Review'])\n",
    "    training_labels = list(training_data['Sentiment'])\n",
    "    test_texts = list(testing_data['Review'])\n",
    "    test_labels = list(testing_data['Sentiment'])\n",
    "\n",
    "    # Return the processed text and labels for training and testing\n",
    "    return list((training_texts, training_labels, test_texts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f98dd9bb-298a-49ee-acb8-eb677f0720a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier():\n",
    "\n",
    "    model = None\n",
    "    history = None\n",
    "    file_id = None\n",
    "\n",
    "    def __init__(self, data, early_stopping_patience, validation_split, tokeniser_word_count, pad_sequences_maxlen, embedding_input_dim, \n",
    "                 embedding_output_dim, lstm_unit, dropout_rate, reg_strength, epochs, batch_size, dense_output_activation_function, model_optimisation, model_loss_function):\n",
    "\n",
    "        self.data = data\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.tokeniser_word_count = tokeniser_word_count\n",
    "        self.pad_sequences_maxlen = pad_sequences_maxlen\n",
    "\n",
    "        self.embedding_input_dim = embedding_input_dim\n",
    "        self.embedding_output_dim = embedding_output_dim\n",
    "        self.lstm_unit = lstm_unit\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dense_output_activation_function = dense_output_activation_function\n",
    "        self.model_optimisation = model_optimisation\n",
    "        self.model_loss_function = model_loss_function \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def tokenisation(self):\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=self.tokeniser_word_count)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(self.data[0])  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(self.data[0])  # Transforms each text in training_texts to a sequence of integers\n",
    "        self.data[0] = pad_sequences(train_sequences, maxlen=self.pad_sequences_maxlen)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        self.data[1] = np.array(self.data[1])  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(self.data[2]) \n",
    "        self.data[2] = pad_sequences(test_sequences, maxlen=self.pad_sequences_maxlen) \n",
    "        self.data[3] = np.array(self.data[3])  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def run_model(self, file_id = None):\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data = pad_sequences_maxlen\n",
    "            Input(shape=(self.pad_sequences_maxlen,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=self.embedding_input_dim, output_dim=self.embedding_output_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(self.lstm_unit, dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation=self.dense_output_activation_function, kernel_regularizer=l2(self.reg_strength))\n",
    "        ])\n",
    "\n",
    "        # Compile the model with Adam optimizer and binary crossentropy loss, tracking accuracy\n",
    "        self.model.compile(optimizer=self.model_optimisation, loss=self.model_loss_function, metrics=['accuracy'])\n",
    "    \n",
    "        if file_id is not None:\n",
    "            model = self.load_model(file_id)\n",
    "        else:\n",
    "            model = self.train_model()\n",
    "\n",
    "        self.evaluate_model()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "    \n",
    "        unique_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save the model params and weight with the lowest validation loss and add early stopping if validation loss does not decrease for 2 epochs \n",
    "        checkpoint_filepath = os.getcwd() + f'/LSTM_Models/Weights/LSTM_Model_{unique_id}.keras'\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
    "        callbacks = [EarlyStopping(patience=self.early_stopping_patience), model_checkpoint_callback]\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting\n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=self.validation_split, callbacks=callbacks)\n",
    "\n",
    "        # Save the history file\n",
    "        with open(os.getcwd() + f'/LSTM_Models/histories/LSTM_History_{unique_id}.pkl', 'wb') as history_file:\n",
    "            pickle.dump(self.history.history, history_file)\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model(self, file_id):\n",
    "    \n",
    "        self.model.load_weights(os.getcwd() + f'/LSTM_Models/Weights/LSTM_Model_{file_id}.keras')\n",
    "        \n",
    "        with open(os.getcwd() + f'/LSTM_Models/Histories/LSTM_History_{file_id}.pkl', 'rb') as history_file:\n",
    "            self.history = pickle.load(history_file)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def evaluate_model(self):\n",
    "\n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_probabilities = self.model.predict(self.data[2])\n",
    "        test_predictions = (test_probabilities > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        conf_matrix = confusion_matrix(self.data[3], test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(self.data[3], test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        self.visualise_accuracy()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def visualise_model_params(self):\n",
    "        \n",
    "        # Example: Visualize the weights of the first Dense layer\n",
    "        dense_layer_weights = model.layers[2].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        plt.hist(dense_layer_weights.flatten())\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Weight Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def visualise_accuracy(self):\n",
    "        \n",
    "        if file_id is not None:\n",
    "            training_accuracy = self.history['accuracy']\n",
    "            validation_accuracy = self.history['val_accuracy']\n",
    "            training_loss = self.history['loss']\n",
    "            validation_loss = self.history['val_loss']\n",
    "        else:\n",
    "            training_accuracy = self.history.history['accuracy']\n",
    "            validation_accuracy = self.history.history['val_accuracy']\n",
    "            training_loss = self.history.history['loss']\n",
    "            validation_loss = self.history.history['val_loss']\n",
    "        \n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "        \n",
    "        with plt.style.context('dark_background'):\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            # Plotting training and validation accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plotting training and validation loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, training_loss, label='Training Loss', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_loss, label='Validation Loss', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, file_id = None):\n",
    "        self.file_id = file_id\n",
    "        self.tokenisation()\n",
    "        self.run_model(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282c1c36-d54d-4e2f-84e5-cf8f83785c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 110ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leon/.pyenv/versions/3.10.0/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 20 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Negative</th>\n",
       "      <td>16</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Positive</th>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Predicted Negative  Predicted Positive\n",
       "Actual Negative                  16                 122\n",
       "Actual Positive                   4                 134"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy:  0.543\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'file_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m prepare_data()\n\u001b[1;32m      3\u001b[0m lstm_classifier \u001b[38;5;241m=\u001b[39m LSTM_Classifier(data, \n\u001b[1;32m      4\u001b[0m                                   early_stopping_patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                   validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                   model_loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                                  )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mlstm_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m20240404_145447\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 194\u001b[0m, in \u001b[0;36mLSTM_Classifier.run\u001b[0;34m(self, file_id)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenisation()\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 78\u001b[0m, in \u001b[0;36mLSTM_Classifier.run_model\u001b[0;34m(self, file_id)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model()\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 130\u001b[0m, in \u001b[0;36mLSTM_Classifier.evaluate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Call the model evaluation function to plot training history\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualise_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 151\u001b[0m, in \u001b[0;36mLSTM_Classifier.visualise_accuracy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualise_accuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfile_id\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         training_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    153\u001b[0m         validation_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_id' is not defined"
     ]
    }
   ],
   "source": [
    "data = prepare_data()\n",
    "\n",
    "lstm_classifier = LSTM_Classifier(data, \n",
    "                                  early_stopping_patience = 2,\n",
    "                                  validation_split = 0.2,\n",
    "                                  tokeniser_word_count = 10000,\n",
    "                                  pad_sequences_maxlen = 100,\n",
    "                                  embedding_input_dim = 10000,\n",
    "                                  embedding_output_dim = 62,\n",
    "                                  lstm_unit = 48,\n",
    "                                  dropout_rate = 0.2,\n",
    "                                  reg_strength = 0.3,\n",
    "                                  epochs = 20,\n",
    "                                  batch_size = 32,\n",
    "                                  dense_output_activation_function = 'sigmoid',\n",
    "                                  model_optimisation = 'adam',\n",
    "                                  model_loss_function = 'binary_crossentropy',\n",
    "                                 )\n",
    "\n",
    "lstm_classifier.run('20240404_145447')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7c4af-30bc-4888-9b01-b55a42bdf45c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
