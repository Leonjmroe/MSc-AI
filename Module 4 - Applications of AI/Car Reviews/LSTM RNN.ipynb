{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5424172a-daa1-4f61-a09e-b0254855ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Keras for deep learning models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For tokenising text data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For sequence padding to uniform length\n",
    "from tensorflow.keras.models import Sequential  # For sequential model architecture\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Dropout  # Layers for model building\n",
    "from tensorflow.keras.regularizers import l2  # L2 regularisation to prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "32f18501-5680-499f-b429-d6a1455ce8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    # Load the dataset from a CSV file\n",
    "    data = pd.read_csv('car-reviews.csv')\n",
    "    \n",
    "    # Splitting the data into positive and negative reviews based on a predetermined index\n",
    "    positive_data = data[691:]  \n",
    "    negative_data = data[:691]  \n",
    "    \n",
    "    # Further split positive and negative datasets into training and testing subsets\n",
    "    train_positive = positive_data[:553]\n",
    "    test_positive = positive_data[553:] \n",
    "    train_negative = negative_data[:553]\n",
    "    test_negative = negative_data[553:]  \n",
    "    \n",
    "    # Concatenate the positive and negative training data, and do the same for testing data\n",
    "    training_data = pd.concat([train_positive, train_negative]).reset_index(drop=True)\n",
    "    testing_data = pd.concat([test_positive, test_negative]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert the 'Sentiment' column into a binary format: 1 for positive, 0 for negative\n",
    "    training_data['Sentiment'] = np.where(training_data['Sentiment'] == 'Pos', 1, 0)\n",
    "    testing_data['Sentiment'] = np.where(testing_data['Sentiment'] == 'Pos', 1, 0)\n",
    "\n",
    "    # Extract reviews (as text) and their corresponding labels for training and testing\n",
    "    training_texts = list(training_data['Review'])\n",
    "    training_labels = list(training_data['Sentiment'])\n",
    "    test_texts = list(testing_data['Review'])\n",
    "    test_labels = list(testing_data['Sentiment'])\n",
    "\n",
    "    # Return the processed text and labels for training and testing\n",
    "    return list((training_texts, training_labels, test_texts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f98dd9bb-298a-49ee-acb8-eb677f0720a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier():\n",
    "\n",
    "    model = None\n",
    "    history = None\n",
    "\n",
    "    def __init__(self, data, load_model_toggle, early_stopping_patience, validation_split, tokeniser_word_count, pad_sequences_maxlen, embedding_input_dim, \n",
    "                 embedding_output_dim, lstm_unit, dropout_rate, reg_strength, epochs, batch_size, dense_output_activation_function, model_optimisation, model_loss_function):\n",
    "\n",
    "        self.data = data\n",
    "        self.load_model_toggle = load_model_toggle\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.tokeniser_word_count = tokeniser_word_count\n",
    "        self.pad_sequences_maxlen = pad_sequences_maxlen\n",
    "\n",
    "        self.embedding_input_dim = embedding_input_dim\n",
    "        self.embedding_output_dim = embedding_output_dim\n",
    "        self.lstm_unit = lstm_unit\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.reg_strength = reg_strength\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dense_output_activation_function = dense_output_activation_function\n",
    "        self.model_optimisation = model_optimisation\n",
    "        self.model_loss_function = model_loss_function \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def tokenisation(self):\n",
    "        \n",
    "        # Initialise the tokenizer with a maximum number of words to keep, based on word frequency\n",
    "        tokenizer = Tokenizer(num_words=self.tokeniser_word_count)  # Setting the size of the vocabulary to the top 10,000 words\n",
    "        tokenizer.fit_on_texts(self.data[0])  # Updates internal vocabulary based on the list of texts\n",
    "        \n",
    "        # Convert the list of texts to a sequence of integers\n",
    "        train_sequences = tokenizer.texts_to_sequences(self.data[0])  # Transforms each text in training_texts to a sequence of integers\n",
    "        self.data[0] = pad_sequences(train_sequences, maxlen=self.pad_sequences_maxlen)  # Ensures all sequences in a list have the same length by padding/truncating\n",
    "        self.data[1] = np.array(self.data[1])  \n",
    "        \n",
    "        # Repeat the tokenisation and padding process for the testing set\n",
    "        test_sequences = tokenizer.texts_to_sequences(self.data[2]) \n",
    "        self.data[2] = pad_sequences(test_sequences, maxlen=self.pad_sequences_maxlen) \n",
    "        self.data[3] = np.array(self.data[3])  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def LTSM_model(self):\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            \n",
    "            # Input layer specifies the shape of input data = pad_sequences_maxlen\n",
    "            Input(shape=(self.pad_sequences_maxlen,)),\n",
    "            \n",
    "            # Embedding layer to turn positive integers (indexes) into dense vectors of fixed size, 10000 is the size of the vocabulary\n",
    "            Embedding(input_dim=self.embedding_input_dim, output_dim=self.embedding_output_dim),\n",
    "            \n",
    "            # LSTM layer with specified units and dropout for regularization\n",
    "            # Wrap the LSTM layer with a Bidirectional layer\n",
    "            Bidirectional(LSTM(self.lstm_unit, dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)),\n",
    "            \n",
    "            # Dense output layer with sigmoid activation for binary classification\n",
    "            Dense(1, activation=self.dense_output_activation_function, kernel_regularizer=l2(self.reg_strength))\n",
    "        ])\n",
    "    \n",
    "        if self.load_model == True:\n",
    "            model = self.load_model()\n",
    "        else:\n",
    "            model = self.compile_model()\n",
    "\n",
    "        self.evaluate_model()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def evaluate_model(self):\n",
    "\n",
    "        # Predict probabilities on the test set and convert to class labels (1 or 0) based on a 0.5 threshold\n",
    "        test_probabilities = self.model.predict(self.data[2])\n",
    "        test_predictions = (test_probabilities > 0.5).astype(\"int32\").flatten()\n",
    "        \n",
    "        # Generating the confusion matrix from test labels and predictions\n",
    "        conf_matrix = confusion_matrix(self.data[3], test_predictions)\n",
    "        conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Actual Negative\", \"Actual Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "        print('\\n')\n",
    "        display(conf_matrix_df)\n",
    "    \n",
    "        accuracy = accuracy_score(self.data[3], test_predictions)\n",
    "        print('\\n')\n",
    "        print('Accuracy: ', round(accuracy, 3))\n",
    "        print('\\n')\n",
    "    \n",
    "        # Call the model evaluation function to plot training history\n",
    "        self.visualise_accuracy()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def compile_model(self):\n",
    "    \n",
    "        # Compile the model with Adam optimizer and binary crossentropy loss, tracking accuracy\n",
    "        self.model.compile(optimizer=self.model_optimisation, loss=self.model_loss_function, metrics=['accuracy'])\n",
    "    \n",
    "        # Save the model params and weight with the lowest validation loss and add early stopping if validation loss does not decrease for 2 epochs \n",
    "        checkpoint_filepath = os.getcwd() + f'/LSTM_Models/weights/LSTM_Model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.keras'\n",
    "        model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_loss', mode='min', save_best_only=True)\n",
    "        callbacks = [EarlyStopping(patience=self.early_stopping_patience), model_checkpoint_callback]\n",
    "        \n",
    "        # Train the model on the training data, with a validation split to monitor overfitting\n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=self.validation_split, callbacks=callbacks)\n",
    "\n",
    "        # Save the history file\n",
    "        with open(f'LSTM_Models/histories/LSTM_History_{unique_id}.pkl', 'wb') as history_file:\n",
    "            pickle.dump(self.history, history_file)\n",
    "\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "    \n",
    "        model.load_weights('/LSTM_Models/weights/LSTM_Model_20240404_141225.keras')\n",
    "        self.history = self.model.fit(self.data[0], self.data[1], batch_size=self.batch_size, epochs=self.epochs, validation_split=self.validation_split, callbacks=callbacks)\n",
    "        \n",
    "\n",
    "    \n",
    "    def visualise_model_params(self):\n",
    "        \n",
    "        # Example: Visualize the weights of the first Dense layer\n",
    "        dense_layer_weights = model.layers[2].get_weights()[0]  # 0 for weights, 1 for biases\n",
    "        plt.hist(dense_layer_weights.flatten())\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Weight Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def visualise_accuracy(self):\n",
    "        \n",
    "        # Accessing training and validation accuracy and loss from the history object\n",
    "        training_accuracy = self.history.history['accuracy']\n",
    "        validation_accuracy = self.history.history['val_accuracy']\n",
    "        training_loss = self.history.history['loss']\n",
    "        validation_loss = self.history.history['val_loss']\n",
    "        epochs = range(1, len(training_accuracy) + 1)\n",
    "        \n",
    "        # Use a dark background style for plots\n",
    "        with plt.style.context('dark_background'):\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            # Plotting training and validation accuracy\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plotting training and validation loss\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, training_loss, label='Training Loss', marker='o', color='lime')\n",
    "            plt.plot(epochs, validation_loss, label='Validation Loss', marker='o', color='cyan')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        self.tokenisation()\n",
    "        self.LTSM_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c1c36-d54d-4e2f-84e5-cf8f83785c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data()\n",
    "\n",
    "lstm_classifier = LSTM_Classifier(data, \n",
    "                                  load_model_toggle = True,\n",
    "                                  early_stopping_patience = 2,\n",
    "                                  validation_split = 0.2,\n",
    "                                  tokeniser_word_count = 10000,\n",
    "                                  pad_sequences_maxlen = 100,\n",
    "                                  embedding_input_dim = 10000,\n",
    "                                  embedding_output_dim = 62,\n",
    "                                  lstm_unit = 48,\n",
    "                                  dropout_rate = 0.2,\n",
    "                                  reg_strength = 0.3,\n",
    "                                  epochs = 20,\n",
    "                                  batch_size = 32,\n",
    "                                  dense_output_activation_function = 'sigmoid',\n",
    "                                  model_optimisation = 'adam',\n",
    "                                  model_loss_function = 'binary_crossentropy',\n",
    "                                 )\n",
    "\n",
    "lstm_classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232fa92-29a5-45c1-8bfc-c7b9d3cf19ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
