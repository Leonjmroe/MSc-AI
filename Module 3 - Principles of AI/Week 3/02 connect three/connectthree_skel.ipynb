{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Three \n",
    "### Skeleton Code\n",
    "Here is an example which adapts the minimax code from the noughts and crosses demo to work on the provided connect 3 code. This is not a “model” solution – it was just quickly put together as a quick demo. You may wish to use this as a base to add some of the aforementioned extensions.\n",
    "\n",
    "Using minimax with alpha-beta pruning is sufficient to play quite quickly when the AI is playing second. If you ask it to play first, it will take a long time, but it is playable. Warning: the game is a forced win for the first player when playing optimally – prepare to lose if you play second!\n",
    "\n",
    "If you increase the size of the grid, or make the number of pieces to connect higher than 3, it goes extremely slowly even in the second player slot.\n",
    "\n",
    "The code would benefit from many relatively simple optimisations. Caching the result of each move (memoisation) would be a good start, though this is really just a programming trick, not a new AI technique.\n",
    "\n",
    "Similarly, one of the reasons it runs slowly is because it has to clone the original game every time it tests a move. If the AI used its own representation of the board and rules it could optimise the depth-first search to save a lot of memory. Copying memory is time expensive too, so this would also speed up execution.\n",
    "\n",
    "But the fun part of this activity would be to try one of the other optimisations, such as a heuristic function to evaluate the board and fix the recursion depth, or a table lookup that pre-computes endgame positions before the code even runs. Have fun if you decide to try!\n",
    "\n",
    "### Explanation\n",
    "There is a lot of code below, but it hardly any more complicated than the alpha-beta pruning code from before. Try to understand that before you continue.\n",
    "\n",
    "First new thing is a bunch of helper code so that you can actually play the game interactively when you run the cell. As with any program: it's easiest to work out what's going on if you read the code in the order it is executed. In this case start at the bottom and work up. Just a few helper functions to present a minimal text-based UI.\n",
    "\n",
    "A few modifications are made to the minimax code as well.\n",
    "\n",
    "As mentioned, the line `copy.deepcopy(game)` is added so that it could test each action without modifying the original game.\n",
    "\n",
    "The `connect.py` code returns `reward = -1` if player one wins, and `reward = 1` if player two wins – but the minimax code always assumes it is the \"max\" player. We abuse the fact that whenever the AI makes a move and the game ends, we know that move resulted in a win or a draw for that player. By taking `abs(reward)` we force it to be `1`, then multiply it by `-1` if we were actually simulating a \"min\" player move at that point in time.\n",
    "\n",
    "Another interesting thing about connect 3 on this small board is that if player 1 goes in the middle space then they are guaranteed to win. The minimax code simply generates a value for each action, so in for player 2 following this move, all of its actions will return a reward of `-1` – if player 1 plays optimally they *will* win. If moves are tied for the best value it will play randomly between them. But this leads to odd looking play, because the AI won't block 2 in a row, even if it hasn't lost yet.\n",
    "\n",
    "In this situation, it looks like the `o` player should play in column 3, but all moves will lead to losses against an optimal `x` player.\n",
    "```\n",
    "[[' ' ' ' ' ' ' ' ' ']\n",
    " [' ' ' ' ' ' ' ' ' ']\n",
    " ['o' ' ' 'x' ' ' 'x']]\n",
    "```\n",
    "\n",
    "To make the AI play more human-like, we modify the reward from the game, and divide by the turn number. So, if a move will result in an immediate loss (anything other than column 3 on the board above) then it gets a score of `-1`. But if it would result in a loss in 2 moves time, it will get a value of `-0.5`, which is better! This simple modification means the AI tries to hang on as long as possible, and if player 1 plays badly, it could even turn the tables.\n",
    "\n",
    "This means that if it can lose in multiple ways on a single move, it might still choose not to block any of them!\n",
    "\n",
    "```\n",
    "[[' ' ' ' 'o' ' ' ' ']\n",
    " [' ' 'x' 'x' ' ' ' ']\n",
    " ['o' 'x' 'x' 'o' ' ']]\n",
    "```\n",
    "\n",
    "At this point all columns are equally instant-lose moves for the AI playing as `o`, so it might move in the far right column even though it looks like a silly move – consider it a resignation! (Maybe it would be interesting to try to \"improve\" this behaviour through another modification of the reward?)\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import connect\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def next_move(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class HumanAgent(Agent):\n",
    "    def next_move(self, state):\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"What's your next move? Available columns:\")\n",
    "                print(state.available_actions)\n",
    "                move = input(\"> \").strip()\n",
    "                move = int(move)\n",
    "                if move not in state.available_actions:\n",
    "                    print(\"Invalid column.\")\n",
    "                else:\n",
    "                    return move\n",
    "            except ValueError:\n",
    "                print(f\"Please enter valid column from: {state.available_actions}\")\n",
    "\n",
    "\n",
    "class ConnectAgent(Agent):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def next_move(self, game):\n",
    "        print(\"\\nThe AI is thinking.\", end=\"\")\n",
    "\n",
    "        best_action = []\n",
    "        best_value = -1 * math.inf\n",
    "        for action in game.available_actions:\n",
    "            # important: take a copy of the entire game so we don't break things\n",
    "            # memory intensive, but saves us rewriting the game\n",
    "            game_copy = copy.deepcopy(game)\n",
    "            game_copy._verbose = False\n",
    "\n",
    "            reward, game_over = game_copy.act(action)\n",
    "\n",
    "            if game_over:\n",
    "                # this code always tries to maximise the score\n",
    "                # but the game returns reward for player 2 ('o')\n",
    "                # if our move ended the game, then it was either win or draw\n",
    "                # so absolute value will ensure we get 0 or 1 no matter if we are p1 or p2\n",
    "                action_value = abs(reward)\n",
    "            else:\n",
    "                # our move didn't end the game, so recurse\n",
    "                action_value = self.get_value(game_copy, get_min=True)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(action_value, end=\" \")\n",
    "            if action_value > best_value:\n",
    "                best_action = [action]\n",
    "                best_value = action_value\n",
    "            if action_value == best_value:\n",
    "                best_action.append(action)\n",
    "\n",
    "        if self.verbose:\n",
    "            print()\n",
    "        print(\"\\n\")\n",
    "\n",
    "        return random.choice(best_action)\n",
    "\n",
    "    def get_value(self, game, get_min, alpha=-math.inf, beta=math.inf, turn=2):\n",
    "        \"\"\"If get_min is set to true, returns the minimum value, otherwise the maximum value\"\"\"\n",
    "\n",
    "        best_value = math.inf\n",
    "        if not get_min:\n",
    "            best_value *= -1\n",
    "\n",
    "        for action in game.available_actions:\n",
    "            game_copy = copy.deepcopy(game)\n",
    "\n",
    "            reward, game_over = game_copy.act(action)\n",
    "\n",
    "            if game_over:\n",
    "                # there is an explanation for diving by the turn number above!\n",
    "                action_value = abs(reward)/turn\n",
    "                if get_min:\n",
    "                    # force the reward to be -1 if this was a winning move while looking to minimise\n",
    "                    # but keep it as zero if it was a draw\n",
    "                    action_value *= -1\n",
    "            else:\n",
    "                action_value = self.get_value(game_copy, get_min=not get_min, alpha=alpha, beta=beta, turn=turn+1)\n",
    "\n",
    "            if not get_min:\n",
    "                alpha = max(alpha, action_value)\n",
    "                if action_value >= beta:\n",
    "                    return action_value\n",
    "            else:\n",
    "                beta = min(beta, action_value)\n",
    "                if action_value <= alpha:\n",
    "                    return action_value\n",
    "\n",
    "            if not get_min and action_value > best_value \\\n",
    "                    or get_min and action_value < best_value:\n",
    "                best_value = action_value\n",
    "\n",
    "        return best_value\n",
    "\n",
    "\n",
    "def run_game(game, player1=HumanAgent(), player2=ConnectAgent()):\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        move = player1.next_move(game)\n",
    "        reward, game_over = game.act(move)\n",
    "\n",
    "        # reward is in terms of player 2, 'o'\n",
    "        if game_over and reward == -1:\n",
    "            print(\"Player one wins!\")\n",
    "            return\n",
    "        elif game_over and reward == 0:\n",
    "            print(\"It's a draw.\")\n",
    "            return\n",
    "\n",
    "        move = player2.next_move(game)\n",
    "        reward, game_over = game.act(move)\n",
    "\n",
    "        if game_over and reward == 1:\n",
    "            print(\"Player two wins!\")\n",
    "            return\n",
    "        elif game_over and reward == 0:\n",
    "            print(\"It's a draw.\")\n",
    "            return\n",
    "\n",
    "\n",
    "def yes_no_input(text, prompt=\"> \"):\n",
    "    print(text + \" (y/n)\")\n",
    "    response = input(prompt).strip()\n",
    "    while response not in ['y', 'n']:\n",
    "        print(\"Please enter y for yes or n for no.\")\n",
    "        print(text)\n",
    "        response = input(prompt).strip()\n",
    "    return response == \"y\"\n",
    "\n",
    "\n",
    "def play():\n",
    "    cols = 5\n",
    "    rows = 3\n",
    "    n = 3\n",
    "\n",
    "    print(f\"Let's play connect {n}!\")\n",
    "    game = connect.Connect(num_cols=cols, num_rows=rows, num_connect=n, verbose=True)\n",
    "    again = True\n",
    "    while again:\n",
    "        response = yes_no_input(\"Would you like to play first?\")\n",
    "        if response:\n",
    "            run_game(game=game, player1=HumanAgent(), player2=ConnectAgent())\n",
    "        else:\n",
    "            run_game(game=game, player1=ConnectAgent(), player2=HumanAgent())\n",
    "\n",
    "        again = yes_no_input(\"Would you like to play again?\")\n",
    "        if again:\n",
    "            game.reset()\n",
    "\n",
    "\n",
    "play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
