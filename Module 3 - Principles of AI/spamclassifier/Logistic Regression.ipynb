{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec45a363-ccf0-41fc-93e0-5124cb7587a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d252ab1-a759-49c7-8cdc-17c278c4523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, training_data, testing_data):\n",
    "        self.training_data = training_data\n",
    "        self.testing_data = testing_data\n",
    "        self.prior_normal_probs = {}\n",
    "    \n",
    "    \n",
    "    def probability_calc(self):\n",
    "        label_arr = self.training_data[:, 0]\n",
    "        # Initialise dictionaries for storing prior probabilities and feature probabilities\n",
    "        prior_probs = {}\n",
    "        all_feature_probs = {}\n",
    "        # Laplace smoothing factor to avoid division by zero\n",
    "        laplace_alpha = 1\n",
    "        unique_labels = np.unique(label_arr)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            # Count occurrences of each class label to calculate prior probabilities\n",
    "            label_count = np.sum(label == label_arr)\n",
    "            prior_probs[label] = np.log(label_count / len(label_arr))\n",
    "            self.prior_normal_probs[label] = label_count / len(label_arr)\n",
    "            probabilities = np.array([])\n",
    "    \n",
    "            # Initialise dictionary for storing probabilities of each feature given the class\n",
    "            feature_probs = {}\n",
    "            # Extract rows corresponding to the current class\n",
    "            features = self.training_data[self.training_data[:, 0] == label][:, 1:]\n",
    "    \n",
    "            for feature in range(np.shape(features[0])[0]):  # Loop through all features \n",
    "                \n",
    "                # Laplace smoothing to remove issues with zero frequency features\n",
    "                prob = np.log((np.sum(features[:, feature]) + laplace_alpha) / (len(features) + laplace_alpha * 2))  \n",
    "                \n",
    "                probabilities = np.append(probabilities, prob) \n",
    "            \n",
    "            all_feature_probs[label] = probabilities  \n",
    "            \n",
    "        return prior_probs, all_feature_probs\n",
    "\n",
    "\n",
    "\n",
    "    def testing(self, prior_probs, all_feature_probs, test_element):\n",
    "        label_probs = {}\n",
    "        for label in np.unique(self.training_data[:, 0]):\n",
    "            probabilities = np.array([])\n",
    "            \n",
    "            for feature in range(len(test_element)):  # Skip label in test_element\n",
    "                if test_element[feature] == 1:  # Consider feature if present\n",
    "                    feature_prob = all_feature_probs[label][feature]  \n",
    "                    final_prob = prior_probs[label] + feature_prob\n",
    "                    probabilities = np.append(probabilities, final_prob)\n",
    "            \n",
    "            # Calculate class probability by multiplying prior with product of feature probabilities\n",
    "            label_probs[label] = np.sum(probabilities)\n",
    "\n",
    "        return label_probs\n",
    "\n",
    "    \n",
    "    def probability_normalisation(self, log_probs):\n",
    "        # Converts log probabilities to normalised probabilities \n",
    "        e_x = np.exp(log_probs - np.max(log_probs))\n",
    "        return e_x / e_x.sum(axis=0)  \n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        prior_probs, all_feature_probs = self.probability_calc()\n",
    "        normalised_probs = []\n",
    "\n",
    "        for test_element in self.testing_data:\n",
    "            log_probs = self.testing(prior_probs, all_feature_probs, test_element)\n",
    "            normalised_prob = self.probability_normalisation(np.array(list(log_probs.values())))\n",
    "            normalised_probs.append(normalised_prob)\n",
    "\n",
    "        normalised_probs = np.array(normalised_probs)\n",
    "        \n",
    "        # Find the indices of the max values in each row\n",
    "        max_indices = np.argmax(normalised_probs, axis=1)\n",
    "        \n",
    "        # Initialise an array to hold the transformed values\n",
    "        transformed_probabilities = np.zeros_like(max_indices, dtype=float)\n",
    "        \n",
    "        # Iterate over each row\n",
    "        for i, index in enumerate(max_indices):\n",
    "            if index == 0:  # If the max value comes from the second column\n",
    "                transformed_probabilities[i] = 1 - normalised_probs[i, index]\n",
    "            else:  # If the max value comes from the first column\n",
    "                transformed_probabilities[i] = normalised_probs[i, index]\n",
    "\n",
    "        return transformed_probabilities, self.prior_normal_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeb02e91-9a6d-4a07-81be-e7e00deca8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10933333333333328 0.18222222222222215\n",
      "0.896\n",
      "0.938\n",
      "0.936\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(self, training_data, training_cycles, learning_rate, lr_bias, buffer_divisor, lambda_penalty):\n",
    "        self.training_data = training_data\n",
    "        self.training_cycles = training_cycles\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_bias = lr_bias\n",
    "        self.buffer_divisor = buffer_divisor\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "    \n",
    "\n",
    "    def train(self, features, constant, training_features, training_labels):\n",
    "        errors = [] \n",
    "        train_count = training_features.shape[1] \n",
    "\n",
    "        for cycle in range(self.training_cycles):\n",
    "            \n",
    "            # Calculate the linear combination of the features multiplied by their respected weights plus the constant. Feature array transpose needed for multiplication \n",
    "            logistic_input = np.dot(features.T, training_features) + constant\n",
    "            probability = self.sigmoid_function(logistic_input)\n",
    "            \n",
    "            # Feature gradient decent using Lasso regularisation to add a penalty\n",
    "            feature_gradient = (1 / train_count) * np.dot(training_features, (probability - training_labels).T)\n",
    "            feature_gradient += (self.lambda_penalty / train_count) * np.sign(features)\n",
    "\n",
    "            # Constant gradient decent \n",
    "            constant_gradient = (1 / train_count) * np.sum(probability - training_labels)\n",
    "\n",
    "            # Update the values of the features and constant weightings\n",
    "            features = features - self.learning_rate * feature_gradient\n",
    "            constant = constant - self.learning_rate * constant_gradient\n",
    "\n",
    "        return features, constant\n",
    "\n",
    "\n",
    "    \n",
    "    def sigmoid_function(self, logistic_input):\n",
    "        \n",
    "        # Predict the probability that logistic_input belongs to class 1. Clip the input to prevent overflow of small numbers.\n",
    "        logistic_input = np.clip(logistic_input, -1000, 1000) \n",
    "        probabilities = 1 / (1 + np.exp(-logistic_input))\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "\n",
    "    \n",
    "    def classify(self, features, constant, test_data):\n",
    "        # Create array for prediction same length as the amount of test_data elements\n",
    "        test_data_count = test_data.shape[1]  \n",
    "\n",
    "        # Compute logistic predictions using trained features weight and trained constant \n",
    "        probabilities = self.sigmoid_function(np.dot(features.T, test_data) + constant)\n",
    "\n",
    "        return probabilities\n",
    "    \n",
    "\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "\n",
    "        # Split the training data \n",
    "        training_labels = self.training_data[:, 0].astype(int)\n",
    "        training_features = self.training_data[:, 1:].astype(int)\n",
    "        \n",
    "        # Change the shape of the feature and training arrays so they can be used for matrix math calculations \n",
    "        training_features = training_features.T\n",
    "        training_labels = training_labels.reshape(1, -1)\n",
    "        \n",
    "        # Train the model using gradient descent\n",
    "        default_features = np.zeros((training_features.shape[0], 1))\n",
    "        default_constant = 0\n",
    "        features, constant = self.train(default_features, default_constant, training_features, training_labels)\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        lr_probabilities = self.classify(features, constant, test_data.T)\n",
    "        \n",
    "        # Instantiate the bayes classifier, run predict and return probabilities \n",
    "        naive_bayes = NaiveBayesClassifier(self.training_data, test_data)\n",
    "        bayes_probabilities, prior_probabilities = naive_bayes.predict()\n",
    "\n",
    "        # Combining the probabilities \n",
    "        same_side_condition = ((lr_probabilities > 0.5) & (bayes_probabilities > 0.5)) | ((lr_probabilities < 0.5) & (bayes_probabilities < 0.5))\n",
    "        average_values = (lr_probabilities * self.lr_bias + bayes_probabilities) / 2\n",
    "        combined_probabilities = np.where(same_side_condition, average_values, lr_probabilities)\n",
    "        \n",
    "        # Using prior probs to add a bias within the buffer to 0.5\n",
    "        max_key = max(prior_probabilities, key=prior_probabilities.get)\n",
    "        max_value = prior_probabilities[max_key]\n",
    "        buffer = max_value - 0.5\n",
    "        \n",
    "        # Adjusting decision thresholds based on the buffer\n",
    "        upper_threshold = 0.5 + buffer / self.buffer_divisor\n",
    "        lower_threshold = 0.5 - buffer / self.buffer_divisor\n",
    "\n",
    "        print(buffer, (buffer / self.buffer_divisor))\n",
    "        \n",
    "        # Condition to check if probabilities are decisively above or below the adjusted thresholds\n",
    "        buffer_condition = (combined_probabilities > upper_threshold) | (combined_probabilities < lower_threshold)\n",
    "        \n",
    "        # Keep lr_probabilities where buffer_condition is true, otherwise, use a default probability\n",
    "        combined_probabilities_buf = np.where(buffer_condition, combined_probabilities, max_value)\n",
    "        \n",
    "        # Generating predictions based on the adjusted combined_probabilities\n",
    "        predictions = (combined_probabilities_buf > 0.5).astype(int)\n",
    "\n",
    "        probabilities = [bayes_probabilities, lr_probabilities, combined_probabilities, combined_probabilities_buf]\n",
    "\n",
    "        return predictions, probabilities\n",
    "\n",
    "# 10000, 0.335, 0.6, 0.6, -0.4)\n",
    "# 9000, 0.32, 0.5, 0.58, -0.38)\n",
    "\n",
    "def create_classifier():\n",
    "    classifier = LogisticRegressionClassifier(training_spam, 10000, 0.335, 0.6, 0.6, -0.4)\n",
    "    return classifier\n",
    "\n",
    "classifier = create_classifier()\n",
    "predictions, probabilities = classifier.predict(testing_spam[:, 1:])\n",
    "\n",
    "for probability in probabilities:\n",
    "    predictions = (probability > 0.5).astype(int)\n",
    "    accuracy = np.count_nonzero(predictions == testing_spam[:, 0])/testing_spam[:, 0].shape[0]\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914a0fb5-0165-4ba4-8705-ffd3e0f6f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the class I used to tune my logistic - bayes classifier. There are 6 different hyperparameters that I tuned \n",
    "\n",
    "class HyperparameterOptimiser:\n",
    "    def __init__(self, data, training_cycle_params, learning_rate_params, lr_bias_params, buffer_divisor_params, lambda_params, data_split_count, data_test_ratio, data_split_toggle):\n",
    "        self.data = data\n",
    "        self.training_cycle_params = training_cycle_params\n",
    "        self.learning_rate_params = learning_rate_params\n",
    "        self.lr_bias_params = lr_bias_params\n",
    "        self.buffer_divisor_params = buffer_divisor_params\n",
    "        self.lambda_params = lambda_params\n",
    "        self.data_split_count = data_split_count\n",
    "        self.data_test_ratio = data_test_ratio\n",
    "        self.optimal_params = {}\n",
    "        self.best_accuracy = 0\n",
    "        self.data_split_toggle = data_split_toggle\n",
    "\n",
    "    # Grid search to loop through all my hyperparameters \n",
    "    \n",
    "    def grid_search(self):\n",
    "        best_accuracy = 0\n",
    "        optimised_parameters = {'learning_rate': None, 'training_cycles': None}\n",
    "        \n",
    "        for learning_rate in self.learning_rate_params:\n",
    "            print('Learning Rate: ', learning_rate)\n",
    "            for cycles in self.training_cycle_params:\n",
    "                print('Cycle: ', cycles)\n",
    "                for lr_bias in self.lr_bias_params:\n",
    "                    print('lr_bias: ', lr_bias)\n",
    "                    for buffer_divisor in self.buffer_divisor_params:\n",
    "                        print('buffer_divisor: ', buffer_divisor)\n",
    "                        for lambda_ in self.lambda_params:\n",
    "                            print('lambda_: ', lambda_)\n",
    " \n",
    "                            self.data_run(learning_rate, cycles, lr_bias, buffer_divisor, lambda_)\n",
    "\n",
    "    \n",
    "    \n",
    "    def data_run(self, learning_rate, cycles, lr_bias, buffer_divisor, lambda_):\n",
    "\n",
    "        if self.data_split_toggle == False:\n",
    "            test_data = testing_spam\n",
    "            train_data = training_spam\n",
    "            classifier = LogisticRegressionClassifier(train_data, int(cycles), learning_rate, lr_bias, buffer_divisor, lambda_)\n",
    "            predictions = classifier.predict(test_data[:, 1:])[0]\n",
    "            accuracy = np.count_nonzero(predictions == test_data[:, 0])/test_data[:, 0].shape[0]\n",
    "        else:\n",
    "            dataset_size = self.data.shape[0]\n",
    "            test_size = int(dataset_size * self.data_test_ratio)\n",
    "            split_accuracies = np.array([])\n",
    "            \n",
    "            for _ in range(self.data_split_count):\n",
    "                np.random.shuffle(data)\n",
    "                test_data = self.data[:test_size]\n",
    "                train_data = self.data[test_size:]\n",
    "            \n",
    "                classifier = LogisticRegressionClassifier(train_data, int(cycles), learning_rate, lr_bias, buffer_divisor, lambda_)\n",
    "                predictions = classifier.predict(test_data[:, 1:])[0]\n",
    "                accuracy = np.count_nonzero(predictions == test_data[:, 0])/test_data[:, 0].shape[0]\n",
    "\n",
    "                split_accuracies = np.append(split_accuracies, accuracy)\n",
    "                print('Split ' + str(_), 'Accuracy: ', accuracy)\n",
    "            \n",
    "            print('Average accuracy: ', np.mean(split_accuracies))\n",
    "            print('\\n')\n",
    "            accuracy = np.mean(split_accuracies)\n",
    "        \n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.optimal_params['learning_rate'] = learning_rate\n",
    "            self.optimal_params['training_cycles'] = cycles\n",
    "            self.optimal_params['lr_bias'] = lr_bias\n",
    "            self.optimal_params['buffer_divisor'] = buffer_divisor\n",
    "            self.optimal_params['lambda_'] = lambda_\n",
    "            print('\\n')\n",
    "            print(f\"New best accuracy: {self.best_accuracy:.4f} with Cycles={cycles}, LR={learning_rate}, LR Bias={lr_bias}, Buffer Divisor={buffer_divisor}, Lambda={lambda_}\")\n",
    "            print('\\n')\n",
    "\n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        optimised_parameters = self.grid_search()\n",
    "        print(\"Optimised Parameters: \", self.optimal_params)\n",
    "        print(\"Best Accuracy: \", self.best_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = np.genfromtxt(open(\"data/all_spam_data.csv\"), delimiter=\",\", encoding='utf-8-sig')\n",
    "\n",
    "hyperparameter_optimiser = HyperparameterOptimiser( \n",
    "            data, \n",
    "            training_cycle_params = np.arange(9000, 11000, 500).tolist(), \n",
    "            learning_rate_params = np.arange(0.32, 0.345, 0.005).tolist(), \n",
    "            lr_bias_params = np.arange(0.5, 0.7, 0.02).tolist(), \n",
    "            buffer_divisor_params = np.arange(0.5, 0.7, 0.02).tolist(), \n",
    "            lambda_params = np.arange(-0.5, -0.3, 0.02).tolist(), \n",
    "            data_split_count = 10, \n",
    "            data_test_ratio = 0.33,\n",
    "            data_split_toggle = False )\n",
    "\n",
    "# hyperparameter_optimiser.run()\n",
    "\n",
    "# New best accuracy: 0.9500 with Cycles=10000, LR=0.335, LR Bias=0.6, Buffer Divisor=0.6, Lambda=-0.40000000000000013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce748f4-9a8b-4a6c-a255-c4d0a6a6ab10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf1bfd-4766-45f1-9724-7564026ca020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
